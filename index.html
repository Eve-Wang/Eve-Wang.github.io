<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-kaggle之房价预测项目" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/" class="article-date">
  <time datetime="2020-11-03T01:40:58.000Z" itemprop="datePublished">2020-11-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/">kaggle之房价预测项目</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>可直接跳到<strong>步骤总结</strong>，前面部分是对训练集的特征进行处理和分析的细节。</p>
<h2 id="项目要求"><a href="#项目要求" class="headerlink" title="项目要求"></a>项目要求</h2><p>已知81个与房价相关的特征，样本量共1460个。得到一个房价预测模型。有关这些特征的解释官网有给出。</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><h3 id="样本信息"><a href="#样本信息" class="headerlink" title="样本信息"></a>样本信息</h3><p>首先先看一下样本的大致轮廓</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data&#x3D;pd.read_csv(&#39;.&#x2F;train.csv&#39;)</span><br><span class="line">data</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C1.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.info()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;</span><br><span class="line">RangeIndex: 1460 entries, 0 to 1459</span><br><span class="line">Data columns (total 81 columns):</span><br><span class="line"> #   Column         Non-Null Count  Dtype  </span><br><span class="line">---  ------         --------------  -----  </span><br><span class="line"> 0   Id             1460 non-null   int64  </span><br><span class="line"> 1   MSSubClass     1460 non-null   int64  </span><br><span class="line"> 2   MSZoning       1460 non-null   object </span><br><span class="line"> 3   LotFrontage    1201 non-null   float64</span><br><span class="line"> 4   LotArea        1460 non-null   int64  </span><br><span class="line"> 5   Street         1460 non-null   object </span><br><span class="line"> 6   Alley          91 non-null     object </span><br><span class="line"> 7   LotShape       1460 non-null   object </span><br><span class="line"> 8   LandContour    1460 non-null   object </span><br><span class="line"> 9   Utilities      1460 non-null   object </span><br><span class="line"> 10  LotConfig      1460 non-null   object </span><br><span class="line"> 11  LandSlope      1460 non-null   object </span><br><span class="line"> 12  Neighborhood   1460 non-null   object </span><br><span class="line"> 13  Condition1     1460 non-null   object </span><br><span class="line"> 14  Condition2     1460 non-null   object </span><br><span class="line"> 15  BldgType       1460 non-null   object </span><br><span class="line"> 16  HouseStyle     1460 non-null   object </span><br><span class="line"> 17  OverallQual    1460 non-null   int64  </span><br><span class="line"> 18  OverallCond    1460 non-null   int64  </span><br><span class="line"> 19  YearBuilt      1460 non-null   int64  </span><br><span class="line"> 20  YearRemodAdd   1460 non-null   int64  </span><br><span class="line"> 21  RoofStyle      1460 non-null   object </span><br><span class="line"> 22  RoofMatl       1460 non-null   object </span><br><span class="line"> 23  Exterior1st    1460 non-null   object </span><br><span class="line"> 24  Exterior2nd    1460 non-null   object </span><br><span class="line"> 25  MasVnrType     1452 non-null   object </span><br><span class="line"> 26  MasVnrArea     1452 non-null   float64</span><br><span class="line"> 27  ExterQual      1460 non-null   object </span><br><span class="line"> 28  ExterCond      1460 non-null   object </span><br><span class="line"> 29  Foundation     1460 non-null   object </span><br><span class="line"> 30  BsmtQual       1423 non-null   object </span><br><span class="line"> 31  BsmtCond       1423 non-null   object </span><br><span class="line"> 32  BsmtExposure   1422 non-null   object </span><br><span class="line"> 33  BsmtFinType1   1423 non-null   object </span><br><span class="line"> 34  BsmtFinSF1     1460 non-null   int64  </span><br><span class="line"> 35  BsmtFinType2   1422 non-null   object </span><br><span class="line"> 36  BsmtFinSF2     1460 non-null   int64  </span><br><span class="line"> 37  BsmtUnfSF      1460 non-null   int64  </span><br><span class="line"> 38  TotalBsmtSF    1460 non-null   int64  </span><br><span class="line"> 39  Heating        1460 non-null   object </span><br><span class="line"> 40  HeatingQC      1460 non-null   object </span><br><span class="line"> 41  CentralAir     1460 non-null   object </span><br><span class="line"> 42  Electrical     1459 non-null   object </span><br><span class="line"> 43  1stFlrSF       1460 non-null   int64  </span><br><span class="line"> 44  2ndFlrSF       1460 non-null   int64  </span><br><span class="line"> 45  LowQualFinSF   1460 non-null   int64  </span><br><span class="line"> 46  GrLivArea      1460 non-null   int64  </span><br><span class="line"> 47  BsmtFullBath   1460 non-null   int64  </span><br><span class="line"> 48  BsmtHalfBath   1460 non-null   int64  </span><br><span class="line"> 49  FullBath       1460 non-null   int64  </span><br><span class="line"> 50  HalfBath       1460 non-null   int64  </span><br><span class="line"> 51  BedroomAbvGr   1460 non-null   int64  </span><br><span class="line"> 52  KitchenAbvGr   1460 non-null   int64  </span><br><span class="line"> 53  KitchenQual    1460 non-null   object </span><br><span class="line"> 54  TotRmsAbvGrd   1460 non-null   int64  </span><br><span class="line"> 55  Functional     1460 non-null   object </span><br><span class="line"> 56  Fireplaces     1460 non-null   int64  </span><br><span class="line"> 57  FireplaceQu    770 non-null    object </span><br><span class="line"> 58  GarageType     1379 non-null   object </span><br><span class="line"> 59  GarageYrBlt    1379 non-null   float64</span><br><span class="line"> 60  GarageFinish   1379 non-null   object </span><br><span class="line"> 61  GarageCars     1460 non-null   int64  </span><br><span class="line"> 62  GarageArea     1460 non-null   int64  </span><br><span class="line"> 63  GarageQual     1379 non-null   object </span><br><span class="line"> 64  GarageCond     1379 non-null   object </span><br><span class="line"> 65  PavedDrive     1460 non-null   object </span><br><span class="line"> 66  WoodDeckSF     1460 non-null   int64  </span><br><span class="line"> 67  OpenPorchSF    1460 non-null   int64  </span><br><span class="line"> 68  EnclosedPorch  1460 non-null   int64  </span><br><span class="line"> 69  3SsnPorch      1460 non-null   int64  </span><br><span class="line"> 70  ScreenPorch    1460 non-null   int64  </span><br><span class="line"> 71  PoolArea       1460 non-null   int64  </span><br><span class="line"> 72  PoolQC         7 non-null      object </span><br><span class="line"> 73  Fence          281 non-null    object </span><br><span class="line"> 74  MiscFeature    54 non-null     object </span><br><span class="line"> 75  MiscVal        1460 non-null   int64  </span><br><span class="line"> 76  MoSold         1460 non-null   int64  </span><br><span class="line"> 77  YrSold         1460 non-null   int64  </span><br><span class="line"> 78  SaleType       1460 non-null   object </span><br><span class="line"> 79  SaleCondition  1460 non-null   object </span><br><span class="line"> 80  SalePrice      1460 non-null   int64  </span><br><span class="line">dtypes: float64(3), int64(35), object(43)</span><br><span class="line">memory usage: 924.0+ KB</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.describe(include&#x3D;&#39;O&#39;)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C2.png" alt></p>
<p>看看SalePrice这列的数据轮廓</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.SalePrice.describe()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">count      1460.000000</span><br><span class="line">mean     180921.195890</span><br><span class="line">std       79442.502883</span><br><span class="line">min       34900.000000</span><br><span class="line">25%      129975.000000</span><br><span class="line">50%      163000.000000</span><br><span class="line">75%      214000.000000</span><br><span class="line">max      755000.000000</span><br><span class="line">Name: SalePrice, dtype: float64</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.hist(data.SalePrice,40)</span><br><span class="line">#sns.distplot(data.SalePrice) </span><br><span class="line">#distplot在新seaborn版本中已弃用</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C3.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#skewness and kurtosis 指相对于正太分布的偏度和峰度</span><br><span class="line">print(&quot;Skewness: %f&quot; % data[&#39;SalePrice&#39;].skew())</span><br><span class="line">print(&quot;Kurtosis: %f&quot; % data[&#39;SalePrice&#39;].kurt())</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Skewness: 1.882876</span><br><span class="line">Kurtosis: 6.536282</span><br></pre></td></tr></table></figure>

<h3 id="理解特征"><a href="#理解特征" class="headerlink" title="理解特征"></a>理解特征</h3><p>在刚才的那些数据分析中，可以发现其实有些特征表达的意思其实是重合的(比如1stFlrSF和2stFlrSF)。有些特征从直觉看似乎要更重要，比如OverallQual。</p>
<p>现在可以先建一个excel表，把这些特征的type(数据类型,categorical or numerical)，Segment(种类，分为building，space，location)，Expectation(从个人经验来看这些特征的重要性，high,medium,low),Conclusion(特征分析后得到的结果，high,medium,low)</p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Type</th>
<th>Segment</th>
<th>Expectation</th>
<th>Conclusion</th>
<th>comments</th>
</tr>
</thead>
<tbody><tr>
<td>Id</td>
<td>c</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MSSubClass</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MSZoning</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>LotFrontage</td>
<td>n</td>
<td>l</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Street</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>LotArea</td>
<td>n</td>
<td>s</td>
<td>h</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Alley</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td>only few</td>
</tr>
<tr>
<td>LotShape</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>LandContour</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Utilities</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>LotConfig</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>LandSlope</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td>similar with flat</td>
</tr>
<tr>
<td>Neighborhood</td>
<td>c</td>
<td>b</td>
<td>h</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Condition1</td>
<td>c</td>
<td>l</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Condition2</td>
<td>c</td>
<td>l</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BldgType</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>HouseStyle</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>OverallQual</td>
<td>c</td>
<td>b</td>
<td>h</td>
<td></td>
<td></td>
</tr>
<tr>
<td>OverallCond</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>YearBuilt</td>
<td>n</td>
<td>b</td>
<td>h</td>
<td></td>
<td></td>
</tr>
<tr>
<td>YearRemodAdd</td>
<td>n</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>RoofStyle</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>RoofMatl</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Exterior1st</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td>房屋外部材料</td>
</tr>
<tr>
<td>Exterior2nd</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MasVnrType</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td>砖石饰面类型</td>
</tr>
<tr>
<td>MasVnrArea</td>
<td>n</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ExterQual</td>
<td>c</td>
<td>b</td>
<td>h</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ExterCond</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Foundation</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td>建造材质</td>
</tr>
<tr>
<td>BsmtQual</td>
<td>c</td>
<td>b</td>
<td>h</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BsmtCond</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BsmtExposure</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td>walkout or garden level walls</td>
</tr>
<tr>
<td>BsmtFinType1</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BsmtFinSF1</td>
<td>n</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BsmtFinType2</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BsmtFinSF2</td>
<td>n</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BsmtUnfSF</td>
<td>n</td>
<td>b</td>
<td>m</td>
<td></td>
<td>similar with the last one</td>
</tr>
<tr>
<td>TotalBsmtSF</td>
<td>n</td>
<td>b</td>
<td>h</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Heating</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>HeatingQC</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>CentralAir</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Electrical</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>1stFlrSF</td>
<td>n</td>
<td>s</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2ndFlrSF</td>
<td>n</td>
<td>s</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>LowQualFinSF</td>
<td>n</td>
<td>s</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GrLivArea</td>
<td>n</td>
<td>s</td>
<td>h</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BsmtFullBath</td>
<td>n</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BsmtHalfBath</td>
<td>n</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>FullBath</td>
<td>n</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>HalfBath</td>
<td>n</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BedroomAbvGr</td>
<td>n</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>KitchenAbvGr</td>
<td>n</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>KitchenQual</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>TotRmsAbvGrd</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td>not include bathroom</td>
</tr>
<tr>
<td>Functional</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fireplaces</td>
<td>n</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>FireplaceQu</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td>only half</td>
</tr>
<tr>
<td>GarageType</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GarageYrBlt</td>
<td>n</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GarageFinish</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GarageCars</td>
<td>n</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GarageArea</td>
<td>n</td>
<td>s</td>
<td>h</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GarageQual</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GarageCond</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>PavedDrive</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>WoodDeckSF</td>
<td>n</td>
<td>s</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>OpenPorchSF</td>
<td>n</td>
<td>s</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>EnclosedPorch</td>
<td>n</td>
<td>s</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>3SsnPorch</td>
<td>n</td>
<td>s</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ScreenPorch</td>
<td>n</td>
<td>s</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>PoolArea</td>
<td>n</td>
<td>s</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>PoolQC</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td>only few</td>
</tr>
<tr>
<td>Fence</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MiscFeature</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td>only few</td>
</tr>
<tr>
<td>MiscVal</td>
<td>n</td>
<td>b</td>
<td>h</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MoSold</td>
<td>c</td>
<td>b</td>
<td>l</td>
<td></td>
<td></td>
</tr>
<tr>
<td>YrSold</td>
<td>n</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SaleType</td>
<td>c</td>
<td>b</td>
<td>h</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SaleCondition</td>
<td>c</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SalePrice</td>
<td>n</td>
<td>b</td>
<td>m</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>根据官网给的特征注释和前文的一些数据分析，填好这个表格后我们会对特征有一个更好的理解。</p>
<h3 id="特征之间的关系"><a href="#特征之间的关系" class="headerlink" title="特征之间的关系"></a>特征之间的关系</h3><p>但直觉并不能作为我们提取特征的依据。接下来进行特征可视化，验证开始的想法是否正确。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#scatter plot grlivarea&#x2F;saleprice</span><br><span class="line">var &#x3D; &#39;GrLivArea&#39;</span><br><span class="line">df &#x3D; data[[&#39;GrLivArea&#39;,&#39;SalePrice&#39;]]</span><br><span class="line">df.plot.scatter(x&#x3D;var, y&#x3D;&#39;SalePrice&#39;, ylim&#x3D;(0,800000)) #pandas.DataFrame.plot.scatter()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C4.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#scatter plot TotalBsmtSF&#x2F;saleprice</span><br><span class="line">var &#x3D; &#39;TotalBsmtSF&#39;</span><br><span class="line">df &#x3D; data[[&#39;TotalBsmtSF&#39;,&#39;SalePrice&#39;]]</span><br><span class="line">df.plot.scatter(x&#x3D;var, y&#x3D;&#39;SalePrice&#39;,ylim&#x3D;(0,800000) ) #pandas.DataFrame.plot.scatter()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C5.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#box plot overallqual&#x2F;saleprice</span><br><span class="line">var &#x3D; &#39;OverallQual&#39;</span><br><span class="line">df &#x3D; data[[&#39;OverallQual&#39;,&#39;SalePrice&#39;]]</span><br><span class="line">fig&#x3D;sns.boxplot(x&#x3D;var,y&#x3D;&#39;SalePrice&#39;,data&#x3D;df)</span><br><span class="line">fig.axis(ymin&#x3D;0,ymax&#x3D;800000)</span><br></pre></td></tr></table></figure>

<p>(-0.5, 9.5, 0.0, 800000.0)</p>
<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C6.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#box plot YearBuilt&#x2F;saleprice</span><br><span class="line">var &#x3D; &#39;YearBuilt&#39;</span><br><span class="line">df &#x3D; data[[&#39;YearBuilt&#39;,&#39;SalePrice&#39;]]</span><br><span class="line">plt.figure(figsize&#x3D;(20, 8))</span><br><span class="line">plt.xticks(rotation&#x3D;90)</span><br><span class="line">plt.grid()</span><br><span class="line">sns.boxplot(x&#x3D;var, y&#x3D;&#39;SalePrice&#39;,data&#x3D;df) #pandas.DataFrame.plot.scatter()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C7.png" alt></p>
<p>以上3个特征确实与价格之间具有比较强的正相关关系，而所建年份与价格之间的关系相对弱一些。</p>
<p>再看看Neighborhood这一特征</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#box plot Neighborhood&#x2F;saleprice</span><br><span class="line">var &#x3D; &#39;Neighborhood&#39;</span><br><span class="line">df &#x3D; data[[&#39;Neighborhood&#39;,&#39;SalePrice&#39;]]</span><br><span class="line">plt.figure(figsize&#x3D;(15,8))</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xticks(rotation&#x3D;45)</span><br><span class="line">sns.boxplot(x&#x3D;var, y&#x3D;&#39;SalePrice&#39;,data&#x3D;df) #pandas.DataFrame.plot.scatter()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C8.png" alt></p>
<p>看看整体上，所有特征（不包括字符串类型）之间的相互关系</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data1&#x3D;data.drop([&#39;Id&#39;],axis&#x3D;1)</span><br><span class="line">corrmat&#x3D;data1.corr()</span><br><span class="line">plt.figure(figsize&#x3D;(9,9))</span><br><span class="line">sns.heatmap(corrmat,vmax&#x3D;.8,square&#x3D;True) #热力图颜色取值的最大值,square：bool类型参数，是否使热力图的每个单元格为正方形，默认为Falseaxis&#x3D;1)</span><br></pre></td></tr></table></figure>

<p>注意到其中的四个特征，TotalBsmtSF,1stFirSF,GarageCars,GarageArea与价格都有很强的正相关关系。还有其他的一些白点，代表这两个特征强相关。</p>
<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C9.png" alt></p>
<p>将前10个与价格具有强相关关系的特征挑选出来进行分析：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">k &#x3D; 10 #number of variables for heatmap</span><br><span class="line">#DataFrame.nlargest()以SalePrice为标准选出前十行,就可以选出与价格关系最为密切的前9个特征，不包括价格本身</span><br><span class="line">cols&#x3D;corrmat.nlargest(k, &#39;SalePrice&#39;)[&#39;SalePrice&#39;].index </span><br><span class="line">cm&#x3D;data1[cols].corr()</span><br><span class="line">#plt.figure(figsize&#x3D;(8,8))</span><br><span class="line">sns.set(font_scale&#x3D;1)</span><br><span class="line">sns.heatmap(cm,annot&#x3D;True)#annot显示系数</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C10.png" alt></p>
<p>将价格与其相关特征（刚才的9个，为避免重复性表达丢弃GarageArea,1stFlrSF,TotRmsAbvGrd）的关系进行汇点。</p>
<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C11.png" alt></p>
<p>之前的分析已经大致了解了一些信息，上图值得注意的是GrlLivArea和TotalBsmt之间似乎存在线性关系，以及年份与价格之间似乎存在的指数关系。</p>
<h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><p>在前面的‘样本信息’中用data.info()我们大概知道哪些特征值缺失比较多。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">null&#x3D;data.isnull().sum().sort_values(ascending&#x3D;False) #缺失数据较多的特征排序</span><br><span class="line">percentage&#x3D;a&#x2F;data.Id.count()</span><br><span class="line">pd.concat([null,percentage],axis&#x3D;1,keys&#x3D;[&#39;null&#39;,&#39;percentage&#39;]).head(20)</span><br></pre></td></tr></table></figure>



<img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/12.png" style="zoom:50%;">

<p>当缺失比例大于15%时，应当舍弃这一特征。（不绝对，这里缺失值多的特征又正好都是不重要的）</p>
<p>剩下的特征，除electrical（只缺失一个，且没有与其他特征有强相关关系）外都可舍弃。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data&#x3D;data.drop([&#39;PoolQC&#39;,&#39;MiscFeature&#39;,&#39;Alley&#39;,&#39;Fence&#39;,&#39;FireplaceQu&#39;,&#39;LotFrontage&#39;,&quot;GarageCond&quot;,&#39;GarageType&#39;,&#39;GarageYrBlt&#39;,&#39;GarageFinish&#39;,&#39;GarageQual&#39;,&#39;BsmtExposure&#39;,&#39;BsmtFinType2&#39;,&#39;BsmtFinType1&#39;,&#39;BsmtCond&#39;,&#39;BsmtQual&#39;,&#39;MasVnrArea&#39;,&#39;MasVnrType&#39;],axis&#x3D;1)</span><br><span class="line"></span><br><span class="line">data&#x3D;data.dropna(axis&#x3D;0,subset&#x3D;[&#39;Electrical&#39;])</span><br></pre></td></tr></table></figure>

<h3 id="使价格标准化"><a href="#使价格标准化" class="headerlink" title="使价格标准化"></a>使价格标准化</h3><p>将样本的价格（几万到十几万）进行标准化：</p>
<p>现在价格范围从-1.到7.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">#fit_transform()先拟合数据，然后将其转化为标准形式，一般应用在训练集中。</span><br><span class="line">#fit_transform(array of shape[n_samples,n_features]) </span><br><span class="line">saleprice_scaled &#x3D; StandardScaler().fit_transform(data1[&#39;SalePrice&#39;].values.reshape(data1.SalePrice.count(),1))</span><br><span class="line">saleprice_scaled[saleprice_scaled.reshape(len(saleprice_scaled)).argsort()][-10:,:] #将价格数据标准化</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[3.82758058],</span><br><span class="line">       [4.0395221 ],</span><br><span class="line">       [4.49473628],</span><br><span class="line">       [4.70872962],</span><br><span class="line">       [4.728631  ],</span><br><span class="line">       [5.06034585],</span><br><span class="line">       [5.42191907],</span><br><span class="line">       [5.58987866],</span><br><span class="line">       [7.10041987],</span><br><span class="line">       [7.22629831]])</span><br></pre></td></tr></table></figure>

<h3 id="消除异常点"><a href="#消除异常点" class="headerlink" title="消除异常点"></a>消除异常点</h3><p>我们主要处理GrLivArea和TotalBsmtSF这两个特征，这两个特征重要，且值都比较大。而其他的重要特征如车位数量由于值本来就比较小，不需要再处理。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#bivariate analysis saleprice&#x2F;grlivarea</span><br><span class="line">var &#x3D; &#39;GrLivArea&#39;</span><br><span class="line">data &#x3D; pd.concat([data1[&#39;SalePrice&#39;], data1[var]], axis&#x3D;1)</span><br><span class="line">data.plot.scatter(x&#x3D;var, y&#x3D;&#39;SalePrice&#39;, ylim&#x3D;(0,800000))</span><br></pre></td></tr></table></figure>

<p>右下角可以看到有两个点明显偏离了正常的趋势，可以单独去看看这两个点的具体情况。</p>
<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C13.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#删除右下两个异常点</span><br><span class="line">data1&#x3D;data1.drop(data1[data1.GrLivArea&gt;4500].index,axis&#x3D;0) #.drop([index名],axis)</span><br></pre></td></tr></table></figure>

<h3 id="使特征标准化"><a href="#使特征标准化" class="headerlink" title="使特征标准化"></a>使特征标准化</h3><p>特征标准化后会更符合正太分布。</p>
<p>查看价格的分布曲线</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from scipy import stats</span><br><span class="line">plt.xlabel(&#39;SalePrice&#39;)</span><br><span class="line">plt.hist(data1.SalePrice,80,density&#x3D;True)</span><br><span class="line">fig&#x3D;plt.figure()</span><br><span class="line">stats.probplot(data1.SalePrice,plot&#x3D;plt)#越贴合红线越符合正太分布</span><br><span class="line">#plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C14.png" alt></p>
<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C15.png" alt></p>
<p>用log函数使其更加符合正太分布</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data1.SalePrice&#x3D;np.log(data1.SalePrice)</span><br><span class="line">stats.probplot(data1.SalePrice,plot&#x3D;plt)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C16.png" alt></p>
<p>查看GrLivArea的分布曲线</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#histogram and normal probability plot</span><br><span class="line">plt.hist(data1.GrLivArea,50)</span><br><span class="line">plt.figure()</span><br><span class="line">stats.probplot(data1.GrLivArea,plot&#x3D;plt)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C17.png" alt></p>
<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C18.png" alt></p>
<p>用log函数使其更加符合正太分布</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data1.GrLivArea&#x3D;np.log(data1.GrLivArea)</span><br><span class="line">stats.probplot(data1.GrLivArea,plot&#x3D;plt)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C19.png" alt></p>
<p>查看TotalBsmtSF的分布曲线</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#histogram and normal probability plot</span><br><span class="line">plt.hist(data1.TotalBsmtSF,50)</span><br><span class="line">plt.figure()</span><br><span class="line">stats.probplot(data1.TotalBsmtSF,plot&#x3D;plt)</span><br></pre></td></tr></table></figure>

<p><img src alt>)<img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C20.png" alt="20"></p>
<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C21.png" alt="20"></p>
<p>用log函数使其更加符合正太分布，但注意这里有一些样本TotalBsmtSF是0，在用log之前应先省略它们因为没有log(0)。(可以用np.log1p(),这样就不用考虑0的情况)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#先创建新列，使得地下室面积大于0的都为1，新列就变成了0，1类。</span><br><span class="line">data1[&#39;HasBsmt&#39;]&#x3D;data1.TotalBsmtSF</span><br><span class="line">data1.loc[data1.HasBsmt&gt;0,&#39;HasBsmt&#39;]&#x3D;1</span><br><span class="line">#转换TotalBsmtSF列中非零的数据</span><br><span class="line">data1.loc[data1.HasBsmt&#x3D;&#x3D;1,&#39;TotalBsmtSF&#39;]&#x3D;np.log(data1.TotalBsmtSF)</span><br><span class="line">data1.TotalBsmtSF</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">0       6.752270</span><br><span class="line">1       7.140453</span><br><span class="line">2       6.824374</span><br><span class="line">3       6.628041</span><br><span class="line">4       7.043160</span><br><span class="line">          ...   </span><br><span class="line">1455    6.859615</span><br><span class="line">1456    7.340836</span><br><span class="line">1457    7.049255</span><br><span class="line">1458    6.982863</span><br><span class="line">1459    7.135687</span><br></pre></td></tr></table></figure>

<p>看一下对数操作之后的效果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats.probplot(data1[data1.TotalBsmtSF&gt;0].TotalBsmtSF,plot&#x3D;plt) #不考虑没有地下室的样本</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C22.png" alt></p>
<p>经过把前面的价格、居住面积和地下室面积标准化以及消除异常点后，居住面积与价格线性关系更加明显：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#scatter plot</span><br><span class="line">data1[[&#39;GrLivArea&#39;,&#39;SalePrice&#39;]].plot.scatter(x&#x3D;&#39;GrLivArea&#39;,y&#x3D;&#39;SalePrice&#39;)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C23.png" alt></p>
<p>同样地下室面积与价格线性关系更加明显：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#scatter plot（没有撇去0样本，撇去看效果更好）</span><br><span class="line">data1[[&#39;TotalBsmtSF&#39;,&#39;SalePrice&#39;]].plot.scatter(x&#x3D;&#39;TotalBsmtSF&#39;,y&#x3D;&#39;SalePrice&#39;)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C24.png" alt></p>
<p>到目前为止，我们分析和处理的都是数字类型的特征，而没有处理字符串型的。我们可以用map函数把类型映射成数字，再进行分析筛选，但需要操作的特征有43个。</p>
<p>用get_dummies()将字符串型的特征都转换为数字：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#convert categorical variable into dummy</span><br><span class="line">pd.get_dummies(data1)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5C25.png" alt></p>
<h2 id="步骤总结"><a href="#步骤总结" class="headerlink" title="步骤总结"></a>步骤总结</h2><h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>上面是针对训练集样本进行的特征工程细节（不完整）。而我们还要处理测试集，下面是我们将两者一起处理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data&#x3D;pd.read_csv(&#39;train.csv&#39;)</span><br><span class="line">test&#x3D;pd.read_csv(&#39;test.csv&#39;)</span><br><span class="line">all&#x3D;[data,test]</span><br></pre></td></tr></table></figure>

<p>步骤1：将训练集特征注释和样本大致浏览一遍，做个excel表，记录这些特征是n型还是c型，重不重要，样本值缺失得多不多…</p>
<p>看看价格的分布情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.hist(data.SalePrice,40)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#skewness and kurtosis</span><br><span class="line">print(&quot;Skewness: %f&quot; % data[&#39;SalePrice&#39;].skew())</span><br><span class="line">print(&quot;Kurtosis: %f&quot; % data[&#39;SalePrice&#39;].kurt())</span><br></pre></td></tr></table></figure>

<p>步骤2：有些object型特征是按等级排序的，可以转化为int型（用 map({}) or replace([],[]))，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">for i in all:</span><br><span class="line">    dic1&#x3D;&#123; &#39;Ex&#39;:5,&#39;Gd&#39;:4,&#39;TA&#39;:3,&#39;Fa&#39;:2,&#39;Po&#39;:1&#125;</span><br><span class="line">    i.ExterQual&#x3D;i.ExterQual.map(dic1)</span><br><span class="line">    i.ExterCond&#x3D;i.ExterCond.map(dic1)</span><br><span class="line">    dic2&#x3D;&#123; &#39;Ex&#39;:6,&#39;Gd&#39;:5,&#39;TA&#39;:4,&#39;Fa&#39;:3,&#39;Po&#39;:2,&#39;NA&#39;:1&#125;</span><br><span class="line">    i.BsmtCond&#x3D;i.BsmtCond.map(dic2)</span><br><span class="line">    i.BsmtQual&#x3D;i.BsmtQual.map(dic2)</span><br><span class="line">    dic3&#x3D;&#123; &#39;Gd&#39;:5,&#39;Av&#39;:4,&#39;Mn&#39;:3,&#39;No&#39;:2,&#39;NA&#39;:1&#125;</span><br><span class="line">    i.BsmtExposure&#x3D;i.BsmtExposure.map(dic3)</span><br><span class="line">    dic4&#x3D;&#123; &#39;GLQ&#39;:7,&#39;ALQ&#39;:6,&#39;BLQ&#39;:5,&#39;Rec&#39;:4,&#39;LwQ&#39;:3,&#39;Unf&#39;:2,&#39;NA&#39;:1&#125;</span><br><span class="line">    i.BsmtFinType1&#x3D;i.BsmtFinType1.map(dic4)</span><br><span class="line">    i.BsmtFinType2&#x3D;i.BsmtFinType2.map(dic4)</span><br><span class="line">    i.HeatingQC&#x3D;i.HeatingQC.map(dic1)</span><br><span class="line">    i.KitchenQual&#x3D;i.KitchenQual.map(dic1)</span><br><span class="line">    i.FireplaceQu&#x3D;i.FireplaceQu.map(dic2)</span><br><span class="line">    dic5&#x3D;&#123; &#39;Fin&#39;:4,&#39;RFn&#39;:3,&#39;Unf&#39;:2,&#39;NA&#39;:1&#125;</span><br><span class="line">    i.GarageFinish&#x3D;i.GarageFinish.map(dic5)</span><br><span class="line">    i.GarageQual&#x3D;i.GarageQual.map(dic2)</span><br><span class="line">    i.GarageCond&#x3D;i.GarageCond.map(dic2)</span><br><span class="line">    dic6&#x3D;&#123; &#39;Ex&#39;:5,&#39;Gd&#39;:4,&#39;TA&#39;:3,&#39;Fa&#39;:2,&#39;NA&#39;:1&#125;</span><br><span class="line">    i.PoolQC&#x3D;i.PoolQC.map(dic6)</span><br><span class="line">    dic7&#x3D;&#123; &#39;GdPrv&#39;:5,&#39;MnPrv&#39;:4,&#39;GdWo&#39;:3,&#39;MnWw&#39;:2,&#39;NA&#39;:1&#125;</span><br><span class="line">    i.Fence&#x3D;i.Fence.map(dic7)</span><br></pre></td></tr></table></figure>



<p>而有些numerical型特征其实是categorical型，转换为字符串更好，有利于后面的热图分析（操作中没有添加这一步）（事实证明这一步操作进一步提升了竞赛分数–0.12307）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i in all:</span><br><span class="line">	i[&#39;MSSubClass&#39;] &#x3D; i[&#39;MSSubClass&#39;].apply(str)</span><br><span class="line">	i[&#39;YrSold&#39;] &#x3D; i[&#39;YrSold&#39;].astype(str)</span><br><span class="line">	i[&#39;MoSold&#39;] &#x3D; i[&#39;MoSold&#39;].astype(str)</span><br></pre></td></tr></table></figure>



<p>对于训练集，查看输出price和其他特征的相关性。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.heatmap(DataFrame.corr())</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">k &#x3D; 10 #number of variables for heatmap</span><br><span class="line">cols&#x3D;corrmat.nlargest(k, &#39;SalePrice&#39;)[&#39;SalePrice&#39;].index #以SalePrice为标准选出前十行,就可以选出与价格关系最为密切的前10个特征</span><br><span class="line">cm&#x3D;data[cols].corr()</span><br><span class="line">sns.set(font_scale&#x3D;1)</span><br><span class="line">sns.heatmap(cm,annot&#x3D;True)#annot显示系数</span><br></pre></td></tr></table></figure>

<p>步骤3：对于相关性高的那几个特征（numerical型），单独与价格画散点图进行分析，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.xlabel(&#39;GrLivArea&#39;)</span><br><span class="line">plt.ylabel(&#39;SalePrice&#39;)</span><br><span class="line">plot.scatter(data.GrLivArea,data.SalePrice)</span><br></pre></td></tr></table></figure>

<p>如果有异常值就删除。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.hist(data.GrLivArea,50)</span><br><span class="line">plt.figure()</span><br><span class="line">stats.probplot(data.GrLivArea,plot&#x3D;plt)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#删除右下两个异常点</span><br><span class="line">data&#x3D;data.drop(data[data.GrLivArea&gt;4500].index,axis&#x3D;0) #.drop([index名],axis)</span><br></pre></td></tr></table></figure>

<p>而对于我们认为重要的categorical型特征，则可以用箱图分析与价格之间的关系，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df &#x3D; data[[&#39;OverallQual&#39;,&#39;SalePrice&#39;]]</span><br><span class="line">fig&#x3D;sns.boxplot(x&#x3D;&#39;OverallQual&#39;,y&#x3D;&#39;SalePrice&#39;,data&#x3D;df)</span><br><span class="line">fig.axis(ymin&#x3D;0,ymax&#x3D;800000)</span><br></pre></td></tr></table></figure>

<p>此后，由于我们这里用线性回归的方法，还需要对特征进行归一化处理。先处理缺失值：</p>
<p>步骤4：处理缺失值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">null&#x3D;data.isnull().sum().sort_values(ascending&#x3D;False) #缺失数据较多的特征排序</span><br><span class="line">percentage&#x3D;null&#x2F;data.Id.count()</span><br><span class="line">pd.concat([null,percentage],axis&#x3D;1,keys&#x3D;[&#39;null&#39;,&#39;percentage&#39;]).head(20)</span><br></pre></td></tr></table></figure>

<p>如果一些特征缺失值太多且不重要（比如缺失比例超过15%），就把那些列全删掉。缺失不多但不重要或表达的意思与其他特征重复，也可以删掉。通过计算（前文有详细说明）这里我们删掉了这些列：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data&#x3D;data.drop([&#39;PoolQC&#39;,&#39;MiscFeature&#39;,&#39;Alley&#39;,&#39;Fence&#39;,&#39;FireplaceQu&#39;,&#39;LotFrontage&#39;,&quot;GarageCond&quot;,&#39;GarageType&#39;,&#39;GarageYrBlt&#39;,&#39;GarageFinish&#39;,&#39;GarageQual&#39;,&#39;BsmtExposure&#39;,&#39;BsmtFinType2&#39;,&#39;BsmtFinType1&#39;,&#39;BsmtCond&#39;,&#39;BsmtQual&#39;,&#39;MasVnrArea&#39;,&#39;MasVnrType&#39;],axis&#x3D;1)</span><br></pre></td></tr></table></figure>

<p>Electrical列只缺失了一个，我们把可以把那个样本删掉：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data&#x3D;data.dropna(axis&#x3D;0,subset&#x3D;[&#39;Electrical&#39;])</span><br></pre></td></tr></table></figure>

<p>当然具体问题具体分析，有时候可以用中位数或平均数去填补缺失值。</p>
<p>而对于训练集，要删除同样的列</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test&#x3D;test.drop([&#39;PoolQC&#39;,&#39;MiscFeature&#39;,&#39;Alley&#39;,&#39;Fence&#39;,&#39;FireplaceQu&#39;,&#39;LotFrontage&#39;,&quot;GarageCond&quot;,&#39;GarageType&#39;,&#39;GarageYrBlt&#39;,&#39;GarageFinish&#39;,&#39;GarageQual&#39;,&#39;BsmtExposure&#39;,&#39;BsmtFinType2&#39;,&#39;BsmtFinType1&#39;,&#39;BsmtCond&#39;,&#39;BsmtQual&#39;,&#39;MasVnrArea&#39;,&#39;MasVnrType&#39;],axis&#x3D;1)</span><br></pre></td></tr></table></figure>

<p>现在我们再查看测试集的缺失值，已经很少了，填补剩下的那些缺失值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.isnull().sum().sort_values(ascending&#x3D;False).head(10) #查看测试集的缺失值</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test&#x3D;test.fillna(test.median()) #用中位数填补数字型</span><br><span class="line">test&#x3D;test.fillna(test.mode().loc[0,:]) #再用频数填补object型</span><br></pre></td></tr></table></figure>



<p>步骤5：：对一些特征可以进行简化，例如（这里省略了这一步骤）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># data[&quot;SimplOverallQual&quot;] &#x3D; data.OverallQual.map(&#123;1 : 1, 2 : 1, 3 : 1, # bad</span><br><span class="line">#                                                        4 : 2, 5 : 2, 6 : 2, # average</span><br><span class="line">#                                                        7 : 3, 8 : 3, 9 : 3, 10 : 3 # good</span><br><span class="line">#                                                       &#125;)</span><br></pre></td></tr></table></figure>

<p>步骤6：归一化</p>
<p>因为线性回归模型对正态分布的数据效果更好。</p>
<p>对于值很大（偏差大于0.75）的特征，进行归一化处理，例如价格，居住面积等：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#stats.probplot(data.SalePrice,plot&#x3D;plt)#归一化前，</span><br><span class="line">#data.SalePrice&#x3D;np.log1p(data1.SalePrice)</span><br><span class="line">#stats.probplot(data.SalePrice,plot&#x3D;plt)#归一化后，越贴近红线越接近正太分布</span><br></pre></td></tr></table></figure>

<p>这里我们将两个样本集中特征值偏差大于0.75的进行归一化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#把所有偏差大于0.75的特征进行归一化，使这些值落到更小的范围，并更符合正太分布，以便后面的模型训练</span><br><span class="line"></span><br><span class="line">#对训练集</span><br><span class="line">numeric_feats &#x3D; data.dtypes[data.dtypes !&#x3D; &quot;object&quot;].index</span><br><span class="line">for i in numeric_feats:</span><br><span class="line">    if data[i].skew()&gt;&#x3D;0.75:</span><br><span class="line">        data[i]&#x3D;np.log1p(data[i]) #log1p可以对0进行对数操作</span><br><span class="line">#对样本集</span><br><span class="line">numeric_feats &#x3D; test.dtypes[test.dtypes !&#x3D; &quot;object&quot;].index</span><br><span class="line">for i in numeric_feats:</span><br><span class="line">    if test[i].skew()&gt;&#x3D;0.75:</span><br><span class="line">        test[i]&#x3D;np.log1p(test[i]) #log1p可以对0进行对数操作</span><br><span class="line">all&#x3D;[data,test]</span><br></pre></td></tr></table></figure>

<p>步骤7：创建新特征</p>
<p>可以结合现有特征(如一个正相关的特征*一个负相关的特征)，或者直接创建新特征：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for i in all:</span><br><span class="line">    i[&quot;OverallGrade&quot;] &#x3D; i[&quot;OverallQual&quot;] * i[&quot;OverallCond&quot;]</span><br><span class="line">    i[&quot;GarageCar-Area&quot;] &#x3D; i[&quot;GarageCars&quot;] * i[&quot;GarageArea&quot;]</span><br><span class="line">    i[&quot;OverallQual-s2&quot;] &#x3D; i[&quot;OverallQual&quot;] * 2</span><br><span class="line">    i[&quot;OverallQual-s3&quot;] &#x3D; i[&quot;OverallQual&quot;] * 3</span><br></pre></td></tr></table></figure>

<p>步骤8：将所有c型转换为n型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">all&#x3D;pd.concat((data,test),0)</span><br><span class="line">all&#x3D;pd.get_dummies(all)</span><br><span class="line">del all[&#39;Id&#39;]</span><br><span class="line">train_x&#x3D;all[:1459]</span><br><span class="line">test_x&#x3D;all[-1459:]</span><br></pre></td></tr></table></figure>

<p>删除价格列以及dummy之后测试集多出来的价格列</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_y&#x3D;data.SalePrice</span><br><span class="line">del train_x[&#39;SalePrice&#39;]</span><br><span class="line">del test_x[&#39;SalePrice&#39;]</span><br></pre></td></tr></table></figure>

<h3 id="模型选取"><a href="#模型选取" class="headerlink" title="模型选取"></a>模型选取</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#根据交叉验证集均方根误差,对每个模型进行评分</span><br><span class="line">n_folds&#x3D;10</span><br><span class="line">def rmsle_cv(model):</span><br><span class="line">    kf&#x3D;KFold(n_folds,shuffle&#x3D;True,random_state&#x3D;1).get_n_splits(train_x)</span><br><span class="line">    rmse&#x3D;np.sqrt(-cross_val_score(model,train_x,train_y,scoring&#x3D;&quot;neg_mean_squared_error&quot;, cv &#x3D; kf))</span><br><span class="line">    return rmse</span><br></pre></td></tr></table></figure>

<h4 id="Single-Model"><a href="#Single-Model" class="headerlink" title="Single Model"></a>Single Model</h4><p>下面一共用了7个模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#lasso对异常值很敏感，因此可以先用RobustScaler()消除异常值</span><br><span class="line">lassoCV&#x3D;make_pipeline(RobustScaler(),LassoCV(max_iter&#x3D;50000,eps&#x3D;0.001, n_alphas&#x3D;100, alphas&#x3D;None,random_state&#x3D;2))</span><br><span class="line">ElasticNetCV&#x3D;make_pipeline(RobustScaler(),ElasticNetCV(eps&#x3D;0.001, n_alphas&#x3D;100, alphas&#x3D;None,l1_ratio&#x3D;.9,random_state&#x3D;3))</span><br><span class="line">RidgeCV&#x3D;make_pipeline(RobustScaler(),RidgeCV())</span><br><span class="line">KRR &#x3D; KernelRidge(alpha&#x3D;0.6, kernel&#x3D;&#39;polynomial&#39;, degree&#x3D;2, coef0&#x3D;2.5)</span><br></pre></td></tr></table></figure>

<p>其中3个梯度增强决策树模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#使用huber loss使GBoost模型对异常值更不敏感</span><br><span class="line">GBoost &#x3D; GradientBoostingRegressor(n_estimators&#x3D;3000, learning_rate&#x3D;0.05,</span><br><span class="line">                                   max_depth&#x3D;4, max_features&#x3D;&#39;sqrt&#39;,</span><br><span class="line">                                   min_samples_leaf&#x3D;15, min_samples_split&#x3D;10, </span><br><span class="line">                                   loss&#x3D;&#39;huber&#39;, random_state &#x3D;5)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#XGBoost :</span><br><span class="line">model_xgb &#x3D; xgb.XGBRegressor(random_state &#x3D;6)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#LightGBM :</span><br><span class="line">model_lgb &#x3D; lgb.LGBMRegressor(objective&#x3D;&#39;regression&#39;)</span><br></pre></td></tr></table></figure>

<p>评分（用来选择模型）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score &#x3D; rmsle_cv(lassoCV)</span><br><span class="line">print(&quot;\nLasso score: &#123;:.4f&#125; (&#123;:.4f&#125;)\n&quot;.format(score.mean(), score.std()))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Lasso score: 0.1222 (0.0248)</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score &#x3D; rmsle_cv(ElasticNetCV)</span><br><span class="line">print(&quot;\nElasticNet score: &#123;:.4f&#125; (&#123;:.4f&#125;)\n&quot;.format(score.mean(), score.std()))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ElasticNet score: 0.1219 (0.0249)</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score &#x3D; rmsle_cv(RidgeCV)</span><br><span class="line">print(&quot;\nRidge score: &#123;:.4f&#125; (&#123;:.4f&#125;)\n&quot;.format(score.mean(), score.std()))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ridge score: 0.1237 (0.0237)</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score &#x3D; rmsle_cv(KRR)</span><br><span class="line">print(&quot;\nKRR score: &#123;:.4f&#125; (&#123;:.4f&#125;)\n&quot;.format(score.mean(), score.std()))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KRR score: 0.1905 (0.0519)</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score &#x3D; rmsle_cv(GBoost)</span><br><span class="line">print(&quot;\nGBoost score: &#123;:.4f&#125; (&#123;:.4f&#125;)\n&quot;.format(score.mean(), score.std()))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GBoost score: 0.1208 (0.0203)</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score &#x3D; rmsle_cv(model_xgb)</span><br><span class="line">print(&quot;\nmodel_xgb score: &#123;:.4f&#125; (&#123;:.4f&#125;)\n&quot;.format(score.mean(), score.std()))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_xgb score: 0.1346 (0.0204)</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score &#x3D; rmsle_cv(model_lgb)</span><br><span class="line">print(&quot;\nmodel_lgb score: &#123;:.4f&#125; (&#123;:.4f&#125;)\n&quot;.format(score.mean(), score.std()))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_lgb score: 0.1304 (0.0194)</span><br></pre></td></tr></table></figure>

<p>上述7个模型中，可以看到除了KRR评分明显比较低之外，其余几个差别不大。</p>
<h4 id="Averaged-Model"><a href="#Averaged-Model" class="headerlink" title="Averaged Model"></a>Averaged Model</h4><p>下面我们用6个模型（除去KRR，实践证明添加KRR评分变得更低了）的平均值来作为一个新模型，增强预测能力。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#定义这个新的模型类</span><br><span class="line"></span><br><span class="line">class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):</span><br><span class="line">    def __init__(self, models):</span><br><span class="line">        self.models &#x3D; models</span><br><span class="line">        </span><br><span class="line">    # we define clones of the original models to fit the data in</span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        self.models_ &#x3D; [clone(x) for x in self.models]</span><br><span class="line">        </span><br><span class="line">        # Train cloned base models</span><br><span class="line">        for model in self.models_:</span><br><span class="line">            model.fit(X, y)</span><br><span class="line"></span><br><span class="line">        return self</span><br><span class="line">    </span><br><span class="line">    #Now we do the predictions for cloned models and average them</span><br><span class="line">    def predict(self, X):</span><br><span class="line">        predictions &#x3D; np.column_stack([</span><br><span class="line">            model.predict(X) for model in self.models_</span><br><span class="line">        ])</span><br><span class="line">        return np.mean(predictions, axis&#x3D;1)</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#平均模型得分</span><br><span class="line">averaged_models &#x3D; AveragingModels(models &#x3D; (lassoCV, ElasticNetCV, RidgeCV,model_lgb,model_xgb,GBoost)) </span><br><span class="line"></span><br><span class="line">score &#x3D; rmsle_cv(averaged_models)</span><br><span class="line">print(&quot; Averaged base models score: &#123;:.4f&#125; (&#123;:.4f&#125;)\n&quot;.format(score.mean(), score.std()))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Averaged base models score: 0.1165 (0.0202)</span><br></pre></td></tr></table></figure>

<p>可以看出新模型的分数更高。</p>
<h4 id="Stacking-averaged-Model"><a href="#Stacking-averaged-Model" class="headerlink" title="Stacking averaged Model"></a>Stacking averaged Model</h4><p>新模型只是将单个模型的结果简单的平均了。下面引入一种stacking方法。</p>
<p>Stacking模型本质上是一种分层的结构，这里简单起见，只分析二级Stacking.假设我们有2个基模型 Model1_1、Model1_2 和 一个次级模型Model2</p>
<ol>
<li>基模型 Model1_1，对训练集train训练，然后用于预测 train 和 test 的标签列，分别是P1，T1。</li>
<li>基模型 Model1_2 ，对训练集train训练，然后用于预测train和test的标签列，分别是P2，T2。</li>
<li>分别把P1,P2以及T1,T2合并，得到一个新的训练集和测试集train2,test2。</li>
<li>再用 次级模型 Model2 以真实训练集标签为标签训练,以train2为特征进行训练，预测test2,得到最终的测试集预测的标签列。</li>
</ol>
<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5Cstacking2.png" alt></p>
<p><img src="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE%5Cstacking.png" alt></p>
<p>代码展示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#Stacking averaged Models Class </span><br><span class="line">#添加元模型</span><br><span class="line">class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):</span><br><span class="line">    def __init__(self, base_models, meta_model, n_folds&#x3D;5):</span><br><span class="line">        self.base_models &#x3D; base_models</span><br><span class="line">        self.meta_model &#x3D; meta_model</span><br><span class="line">        self.n_folds &#x3D; n_folds</span><br><span class="line">   </span><br><span class="line">    # We again fit the data on clones of the original models</span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        self.base_models_ &#x3D; [list() for x in self.base_models]</span><br><span class="line">        self.meta_model_ &#x3D; clone(self.meta_model)</span><br><span class="line">        kfold &#x3D; KFold(n_splits&#x3D;self.n_folds, shuffle&#x3D;True, random_state&#x3D;156)</span><br><span class="line">        </span><br><span class="line">        # Train cloned base models then create out-of-fold predictions</span><br><span class="line">        # that are needed to train the cloned meta-model</span><br><span class="line">        out_of_fold_predictions &#x3D; np.zeros((X.shape[0], len(self.base_models)))</span><br><span class="line">        for i, model in enumerate(self.base_models):</span><br><span class="line">            for train_index, holdout_index in kfold.split(X, y):</span><br><span class="line">                instance &#x3D; clone(model)</span><br><span class="line">                self.base_models_[i].append(instance)</span><br><span class="line">                instance.fit(X[train_index], y[train_index])</span><br><span class="line">                y_pred &#x3D; instance.predict(X[holdout_index])</span><br><span class="line">                out_of_fold_predictions[holdout_index, i] &#x3D; y_pred</span><br><span class="line">                </span><br><span class="line">        # Now train the cloned  meta-model using the out-of-fold predictions as new feature</span><br><span class="line">        self.meta_model_.fit(out_of_fold_predictions, y)</span><br><span class="line">        return self</span><br><span class="line">   </span><br><span class="line">    #Do the predictions of all base models on the test data and use the averaged predictions as </span><br><span class="line">    #meta-features for the final prediction which is done by the meta-model</span><br><span class="line">    def predict(self, X):</span><br><span class="line">        meta_features &#x3D; np.column_stack([</span><br><span class="line">            np.column_stack([model.predict(X) for model in base_models]).mean(axis&#x3D;1)</span><br><span class="line">            for base_models in self.base_models_ ])</span><br><span class="line">        return self.meta_model_.predict(meta_features)</span><br></pre></td></tr></table></figure>

<p>发现在用Stacking Averaged models时，只有将meta_model 设置为ElasticNetCV才不会出现‘’无法收敛‘’的提示。这里还不太清楚原因是什么。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#Stacking Averaged models的分数</span><br><span class="line">stacked_averaged_models &#x3D; StackingAveragedModels(base_models &#x3D; (GBoost,RidgeCV,model_lgb,model_xgb,lassoCV),#lassoCV,RidgeCV,,ElasticNetCV</span><br><span class="line">                                                 meta_model &#x3D; ElasticNetCV)</span><br><span class="line"></span><br><span class="line">score &#x3D; rmsle_cv(stacked_averaged_models)</span><br><span class="line">print(&quot;Stacking Averaged models score: &#123;:.4f&#125; (&#123;:.4f&#125;)&quot;.format(score.mean(), score.std()))</span><br><span class="line">#0.1173</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Stacking Averaged models score: 0.1173 (0.0229)</span><br></pre></td></tr></table></figure>



<h3 id="训练与测试"><a href="#训练与测试" class="headerlink" title="训练与测试"></a>训练与测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#定义拟合误差</span><br><span class="line">def rmsle(y, y_pred):</span><br><span class="line">    return np.sqrt(mean_squared_error(y, y_pred))</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#新模型的拟合误差，看起来似乎没有别的模型好</span><br><span class="line">averaged_models.fit(train_x.values,train_y)</span><br><span class="line">averaged_train_pred &#x3D; averaged_models.predict(train_x.values)</span><br><span class="line">averaged_pred &#x3D; np.expm1(averaged_models.predict(test_x.values))</span><br><span class="line">print(rmsle(train_y, averaged_train_pred))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.06887290976694459</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#XGBoost:</span><br><span class="line">model_xgb.fit(train_x,train_y)</span><br><span class="line">xgb_train_pred &#x3D; model_xgb.predict(train_x)</span><br><span class="line">xgb_pred &#x3D; np.expm1(model_xgb.predict(test_x))</span><br><span class="line">print(rmsle(train_y, xgb_train_pred))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.014272634107394586</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#LightGBM:</span><br><span class="line">model_lgb.fit(train_x,train_y)</span><br><span class="line">lgb_train_pred &#x3D; model_lgb.predict(train_x)</span><br><span class="line">lgb_pred &#x3D; np.expm1(model_lgb.predict(test_x))</span><br><span class="line">print(rmsle(train_y, lgb_train_pred))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.049314070591082906</span><br></pre></td></tr></table></figure>

<p>可以将之前那些模型的预测结果再进行组合：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#39;&#39;&#39;RMSE on the entire Train data when averaging&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">print(&#39;RMSLE score on train data:&#39;)</span><br><span class="line">print(rmsle(train_y,averaged_train_pred*0.3 +</span><br><span class="line">               xgb_train_pred*0.4 + lgb_train_pred*0.3 ))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RMSLE score on train data:</span><br><span class="line">0.03747221379925178</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ensemble&#x3D;averaged_pred*0.3 + xgb_pred*0.4 + lgb_pred*0.3</span><br><span class="line">ensemble</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([129516.9681314 , 161629.70202685, 179657.28537867, ...,</span><br><span class="line">       165973.18985545, 113995.11453978, 218326.37093474])</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#stacked_averaged_models的拟合误差，也很高</span><br><span class="line">stacked_averaged_models.fit(train_x.values,train_y.values)</span><br><span class="line">stack_train_pred&#x3D;stacked_averaged_models.predict(train_x.values)</span><br><span class="line">stack_pred&#x3D;np.expm1(stacked_averaged_models.predict(test_x.values))</span><br><span class="line">print(rmsle(train_y.values,stack_train_pred))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.07848317662351782</span><br></pre></td></tr></table></figure>

<p>上面两个拟合程度评分较低的模型反而是评分最高预测能力最好的，这说明拟合误差并不能作为评价模型唯一的标准，更重要的是交叉验证评分。</p>
<h3 id="结果保存"><a href="#结果保存" class="headerlink" title="结果保存"></a>结果保存</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result1&#x3D;pd.DataFrame()</span><br><span class="line">result1[&#39;Id&#39;]&#x3D;test.Id</span><br><span class="line">result1[&#39;SalePrice&#39;]&#x3D;ensemble</span><br><span class="line">result1.to_csv(&#39;result1.csv&#39;,index&#x3D;False)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result2&#x3D;pd.DataFrame()</span><br><span class="line">result2[&#39;Id&#39;]&#x3D;test.Id</span><br><span class="line">result2[&#39;SalePrice&#39;]&#x3D;xgb_pred</span><br><span class="line">result2.to_csv(&#39;result2.csv&#39;,index&#x3D;False)</span><br></pre></td></tr></table></figure>

<p>…</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#新模型</span><br><span class="line">result5&#x3D;pd.DataFrame()</span><br><span class="line">result5[&#39;Id&#39;]&#x3D;test.Id</span><br><span class="line">result5[&#39;SalePrice&#39;]&#x3D;averaged_pred</span><br><span class="line">result5.to_csv(&#39;result5.csv&#39;,index&#x3D;False)</span><br></pre></td></tr></table></figure>

<p>把这些结果提交到kaggle，发现得分第二高的就是那个平均后的新模型，排名TOP18.%</p>
<p>而Stacking averaged Models分数为0.12329（比新模型分数提升了0.001），排名TOP17.%</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#Stacking averaged Models</span><br><span class="line">result8&#x3D;pd.DataFrame()</span><br><span class="line">result8[&#39;Id&#39;]&#x3D;test.Id</span><br><span class="line">result8[&#39;SalePrice&#39;]&#x3D;stack_pred</span><br><span class="line">result8.to_csv(&#39;result8.csv&#39;,index&#x3D;False)</span><br></pre></td></tr></table></figure>



<p>机器学习知识</p>
<p><a href="https://www.cbedai.net/u011630575/" target="_blank" rel="noopener">https://www.cbedai.net/u011630575/</a></p>
<p>LightGBM(GBDT)</p>
<p><a href="https://zhuanlan.zhihu.com/p/99069186" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/99069186</a></p>
<p>XGBoost(GBDT)</p>
<p><a href="https://zhuanlan.zhihu.com/p/75217528" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/75217528</a></p>
<p>GBDT</p>
<p><a href="https://www.jianshu.com/p/005a4e6ac775" target="_blank" rel="noopener">https://www.jianshu.com/p/005a4e6ac775</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/" data-id="ckn1t0ign001rrgvtgg2vhrnn" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-YOLOv5-口罩与帽子识别" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/" class="article-date">
  <time datetime="2020-09-27T06:35:46.000Z" itemprop="datePublished">2020-09-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/">YOLOv5--口罩与帽子识别</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><p>yolov5的官方地址：<a href="https://github.com/ultralytics/yolov5" target="_blank" rel="noopener">https://github.com/ultralytics/yolov5</a>  ，其中包含了yolov5的使用教程。下载yolov5相关文件保存到D:\yolov5-master</p>
<p>yolov5根据用户的需求提供不同size的模型，将这些模型下载下来，放到models目录下。</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C3.png" alt></p>
<p>打开conda prompter，为这个项目创建一个新conda环境(yolov5要求python版本为3.8及以上)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n yolov5 python&#x3D;&#x3D;3.8</span><br></pre></td></tr></table></figure>

<p>等待一段时间后基础Python环境搭建完成，此时可以进入该环境继续安装依赖库：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate yolov5</span><br></pre></td></tr></table></figure>

<p>安装依赖库</p>
<p>根据<a href="https://github.com/ultralytics/yolov5" target="_blank" rel="noopener">YOLOv5 github</a>中的介绍，requirements.txt文件(存放在yolov5-master文件)中为所需要安装的依赖库。进入该文件路径D:\yolov5-master后：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(yolov5) D:\yolov5-master&gt;pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<p>输入pip install -r requirements.txt 安装yolo所需要的库。</p>
<p>（注意如果下载过程中出现pytorch版本问题则先去pytorch官网复制命令安装适合的pytorch）</p>
<p>放一张狗的图片在images目录下</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C2.png" alt></p>
<p>运行detect.py（只单独输出这一张，若要输出所有图片则直接python detect.py）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python detect.py --source inference&#x2F;images&#x2F;dog.jpg</span><br></pre></td></tr></table></figure>

<p>在output下查看输出</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C1.jpg" alt></p>
<p>能识别出人和狗，可以看到大神们开发的yolov5效果不错。</p>
<h2 id="训练自己的数据集"><a href="#训练自己的数据集" class="headerlink" title="训练自己的数据集"></a>训练自己的数据集</h2><p>目标–能够识别人脸，口罩和帽子</p>
<p>收集一些有口罩和帽子的照片，下载labelmage进行标注，标注时我将0,1,2分别代表mask,hat,face，生成txt格式（注意要切换到yolo模式，否则会生成xml格式的标签）的标签存放到指定路径（标签生成后可以看到还自动生成了一个名为class的txt，里面表明了三个类别，删掉即可）。训练集测试集交叉验证集可以按照6：2：2的比例存放（it’s up to you）。</p>
<p>三种样本存放在yolov5-master，新建一个名为MaskHatFace的文件夹中。</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C4.png" alt></p>
<p>设置data.yaml里的路径，类别数和类别名。</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C6.png" alt></p>
<p>标签与图片分开，标签名要与图片名一致。我这里顺序也一一对应了，避免麻烦。（可以试试顺序不一样能不能训练）</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C5.png" alt></p>
<p>先用小模型训练看看，由于有三类，在对应的yolo5s.yaml文件中将nc参数修改为3。</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C7.png" alt></p>
<p>现在可以开始训练了。</p>
<p>由于我的小破电脑显卡只有2G，batchsize我只设置了5个。yolov5的官方GitHub里提到可以用谷歌的colab或者kaggle平台的GPU，都是免费的。</p>
<p>训练时运行train.py,其他的一些如epochs参数如下可以在运行时写上，也可以在train.py中修改。关于有哪些参数可以修改网上有很多介绍，不再累述。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py --img 640 --batch 5 --epochs 300 --data .&#x2F;MaskHatFace&#x2F;data.yaml --cfg models&#x2F;yolov5s.yaml --weights &#39;&#39;</span><br></pre></td></tr></table></figure>

<p>训练中…</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C2.jpg" alt></p>
<p>训练结束后，在runs文件中生成结果，weights文件保存best.pt和last.pt，分别代表最好的一次模型和最后一次生成的模型。</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C8.png" alt></p>
<p>测试</p>
<p>用生成的那个最好模型进行测试。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python detect.py --weight .&#x2F;runs&#x2F;exp27&#x2F;weights&#x2F;best.pt --source .&#x2F;MaskHatFace&#x2F;test&#x2F;images&#x2F;onlyfortest.jpg</span><br></pre></td></tr></table></figure>

<p>输出结果保存在inference的output中。</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C3.jpg" alt></p>
<p>可以看到帽子和口罩都被识别出来了，但有识别错误。原因我的训练样本量很小，只有100多张。模型选择了一个小模型，复杂度低（非主要原因）。</p>
<h2 id="在服务器上训练帽子识别模型"><a href="#在服务器上训练帽子识别模型" class="headerlink" title="在服务器上训练帽子识别模型"></a>在服务器上训练帽子识别模型</h2><p>由于自己的笔记本无法带起大batch的训练，用了别人的服务器训练模型。训练的步骤如下：</p>
<p>把官方压缩包yolov5-master上传到服务器，命名一个wy文件夹存放（<a href="http://10.0.77.98:8888/tree/wy），在wy文件中新建一个python文档，输入以下代码进行解压" target="_blank" rel="noopener">http://10.0.77.98:8888/tree/wy），在wy文件中新建一个python文档，输入以下代码进行解压</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import zipfile</span><br><span class="line">import os</span><br><span class="line">files &#x3D; zipfile.ZipFile(&#39;yolov5-master.zip&#39;,&#39;r&#39;)</span><br><span class="line">files.extractall(os.getcwd())</span><br></pre></td></tr></table></figure>

<p>把安全帽的样本数据（网上下载，有1w张左右的照片）放到yolov5-master中</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C9.png" alt></p>
<p>按照之前的格式存放样本和标签。</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C10.png" alt></p>
<p>在yolov5-master文件中新建一个python文档</p>
<p>训练前打开terminal，查看服务器运行状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watch -n 1 nvidia-smi</span><br></pre></td></tr></table></figure>

<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C13.png" alt></p>
<p>开始训练(如果要使用第二张显卡)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!python train.py --img 640 --batch 20 --epochs 300 --data .&#x2F;hat&#x2F;data.yaml --cfg models&#x2F;yolov5s.yaml --weights &#39;&#39;--device 1</span><br></pre></td></tr></table></figure>

<p>训练中：</p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C11.png" alt></p>
<p><img src="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB%5C12.png" alt></p>
<p>训练之后进行测试发现有些场景识别帽子的准确度低，我有针对性的再标记了一些图片，注意标注数据时0，1对应的类别名需与之前的数据一致（0:no-hat  1:hat）。此后可以在原来的模型基础上进行训练，训练时添加一句weights信息，如之前的模型存在当前目录下，名为hat.pt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!python train.py  --weights &#39;.&#x2F;hat.pt&#39;</span><br></pre></td></tr></table></figure>

<p>还有一个点需要注意的是之前的模型是用1w多张图片训练出来的，我后面只标了近200张图片，第二次训练的epoch可以增加到400代这样模型修改的效果会明显些。事实证明确实如此。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/" data-id="ckn1t0igg001jrgvtako9d5hn" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Kaggle之猫狗识别项目(神经网络)" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/16/Kaggle%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE(%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)/" class="article-date">
  <time datetime="2020-09-16T05:52:57.000Z" itemprop="datePublished">2020-09-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/09/16/Kaggle%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE(%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)/">Kaggle之猫狗识别项目(神经网络)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="函数介绍"><a href="#函数介绍" class="headerlink" title="函数介绍"></a>函数介绍</h1><p>在后文的代码中涉及到几个函数，这里先记录一下它们的使用方法。</p>
<h2 id="ImageFolder-函数"><a href="#ImageFolder-函数" class="headerlink" title="ImageFolder()函数"></a>ImageFolder()函数</h2><p><strong><code>ImageFolder</code>是一个通用的数据加载器，它要求我们以下面这种格式来组织数据集的训练、验证或者测试图片。</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root/dog/xxx.png</span><br><span class="line">root/dog/xxy.png</span><br><span class="line">root/dog/xxz.png</span><br><span class="line"></span><br><span class="line">root/cat/123.png</span><br><span class="line">root/cat/nsdf3.png</span><br><span class="line">root/cat/asd932_.png</span><br></pre></td></tr></table></figure>

<h5 id="ImageFolder参数详解"><a href="#ImageFolder参数详解" class="headerlink" title="ImageFolder参数详解"></a>ImageFolder参数详解</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataset=torchvision.datasets.ImageFolder(</span><br><span class="line">                       root, transform=<span class="literal">None</span>, </span><br><span class="line">                       target_transform=<span class="literal">None</span>, </span><br><span class="line">                       loader=&lt;function default_loader&gt;, </span><br><span class="line">                       is_valid_file=<span class="literal">None</span>)</span><br><span class="line"><span class="number">12345</span></span><br></pre></td></tr></table></figure>

<p>参数详解：</p>
<ul>
<li>root：图片存储的根目录，即各类别文件夹所在目录的上一级目录。</li>
<li>transform：对图片进行预处理的操作（函数），原始图片作为输入，返回一个转换后的图片。</li>
<li>target_transform：对图片类别进行预处理的操作，输入为 target，输出对其的转换。如果不传该参数，即对 target 不做任何转换，返回的顺序索引 0,1, 2…</li>
<li>loader：表示数据集加载方式，通常默认加载方式即可。</li>
<li>is_valid_file：获取图像文件的路径并检查该文件是否为有效文件的函数(用于检查损坏文件)</li>
</ul>
<p>返回的dataset都有以下三种属性：</p>
<ul>
<li><code>self.classes</code>：用一个 list 保存类别名称</li>
<li><code>self.class_to_idx</code>：类别对应的索引，与不做任何转换返回的 target 对应</li>
<li><code>self.imgs</code>：保存(img-path, class) tuple的 list</li>
</ul>
<h2 id="imshow-函数"><a href="#imshow-函数" class="headerlink" title="imshow()函数"></a>imshow()函数</h2><p>matplotlib.pyplot.imshow()函数的输入需要是二维的numpy或者是第三维度是3或4的numpy，当第3维深度是1时，使用np.squeeze()函数压缩数据成为二维数组。在pytorch环境下使用，得到结果的输出是(batch_size,channel,width,height)的tensor，因此我首先需要detach()函数切断反向传播。需要指出的是，imshow不支持显示tensor，因此，我需要使用.cpu()函数转移到cpu上来。正如前面说到的，imshow函数的输入需要是二维的numpy或者第三维度是3或4的numpy，多了一个batch_size维度时，不过还好，我设置batch_size仅为1，这时候可以使用.squeeze()函数把1给去掉，得到了是一个(channel,widht,height)的numpy，这显然与imshow的输入要求不符。因此，我们需要使用transpose函数把channel(=3)移动到最后，这也是为什么才有了.transpose(1,2,0)这种用法。当然，如果待显示的图像本身就是channel=1，那么完全可以使用squeeze()函数把其搞掉，直接输入给imshow函数一个二维的numpy.</p>
<h2 id="transforms中的函数及作用"><a href="#transforms中的函数及作用" class="headerlink" title="transforms中的函数及作用"></a>transforms中的函数及作用</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">Resize</span>：把给定的图片<span class="selector-tag">resize</span>到<span class="selector-tag">given</span> <span class="selector-tag">size</span></span><br><span class="line"><span class="selector-tag">Normalize</span>：<span class="selector-tag">Normalized</span> <span class="selector-tag">an</span> <span class="selector-tag">tensor</span> <span class="selector-tag">image</span> <span class="selector-tag">with</span> <span class="selector-tag">mean</span> <span class="selector-tag">and</span> <span class="selector-tag">standard</span> <span class="selector-tag">deviation</span></span><br><span class="line"><span class="selector-tag">ToTensor</span>：<span class="selector-tag">convert</span> <span class="selector-tag">a</span> <span class="selector-tag">PIL</span> <span class="selector-tag">image</span> <span class="selector-tag">to</span> <span class="selector-tag">tensor</span> (<span class="selector-tag">H</span>*<span class="selector-tag">W</span>*<span class="selector-tag">C</span>) <span class="selector-tag">in</span> <span class="selector-tag">range</span> <span class="selector-attr">[0,255]</span> <span class="selector-tag">to</span> <span class="selector-tag">a</span> <span class="selector-tag">torch</span><span class="selector-class">.Tensor</span>(<span class="selector-tag">C</span>*<span class="selector-tag">H</span>*<span class="selector-tag">W</span>) <span class="selector-tag">in</span> <span class="selector-tag">the</span> <span class="selector-tag">range</span> <span class="selector-attr">[0.0,1.0]</span></span><br><span class="line"><span class="selector-tag">ToPILImage</span>: <span class="selector-tag">convert</span> <span class="selector-tag">a</span> <span class="selector-tag">tensor</span> <span class="selector-tag">to</span> <span class="selector-tag">PIL</span> <span class="selector-tag">image</span></span><br><span class="line"><span class="selector-tag">Scale</span>：目前已经不用了，推荐用<span class="selector-tag">Resize</span></span><br><span class="line"><span class="selector-tag">CenterCrop</span>：在图片的中间区域进行裁剪</span><br><span class="line"><span class="selector-tag">RandomCrop</span>：在一个随机的位置进行裁剪</span><br><span class="line"><span class="selector-tag">RandomHorizontalFlip</span>：以0<span class="selector-class">.5</span>的概率水平翻转给定的<span class="selector-tag">PIL</span>图像</span><br><span class="line"><span class="selector-tag">RandomVerticalFlip</span>：以0<span class="selector-class">.5</span>的概率竖直翻转给定的<span class="selector-tag">PIL</span>图像</span><br><span class="line"><span class="selector-tag">RandomResizedCrop</span>：将<span class="selector-tag">PIL</span>图像裁剪成任意大小和纵横比</span><br><span class="line"><span class="selector-tag">Grayscale</span>：将图像转换为灰度图像</span><br><span class="line"><span class="selector-tag">RandomGrayscale</span>：将图像以一定的概率转换为灰度图像</span><br><span class="line"><span class="selector-tag">FiceCrop</span>：把图像裁剪为四个角和一个中心</span><br><span class="line"><span class="selector-tag">Pad</span>：填充</span><br><span class="line"><span class="selector-tag">ColorJitter</span>：随机改变图像的亮度对比度和饱和度</span><br></pre></td></tr></table></figure>



<h1 id="文件存放"><a href="#文件存放" class="headerlink" title="文件存放"></a>文件存放</h1><p>将猫和狗的图片从Kaggle上下载下来（训练集有25000张图片，猫狗各一半。测试集有12500张图片。将两个文件夹解压）</p>
<p>（训练集）</p>
<p><img src="/2020/09/16/Kaggle%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE(%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE%5C1.png" alt></p>
<p>需要按照ImageFolder要求的路径格式存储在我的本地路径里，由于下载下来的训练集猫和狗是放在一个文件夹的，我需要把它们分开存放。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">将训练图片复制到两个文件夹</span><br><span class="line">import os</span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line">def choseCopy(folder, targetFolder):</span><br><span class="line">    folder &#x3D; os.path.abspath(folder)</span><br><span class="line">    targetFolder &#x3D; os.path.abspath(targetFolder)</span><br><span class="line">    if not os.path.exists(targetFolder):</span><br><span class="line">        os.makedirs(targetFolder)</span><br><span class="line"></span><br><span class="line">    for root, dirs, files in os.walk(folder):</span><br><span class="line">        for file in files:</span><br><span class="line">            if file.startswith(&#39;dog&#39;):</span><br><span class="line">                shutil.copy(os.path.join(root, file), targetFolder)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">choseCopy(&#39;D:&#x2F;jupyter_project&#x2F;dog&amp;cat&#x2F;train&#39;, &#39;D:&#x2F;jupyter_project&#x2F;dog&amp;cat&#x2F;train_image&#x2F;dog&#39;)</span><br><span class="line">#修改一下上述代码将猫的图片也复制到我的指定路径</span><br></pre></td></tr></table></figure>

<p>现在两个文件中各自有12500张图片。</p>
<p><img src="/2020/09/16/Kaggle%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE(%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE%5C2.png" alt></p>
<p>为了在训练时测试训练的效果，再建一个用于交叉验证的测试集，我从训练集猫狗图片中各自剪切了20张到测试集。（其实应该要更多，但我想让代码快一点计算测试集准确度，先让代码先跑成功，后面再修改）</p>
<p><img src="/2020/09/16/Kaggle%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE(%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE%5C3.png" alt></p>
<h1 id="图片处理"><a href="#图片处理" class="headerlink" title="图片处理"></a>图片处理</h1><p>训练集的图片稍微进行点加工变形，增加算法的鲁棒性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">BATCH_SIZE=<span class="number">5</span></span><br><span class="line"><span class="comment">#LR=0.02</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#ToTensor()能够把灰度范围从0-255变换到0-1之间，而后面的transform.Normalize()则把0-1变换到(-1,1).tansform中应先剪切再转换为tensor</span></span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize([<span class="number">256</span>,<span class="number">256</span>]),<span class="comment">#图片宽高不一样时，要传两个参数</span></span><br><span class="line">    transforms.ColorJitter(),</span><br><span class="line">    transforms.RandomCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.Resize(<span class="number">128</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.ImageFolder(root=<span class="string">'./train_image'</span>, transform=data_transform)</span><br><span class="line">train_loader = Data.DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>) </span><br><span class="line"></span><br><span class="line">data_transform1 = transforms.Compose([</span><br><span class="line">    transforms.Resize([<span class="number">128</span>,<span class="number">128</span>]),<span class="comment">#图片宽高不一样时，要传两个参数</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line">test_data = torchvision.datasets.ImageFolder(root=<span class="string">'./test_image'</span>, transform=data_transform1)</span><br><span class="line">test_loader = Data.DataLoader(dataset=test_data,batch_size=<span class="number">1</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#.imshow()函数的输入需要是二维的numpy或者是第三维度是3或4的numpy,当第3维深度是1时，使用np.squeeze()函数压缩数据成为二维数组。</span></span><br><span class="line">plt.imshow(train_data[<span class="number">8</span>][<span class="number">0</span>].numpy().transpose(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)) </span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.imshow(test_data[<span class="number">15</span>][<span class="number">0</span>].numpy().transpose(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>输出</p>
<p><img src="/2020/09/16/Kaggle%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE(%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE%5C4.png" alt></p>
<h1 id="搭建网络"><a href="#搭建网络" class="headerlink" title="搭建网络"></a>搭建网络</h1><p>这里用的是前人搭建好的一个卷积神经网络densenet121</p>
<p>刚开始模仿数字识别那个例子自己搭建，但后来发现训练后的模型梯度弥散掉了，修改学习率并没有用。我认为是搭建的模型太过简单，而动物图片还是比较复杂的。参考别人用的densenet121这个模型，可以试一试效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#device = 'cuda' </span></span><br><span class="line"><span class="comment">#我的电脑显卡内存不够，就没有使用cuda训练</span></span><br><span class="line">model = torchvision.models.densenet121(pretrained=<span class="literal">True</span>)</span><br><span class="line">num_ftrs = model.classifier.in_features</span><br><span class="line">model.classifier = nn.Sequential(</span><br><span class="line">    nn.Linear(num_ftrs, <span class="number">500</span>),</span><br><span class="line">    nn.Linear(<span class="number">500</span>, <span class="number">2</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#model = model.to(device)</span></span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.train()</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br></pre></td><td class="code"><pre><span class="line">DenseNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (conv0): Conv2d(3, 64, kernel_size&#x3D;(7, 7), stride&#x3D;(2, 2), padding&#x3D;(3, 3), bias&#x3D;False)</span><br><span class="line">    (norm0): BatchNorm2d(64, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">    (relu0): ReLU(inplace&#x3D;True)</span><br><span class="line">    (pool0): MaxPool2d(kernel_size&#x3D;3, stride&#x3D;2, padding&#x3D;1, dilation&#x3D;1, ceil_mode&#x3D;False)</span><br><span class="line">    (denseblock1): _DenseBlock(</span><br><span class="line">      (denselayer1): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(64, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(64, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer2): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(96, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(96, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer3): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer4): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(160, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(160, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer5): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(192, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(192, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer6): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(224, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(224, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (transition1): _Transition(</span><br><span class="line">      (norm): BatchNorm2d(256, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">      (relu): ReLU(inplace&#x3D;True)</span><br><span class="line">      (conv): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      (pool): AvgPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0)</span><br><span class="line">    )</span><br><span class="line">    (denseblock2): _DenseBlock(</span><br><span class="line">      (denselayer1): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(128, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer2): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(160, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(160, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer3): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(192, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(192, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer4): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(224, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(224, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer5): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(256, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer6): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(288, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(288, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer7): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(320, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(320, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer8): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(352, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(352, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer9): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(384, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(384, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer10): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(416, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(416, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer11): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(448, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(448, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer12): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(480, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(480, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (transition2): _Transition(</span><br><span class="line">      (norm): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">      (relu): ReLU(inplace&#x3D;True)</span><br><span class="line">      (conv): Conv2d(512, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      (pool): AvgPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0)</span><br><span class="line">    )</span><br><span class="line">    (denseblock3): _DenseBlock(</span><br><span class="line">      (denselayer1): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(256, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(256, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer2): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(288, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(288, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer3): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(320, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(320, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer4): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(352, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(352, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer5): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(384, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(384, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer6): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(416, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(416, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer7): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(448, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(448, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer8): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(480, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(480, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer9): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer10): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(544, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(544, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer11): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(576, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(576, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer12): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(608, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(608, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer13): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(640, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(640, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer14): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(672, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(672, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer15): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(704, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(704, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer16): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(736, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(736, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer17): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(768, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(768, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer18): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(800, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(800, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer19): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(832, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(832, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer20): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(864, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(864, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer21): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(896, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(896, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer22): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(928, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(928, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer23): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(960, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(960, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer24): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(992, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(992, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (transition3): _Transition(</span><br><span class="line">      (norm): BatchNorm2d(1024, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">      (relu): ReLU(inplace&#x3D;True)</span><br><span class="line">      (conv): Conv2d(1024, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      (pool): AvgPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0)</span><br><span class="line">    )</span><br><span class="line">    (denseblock4): _DenseBlock(</span><br><span class="line">      (denselayer1): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(512, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer2): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(544, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(544, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer3): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(576, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(576, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer4): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(608, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(608, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer5): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(640, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(640, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer6): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(672, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(672, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer7): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(704, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(704, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer8): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(736, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(736, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer9): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(768, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(768, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer10): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(800, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(800, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer11): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(832, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(832, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer12): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(864, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(864, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer13): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(896, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(896, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer14): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(928, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(928, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer15): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(960, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(960, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">      (denselayer16): _DenseLayer(</span><br><span class="line">        (norm1): BatchNorm2d(992, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu1): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv1): Conv2d(992, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">        (norm2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">        (relu2): ReLU(inplace&#x3D;True)</span><br><span class="line">        (conv2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm5): BatchNorm2d(1024, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">  )</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Linear(in_features&#x3D;1024, out_features&#x3D;500, bias&#x3D;True)</span><br><span class="line">    (1): Linear(in_features&#x3D;500, out_features&#x3D;2, bias&#x3D;True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h1 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">EPOCH=<span class="number">2</span></span><br><span class="line"><span class="comment"># training and testing</span></span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.0001</span>)  <span class="comment"># optimize all cnn parameters</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class="number">500</span>,<span class="number">1000</span>,<span class="number">1500</span>], gamma=<span class="number">0.5</span>)</span><br><span class="line">loss_func = nn.CrossEntropyLoss()  <span class="comment"># the target label is not one-hotted</span></span><br><span class="line"><span class="comment">#测试集样本特征和标签是放到一起的，要分别取出来才能计算测试的准确率</span></span><br><span class="line">a=[test_x <span class="keyword">for</span> (test_x,test_y) <span class="keyword">in</span> test_loader]</span><br><span class="line">tensor_x=torch.cat(a)</span><br><span class="line"><span class="comment">#如果要使用显卡，加下行代码</span></span><br><span class="line"><span class="comment">#tensor_x=tensor_x.to(device)</span></span><br><span class="line">b=[test_y <span class="keyword">for</span> (test_x,test_y) <span class="keyword">in</span> test_loader]</span><br><span class="line">tensor_y=torch.cat(b)</span><br><span class="line"><span class="comment">#tensor_y=tensor_y.to(device)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step,(b_x, b_y) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line"><span class="comment">#         b_x=b_x.to(device)</span></span><br><span class="line"><span class="comment">#         b_y=b_y.to(device)</span></span><br><span class="line">        output = model(b_x)</span><br><span class="line">        loss = loss_func(output, b_y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">300</span> == <span class="number">0</span>: <span class="comment">#每经过300个batch输出一次结果</span></span><br><span class="line">            test_output = model(tensor_x)</span><br><span class="line">            pred_y = torch.max(test_output,dim=<span class="number">1</span>)[<span class="number">1</span>].data.numpy()</span><br><span class="line">            accuracy = (sum(pred_y == np.array(tensor_y.data)).item()) / tensor_y.size(<span class="number">0</span>)</span><br><span class="line">            print(<span class="string">'Epoch:'</span>, epoch, <span class="string">'| train loss:%.4f'</span> % loss.item(), <span class="string">'| test accuracy:%.4f'</span> % accuracy)</span><br></pre></td></tr></table></figure>





<p>总结</p>
<p>用Kaggle环境运行的代码已放到kaggle的个人笔记中了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/09/16/Kaggle%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE(%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)/" data-id="ckn1t0idi0001rgvtbmglaglw" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-PyTorch学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2020-09-10T01:45:36.000Z" itemprop="datePublished">2020-09-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">PyTorch学习笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>神经网络的典型训练过程如下:<br>\1. 定义神经网络模型,它有一些可学习的参数(或者权重);<br>\2. 在数据集上迭代;<br>\3. 通过神经网络处理输入;<br>\4. 计算损失(输出结果和正确值的差距大小)<br>\5. 将梯度反向传播会网络的参数;<br>\6. 更新网络的参数,主要使用如下简单的更新原则:</p>
<h1 id="Torch基本操作"><a href="#Torch基本操作" class="headerlink" title="Torch基本操作"></a>Torch基本操作</h1><h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">#直接创建1</span><br><span class="line">x&#x3D;torch.Tensor(2,3) #创建2行3列的二维零张量</span><br><span class="line">print(x)</span><br><span class="line">torch.ones(2,3) #创建全为1张量</span><br><span class="line">torch.rand(3)#创建随机一维张量</span><br><span class="line">#直接创建2</span><br><span class="line">torch.arange(4).reshape(2,2)</span><br><span class="line">#间接创建1</span><br><span class="line">data&#x3D;[[-1,-2],[1,2]]</span><br><span class="line">tensor&#x3D;torch.FloatTensor(data)#将list传入</span><br><span class="line">print(tensor)  </span><br><span class="line">#间接创建2</span><br><span class="line">data&#x3D;np.array(data)</span><br><span class="line">tensor1&#x3D;torch.from_numpy(data) #将array传入</span><br><span class="line">print(tensor1)</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.0000e+00, 1.8750e+00, 1.8754e+28],</span><br><span class="line">        [6.3455e-10, 2.1953e-04, 1.0471e-11]])</span><br><span class="line">tensor([[-1., -2.],</span><br><span class="line">        [ 1.,  2.]])</span><br><span class="line">tensor([[-1, -2],</span><br><span class="line">        [ 1,  2]], dtype&#x3D;torch.int32)</span><br></pre></td></tr></table></figure>

<h2 id="使用GPU计算"><a href="#使用GPU计算" class="headerlink" title="使用GPU计算"></a>使用GPU计算</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(torch.cuda.is_available()) #显卡是否可用</span><br><span class="line">a&#x3D;torch.rand(3,4)</span><br><span class="line">b&#x3D;torch.rand(3,4)</span><br><span class="line">a&#x3D;a.cuda()</span><br><span class="line">b&#x3D;b.cuda()</span><br><span class="line">a+b</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.2660, 0.7162, 0.2329, 1.4352],</span><br><span class="line">        [1.3916, 1.0633, 1.5543, 1.3646],</span><br><span class="line">        [1.8740, 1.2912, 0.6832, 1.7123]], device&#x3D;&#39;cuda:0&#39;)</span><br></pre></td></tr></table></figure>

<h2 id="numpy与torch"><a href="#numpy与torch" class="headerlink" title="numpy与torch"></a>numpy与torch</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">data&#x3D;[[-1,-2],[1,2]]</span><br><span class="line">tensor&#x3D;torch.FloatTensor(data)</span><br><span class="line">data1&#x3D;tensor.numpy() #tensor转化为numpy </span><br><span class="line">print(np.abs(data),torch.abs(tensor))#取绝对值</span><br><span class="line">print(np.sin(data),torch.sin(tensor))#求sin</span><br><span class="line">print(np.matmul(data,data))#np的矩阵相乘</span><br><span class="line">print(torch.mm(tensor,tensor))#torch的矩阵相乘</span><br><span class="line">print(np.array(data).dot(data)) #使用dot前要转换成array格式</span><br><span class="line">data1[0,1],tensor[0,1]#取1行2列。如果是pandas则是.iloc[行索引，列索引]或.loc[行名,列名]</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[[1 2]</span><br><span class="line"> [1 2]]                                                                              tensor([[1., 2.],</span><br><span class="line">        [1., 2.]])</span><br><span class="line">[[-0.84147098 -0.90929743]</span><br><span class="line"> [ 0.84147098  0.90929743]]                                               tensor([[-0.8415, -0.9093],</span><br><span class="line">        [ 0.8415,  0.9093]])</span><br><span class="line">[[-1 -2]</span><br><span class="line"> [ 1  2]]</span><br><span class="line">tensor([[-1., -2.],</span><br><span class="line">        [ 1.,  2.]])</span><br><span class="line">[[-1 -2]</span><br><span class="line"> [ 1  2]]</span><br><span class="line">-2 tensor(-2.)</span><br></pre></td></tr></table></figure>

<h1 id="变量-Variable"><a href="#变量-Variable" class="headerlink" title="变量(Variable)"></a>变量(Variable)</h1><p>PyTorch 中所有神经网络的核心是<code>autograd</code>包</p>
<p><em><code>autograd.Variable</code><em>是<code>autograd</code>包的核心类.它包装了</em>张量</em>(<code>Tensor</code>),支持几乎所有的张量上的操作.一旦你完成你的前向计算,可以通过<code>.backward()</code>方法来自动计算所有的梯度.</p>
<p>可以通过<code>.data</code>属性来访问变量中的原始张量,关于这个变量的梯度被计算放入<code>.grad</code>属性中</p>
<h2 id="求梯度–例1"><a href="#求梯度–例1" class="headerlink" title="求梯度–例1"></a>求梯度–例1</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">x&#x3D;Variable(torch.ones(2,2),requires_grad&#x3D;True) #requires_grad指当前量是否需要在计算中保留对应的梯度信息</span><br><span class="line">y&#x3D;x+2</span><br><span class="line">z&#x3D;y*y*3</span><br><span class="line">out&#x3D;z.mean()</span><br><span class="line">print(out)</span><br><span class="line">out.backward() #out是一个标量，可以不添加参数</span><br><span class="line">print(x.grad)</span><br><span class="line">print(x.data) #.data转化为tensor格式,之后就可以用.numpy()转化为array格式</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor(27., grad_fn&#x3D;&lt;MeanBackward0&gt;)</span><br><span class="line">tensor([[4.5000, 4.5000],</span><br><span class="line">        [4.5000, 4.5000]])</span><br><span class="line">tensor([[1., 1.],</span><br><span class="line">        [1., 1.]])</span><br></pre></td></tr></table></figure>

<h2 id="例2"><a href="#例2" class="headerlink" title="例2"></a>例2</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x&#x3D;torch.rand(3)</span><br><span class="line">x&#x3D;Variable(x,requires_grad&#x3D;True)</span><br><span class="line">y&#x3D;x*2</span><br><span class="line">while y.data.norm()&lt;1000:</span><br><span class="line">    y&#x3D;y*2</span><br><span class="line">#.data.norm()对张量y每个元素进行平方，然后对它们求和，最后取平方根。 这些操作计算所谓的L2或欧几里德范数</span><br><span class="line">print(y)</span><br><span class="line">gradients&#x3D;torch.FloatTensor([0.1,1.0,0.0001])</span><br><span class="line">y.backward(gradients)#参数要与y维度相同。</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([915.7226, 560.8721, 120.4810], grad_fn&#x3D;&lt;MulBackward0&gt;)</span><br><span class="line">tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</span><br></pre></td></tr></table></figure>

<h2 id="例3"><a href="#例3" class="headerlink" title="例3"></a>例3</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; Variable(torch.Tensor([[2,4],[3,6]]),requires_grad&#x3D;True)</span><br><span class="line">b &#x3D; torch.zeros(2,2)</span><br><span class="line">b[0,0] &#x3D; a[0,0] ** 2 #平方</span><br><span class="line">b[0,1] &#x3D; a[0,1] ** 3 #3次方</span><br><span class="line">out&#x3D;b*2</span><br><span class="line">#参数要传入和out维度一样的矩阵</span><br><span class="line">out.backward(torch.FloatTensor([[1,0.5],[1,2]]))</span><br><span class="line">a.grad</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 8., 48.],</span><br><span class="line">        [ 0.,  0.]])</span><br></pre></td></tr></table></figure>

<h1 id="激活函数（Activation-Function）"><a href="#激活函数（Activation-Function）" class="headerlink" title="激活函数（Activation Function）"></a>激活函数（Activation Function）</h1><p>常见的激活函数</p>
<img src="/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.png" style="zoom: 50%;">

<p>使用<code>torch.nn</code>包来构建神经网络.</p>
<p><code>autograd</code>包,<code>nn</code>包依赖<code>autograd</code>包来定义模型并求导.一个<code>nn.Module</code>包含各个层和一个<code>faward(input)</code>方法,该方法返回<code>output</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x&#x3D;torch.linspace(-5,5,200)#从—5到5取200个点</span><br><span class="line"></span><br><span class="line">y_relu&#x3D;torch.relu(x)</span><br><span class="line">y_sigmoid&#x3D;torch.sigmoid(x)</span><br><span class="line">y_tanh&#x3D;torch.tanh(x)</span><br><span class="line">y_softplus&#x3D;F.softplus(x)</span><br><span class="line"></span><br><span class="line">#画图</span><br><span class="line"></span><br><span class="line"># 使用plt.subplot来创建小图. plt.subplot(221)表示将整个图像窗口分为2行2列, 当前位置为1.</span><br><span class="line">plt.subplot(221)</span><br><span class="line">plt.ylim(-1,5) #限制y的范围</span><br><span class="line">plt.plot(x,y_relu,label&#x3D;&#39;relu&#39;)</span><br><span class="line">plt.legend()</span><br><span class="line"># plt.subplot(222)表示将整个图像窗口分为2行2列, 当前位置为2.</span><br><span class="line">plt.subplot(222) # 第一行的右图</span><br><span class="line">plt.plot(x,y_sigmoid,label&#x3D;&#39;sigmoid&#39;)</span><br><span class="line">plt.legend()</span><br><span class="line"># plt.subplot(223)表示将整个图像窗口分为2行2列, 当前位置为3.</span><br><span class="line">plt.subplot(223)</span><br><span class="line">plt.plot(x,y_tanh,label&#x3D;&#39;tanh&#39;)</span><br><span class="line">plt.legend()</span><br><span class="line"># plt.subplot(224)表示将整个图像窗口分为2行2列, 当前位置为4.</span><br><span class="line">plt.subplot(224)</span><br><span class="line">plt.plot(x,y_softplus,label&#x3D;&#39;softplus&#39;)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>

<p>输出</p>
<p><img src="/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C2.png" alt></p>
<p>1.从<code>__init__</code>函数中可以看出Linear中包含四个属性</p>
<ul>
<li>in_features: 上层神经元个数</li>
<li>out_features： 本层神经元个数</li>
<li>weight：权重， 形状[out_features , in_features]</li>
<li>bias: 偏置， 形状[out_features]</li>
</ul>
<p>2.<code>reset_parameters(self)</code><br>参数初始化函数<br>在<strong>init</strong>中调用此函数，权重采用Xvaier initialization 初始化方式初始参数。</p>
<p>3.<code>forward(self, input)</code></p>
<h2 id="1-nn-Linear-源码解读"><a href="#1-nn-Linear-源码解读" class="headerlink" title="1. nn.Linear 源码解读"></a>1. nn.Linear 源码解读</h2><p>先看一下Linear类的实现：<a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html" target="_blank" rel="noopener">源码地址</a><br>Linear继承于nn.Module,内部函数主要有<code>__init__，reset_parameters, forward和 extra_repr函数</code>，下面是部分源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features, bias=True)</span>:</span></span><br><span class="line">       super(Linear, self).__init__()</span><br><span class="line">       self.in_features = in_features</span><br><span class="line">       self.out_features = out_features</span><br><span class="line">       self.weight = Parameter(torch.Tensor(out_features, in_features))</span><br><span class="line">       <span class="keyword">if</span> bias:</span><br><span class="line">           self.bias = Parameter(torch.Tensor(out_features))</span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           self.register_parameter(<span class="string">'bias'</span>, <span class="literal">None</span>)</span><br><span class="line">       self.reset_parameters()</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">       init.kaiming_uniform_(self.weight, a=math.sqrt(<span class="number">5</span>))</span><br><span class="line">       <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">           fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)</span><br><span class="line">           bound = <span class="number">1</span> / math.sqrt(fan_in)</span><br><span class="line">           init.uniform_(self.bias, -bound, bound)</span><br><span class="line"></span><br><span class="line"><span class="meta">   @weak_script_method</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">       <span class="keyword">return</span> F.linear(input, self.weight, self.bias)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">extra_repr</span><span class="params">(self)</span>:</span></span><br><span class="line">       <span class="keyword">return</span> <span class="string">'in_features=&#123;&#125;, out_features=&#123;&#125;, bias=&#123;&#125;'</span>.format(</span><br><span class="line">           self.in_features, self.out_features, self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">       )</span><br></pre></td></tr></table></figure>

<p>在Module的<strong>call</strong>函数调用此函数，使得类对象具有函数调用的功能，同过此功能实现pytorch的网络结构堆叠</p>
<h1 id="回归问题–网络搭建"><a href="#回归问题–网络搭建" class="headerlink" title="回归问题–网络搭建"></a>回归问题–网络搭建</h1><p>代码实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x=torch.unsqueeze(torch.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">100</span>),dim=<span class="number">1</span>) <span class="comment">#给指定位置1加上维数为一的维度</span></span><br><span class="line">y=x**<span class="number">2</span> + <span class="number">0.2</span>*torch.rand(x.shape)</span><br><span class="line"><span class="comment"># plt.scatter(x,y)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#先定义一个网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="comment">#定义隐藏层的一些属性或信息</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,n_feature,n_hidden,n_output)</span>:</span></span><br><span class="line">        super().__init__()<span class="comment">#继承父类的构造方法</span></span><br><span class="line">        <span class="comment">#torch.nn.linear()用于设置网络当中的全连接层，输入和输出都是二维张量</span></span><br><span class="line">        self.hidden=torch.nn.Linear(n_feature,n_hidden)<span class="comment">#隐藏层的属性,将层信息（多少个输入和隐藏层神经元数）传给它</span></span><br><span class="line">        self.predict=torch.nn.Linear(n_hidden,n_output)<span class="comment">#定义预测层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span><span class="comment">#将刚才定义的信息放到forward上组合起来,搭好神经网络</span></span><br><span class="line">        x=torch.relu(self.hidden(x))<span class="comment">#用激活函数激活x经过隐藏层所输出的信息（神经元个数）</span></span><br><span class="line">        x=self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment">#搭好之后,创建对象</span></span><br><span class="line">net=Net(<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>)<span class="comment">#一个输入，隐藏层10个神经元，一个输出</span></span><br><span class="line">print(net) <span class="comment">#查看搭好的网络</span></span><br><span class="line">plt.ion()<span class="comment">#画动图</span></span><br><span class="line"><span class="comment">#优化参数</span></span><br><span class="line">optimizer=torch.optim.SGD(net.parameters(),lr=<span class="number">0.5</span>) <span class="comment">#SGD是指随机梯度下降,lr指学习效率</span></span><br><span class="line"><span class="comment">#loss_func=torch.nn.MSELoss()#计算误差,MSE是均方差，用来处理回归问题</span></span><br><span class="line">loss_func=torch.nn.MSELoss()</span><br><span class="line"><span class="comment">#开始训练</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">    prediction=net(x) <span class="comment">#在Module父类中有个__call__方法,net(x)就相当于net.forward(x)</span></span><br><span class="line">    loss=loss_func(prediction,y)</span><br><span class="line">    optimizer.zero_grad() <span class="comment">#将所有参数的梯度降为0,因为每次更新后原有的梯度还保留在net里</span></span><br><span class="line">    loss.backward()<span class="comment">#反向传播计算出节点的梯度</span></span><br><span class="line">    optimizer.step()<span class="comment">#以0.5的学习效率优化梯度</span></span><br><span class="line">    <span class="keyword">if</span> t%<span class="number">5</span>==<span class="number">0</span>: <span class="comment">#show learning process</span></span><br><span class="line">        plt.cla()<span class="comment"># Clear axis即清除当前图形中的当前活动轴。其他轴不受影响</span></span><br><span class="line">        plt.scatter(x,y)</span><br><span class="line">        plt.plot(x,prediction.data,<span class="string">'r-'</span>,lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>,<span class="number">0</span>,<span class="string">'loss=%.4f'</span>% loss.item())<span class="comment">#给(0.5,0)这个坐标点添加标签。.item()得到loss的数值</span></span><br><span class="line">        plt.pause(<span class="number">0.1</span>)<span class="comment">#图片的间隔</span></span><br><span class="line">plt.ioff</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Net(</span><br><span class="line">  (hidden): Linear(in_features&#x3D;1, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">  (predict): Linear(in_features&#x3D;10, out_features&#x3D;1, bias&#x3D;True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C3.png" alt></p>
<p>…</p>
<p>…</p>
<p>…</p>
<p><img src="/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C4.png" alt></p>
<h2 id="快速搭建神经网络"><a href="#快速搭建神经网络" class="headerlink" title="快速搭建神经网络"></a>快速搭建神经网络</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#method2--快速搭建神经网络</span><br><span class="line">net2&#x3D;torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(1,10),</span><br><span class="line">    torch.nn.ReLU(),#一个类</span><br><span class="line">    torch.nn.Linear(10,1)</span><br><span class="line">)</span><br><span class="line">print(net2)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Linear(in_features&#x3D;1, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">  (1): ReLU()</span><br><span class="line">  (2): Linear(in_features&#x3D;10, out_features&#x3D;1, bias&#x3D;True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h1 id="分类问题–网络搭建"><a href="#分类问题–网络搭建" class="headerlink" title="分类问题–网络搭建"></a>分类问题–网络搭建</h1><p>代码实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">n_data=torch.ones(<span class="number">100</span>,<span class="number">2</span>)<span class="comment">#二维</span></span><br><span class="line">x0=torch.normal(<span class="number">2</span>*n_data,<span class="number">1</span>)<span class="comment">#生成100个符合正态分布的均值为2,标准差都为1的随机数（标准差也可以设置100个）</span></span><br><span class="line">y0=torch.zeros(<span class="number">100</span>)</span><br><span class="line">x1=torch.normal(<span class="number">-2</span>*n_data,<span class="number">1</span>)<span class="comment">#生成100个符合正态分布的均值为-2,标准差都为1的随机数</span></span><br><span class="line">y1=torch.ones(<span class="number">100</span>)</span><br><span class="line">x=torch.cat((x0,x1),<span class="number">0</span>).type(torch.FloatTensor) <span class="comment">#cat按维数0拼接（竖着拼）</span></span><br><span class="line">y=torch.cat((y0,y1),).type(torch.LongTensor)<span class="comment">#64位integer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#plt.scatter(x.data[:,0],x.data[:,1],c=y.data.numpy())</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#先定义一个网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="comment">#定义隐藏层的一些属性或信息</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,n_feature,n_hidden,n_output)</span>:</span></span><br><span class="line">        super().__init__()<span class="comment">#继承父类的构造方法</span></span><br><span class="line">        <span class="comment">#torch.nn.linear()用于设置网络当中的全连接层，输入和输出都是二维张量</span></span><br><span class="line">        self.hidden=torch.nn.Linear(n_feature,n_hidden)<span class="comment">#隐藏层的属性,将层信息（多少个输入和隐藏层神经元数）传给它</span></span><br><span class="line">        <span class="comment">#一个类创建的对象可以是另一个类创建的对象的属性</span></span><br><span class="line">        self.predict=torch.nn.Linear(n_hidden,n_output)<span class="comment">#定义预测层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span><span class="comment">#将刚才定义的信息放到forward上组合起来,搭好神经网络</span></span><br><span class="line">        x=torch.relu(self.hidden(x))<span class="comment">#用激活函数激活x经过隐藏层所输出的信息（神经元个数）</span></span><br><span class="line">        x=self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment">#搭好之后,创建对象</span></span><br><span class="line">net1=Net(<span class="number">2</span>,<span class="number">10</span>,<span class="number">2</span>)<span class="comment">#一个输入，隐藏层10个神经元，一个输出</span></span><br><span class="line">print(net1) <span class="comment">#查看搭好的网络</span></span><br><span class="line"><span class="comment">#method2--快速搭建神经网络</span></span><br><span class="line">net2=torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">2</span>,<span class="number">10</span>),</span><br><span class="line">    torch.nn.ReLU(),<span class="comment">#一个类</span></span><br><span class="line">    torch.nn.Linear(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">)</span><br><span class="line">print(net2)</span><br><span class="line">plt.ion()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#优化参数</span></span><br><span class="line">optimizer=torch.optim.SGD(net.parameters(),lr=<span class="number">0.02</span>) <span class="comment">#SGD是指随机梯度下降,lr指学习效率</span></span><br><span class="line">loss_func=torch.nn.CrossEntropyLoss()<span class="comment">#计算误差,用来处理分类问题</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">60</span>):</span><br><span class="line">    out=net(x)</span><br><span class="line">    loss=loss_func(out,y)</span><br><span class="line">    optimizer.zero_grad() <span class="comment">#将所有参数的梯度降为0,因为每次更新后原有的梯度还保留在net里</span></span><br><span class="line">    loss.backward()<span class="comment">#反向传播计算出节点的梯度</span></span><br><span class="line">    optimizer.step()<span class="comment">#以0.5的学习效率优化梯度</span></span><br><span class="line">    <span class="keyword">if</span> t%<span class="number">2</span>==<span class="number">0</span>: <span class="comment">#show learning process</span></span><br><span class="line">        plt.cla()</span><br><span class="line">        <span class="comment">#torch.max()返回两组数据，第一组是最大值，第二组是最大值的索引</span></span><br><span class="line">        prediction=torch.max(out,dim=<span class="number">1</span>)[<span class="number">1</span>]<span class="comment">#用.max将out转换为概率.dim=1为横向</span></span><br><span class="line">        pred_y=prediction.numpy()</span><br><span class="line">        target_y=y.numpy()</span><br><span class="line">        plt.scatter(x.data[:,<span class="number">0</span>],x.data[:,<span class="number">1</span>],c=pred_y)</span><br><span class="line">        accuracy=sum(pred_y==target_y)/<span class="number">200</span></span><br><span class="line">        plt.text(<span class="number">1.5</span>,<span class="number">-4</span>,<span class="string">'accuracy=%.4f'</span>% accuracy)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br><span class="line">plt.ioff</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Net(</span><br><span class="line">  (hidden): Linear(in_features&#x3D;2, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">  (predict): Linear(in_features&#x3D;10, out_features&#x3D;2, bias&#x3D;True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C5.png" alt></p>
<p>…</p>
<p>…</p>
<p>…</p>
<p><img src="/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C6.png" alt></p>
<h1 id="神经网络保存及提取"><a href="#神经网络保存及提取" class="headerlink" title="神经网络保存及提取"></a>神经网络保存及提取</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#神经网络保存及提取</span></span><br><span class="line"><span class="comment">#保存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#构建网络并进行训练</span></span><br><span class="line">    net=torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">2</span>,<span class="number">10</span>),</span><br><span class="line">    torch.nn.ReLU()</span><br><span class="line">    torch.nn.Linear(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">)</span><br><span class="line">           		            optimizer=torch.optim.SGD(net.parameters(),lr=<span class="number">0.5</span>) </span><br><span class="line">    loss_func=torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">80</span>):</span><br><span class="line">		out=net(x)</span><br><span class="line">		loss=loss_func(out,y)</span><br><span class="line">		optimizer.zero_grad() </span><br><span class="line">		loss.backward()</span><br><span class="line">		optimizer.step()</span><br><span class="line">    <span class="comment">#保存整个网络</span></span><br><span class="line">    torch.save(net,<span class="string">'net.pkl'</span>)</span><br><span class="line">    <span class="comment">#保存网络的参数</span></span><br><span class="line">    torch.save(net.state_dict,<span class="string">'net_para.pkl'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#提取这个完整网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_net</span><span class="params">()</span>:</span></span><br><span class="line">    net1=torch.load(<span class="string">'net.pkl'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#提取参数时,要先建立一个结构一模一样的神经网络(速度更快)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_params</span><span class="params">()</span>:</span></span><br><span class="line">    net2=torch.nn.Sequential(</span><br><span class="line">        torch.nn.Linear(<span class="number">2</span>,<span class="number">10</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        rotch.nn.Linear(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">)</span><br><span class="line">    net2.load.state_dict(torch.load(<span class="string">'net_para.pkl'</span>))</span><br></pre></td></tr></table></figure>

<h1 id="批数据训练"><a href="#批数据训练" class="headerlink" title="批数据训练"></a>批数据训练</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">x=torch.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">y=torch.linspace(<span class="number">10</span>,<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">Batch_Size=<span class="number">5</span> <span class="comment">#每批有五组数据</span></span><br><span class="line">torch_dataset=Data.TensorDataset(x,y)<span class="comment">#告诉数据库使用(x,y)进行训练</span></span><br><span class="line">loader=Data.DataLoader(dataset=torch_dataset,batch_size=Batch_Size,shuffle=<span class="literal">True</span>) <span class="comment">#使训练批量进行,shuffle指训练时要不要随机打乱顺序</span></span><br><span class="line">print(loader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">3</span>): <span class="comment">#迭代三代</span></span><br><span class="line">    <span class="keyword">for</span> step,(batch_x,batch_y) <span class="keyword">in</span> enumerate(loader): <span class="comment">#每代迭代2次</span></span><br><span class="line">        print(<span class="string">'epoch:'</span>, epoch,<span class="string">'step:'</span>, step,<span class="string">'batch x:'</span>, batch_x.numpy(),<span class="string">'batch y:'</span>, batch_y.numpy())</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;torch.utils.data.dataloader.DataLoader object at 0x00000185BDEC82C8&gt;</span><br><span class="line">epoch: 0 step: 0 batch x: [ 8.  6.  2.  4. 10.] batch y: [3. 5. 9. 7. 1.]</span><br><span class="line">epoch: 0 step: 1 batch x: [7. 5. 3. 1. 9.] batch y: [ 4.  6.  8. 10.  2.]</span><br><span class="line">epoch: 1 step: 0 batch x: [ 3.  2. 10.  5.  7.] batch y: [8. 9. 1. 6. 4.]</span><br><span class="line">epoch: 1 step: 1 batch x: [4. 8. 6. 1. 9.] batch y: [ 7.  3.  5. 10.  2.]</span><br><span class="line">epoch: 2 step: 0 batch x: [8. 5. 1. 2. 6.] batch y: [ 3.  6. 10.  9.  5.]</span><br><span class="line">epoch: 2 step: 1 batch x: [ 7.  4.  9.  3. 10.] batch y: [4. 7. 2. 8. 1.]</span><br></pre></td></tr></table></figure>



<h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><p>优化器性能比较（横坐标为）</p>
<p><img src="/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C7.png" alt></p>
<h2 id="将批数据，超参数与几种常见优化器运用到神经网络中："><a href="#将批数据，超参数与几种常见优化器运用到神经网络中：" class="headerlink" title="将批数据，超参数与几种常见优化器运用到神经网络中："></a>将批数据，超参数与几种常见优化器运用到神经网络中：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment">#超参数</span></span><br><span class="line">EPOCH=<span class="number">3</span></span><br><span class="line">Batch_Size=<span class="number">10</span> <span class="comment">#每批有五组数据</span></span><br><span class="line">LR=<span class="number">0.5</span></span><br><span class="line"><span class="comment">#批数据</span></span><br><span class="line">x=torch.unsqueeze(torch.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">100</span>),dim=<span class="number">1</span>)</span><br><span class="line">y=torch.unsqueeze(torch.linspace(<span class="number">10</span>,<span class="number">1</span>,<span class="number">100</span>),dim=<span class="number">1</span>)</span><br><span class="line">torch_dataset=Data.TensorDataset(x,y)<span class="comment">#告诉数据库使用(x,y)进行训练</span></span><br><span class="line">loader=Data.DataLoader(dataset=torch_dataset,batch_size=Batch_Size,shuffle=<span class="literal">True</span>) <span class="comment">#使训练批量进行,shuffle指训练时要不要随机打乱顺序</span></span><br><span class="line"><span class="comment">#搭建神经网络</span></span><br><span class="line">net=torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">1</span>,<span class="number">10</span>),</span><br><span class="line">    torch.nn.ReLU(), <span class="comment">#一个类</span></span><br><span class="line">    torch.nn.Linear(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"><span class="comment">#几种常见优化器</span></span><br><span class="line">optimizer=torch.optim.SGD(net.parameters(),lr=LR) <span class="comment">#SGD是指随机梯度下降,lr指学习效率</span></span><br><span class="line">optimizer2=torch.optim.SGD(net.parameters(),lr=LR,momentum=<span class="number">0.8</span>) <span class="comment">#有momentum使梯度下降带有惯性因素， 相当于从平坡变为斜坡</span></span><br><span class="line">optimizer3=torch.optim.RMSprop(net.parameters(),lr=LR,alpha=<span class="number">0.9</span>)<span class="comment"># momentum与AdaGrad结合</span></span><br><span class="line">optimizer4=torch.optim.Adam(net.parameters(),lr=LR,betas=(<span class="number">0.9</span>,<span class="number">0.99</span>)) <span class="comment">#momentum与AdaGrad的另一种结合</span></span><br><span class="line"></span><br><span class="line">loss_func=torch.nn.MSELoss()<span class="comment">#计算误差</span></span><br><span class="line"><span class="comment">#传入数据</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH): <span class="comment">#迭代三代</span></span><br><span class="line">    <span class="keyword">for</span> step,(batch_x,batch_y) <span class="keyword">in</span> enumerate(loader): <span class="comment">#每代迭代10次</span></span><br><span class="line">        <span class="comment">#现在的版本已经整合了tensor和variable</span></span><br><span class="line"><span class="comment">#        b_x=Variable(batch_x)</span></span><br><span class="line"><span class="comment">#        b_y=Variable(batch_y) </span></span><br><span class="line">        out=net(batch_x)</span><br><span class="line">        loss=loss_func(out,batch_y)</span><br><span class="line">        optimizer.zero_grad() <span class="comment">#将所有参数的梯度降为0,因为每次更新后原有的梯度还保留在net里</span></span><br><span class="line">        loss.backward()<span class="comment">#反向传播计算出节点的梯度</span></span><br><span class="line">        optimizer.step()<span class="comment">#以0.5的学习效率优化梯度</span></span><br><span class="line">        print(out)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-2.4038],</span><br><span class="line">        [-0.6541],</span><br><span class="line">        [-1.7248],</span><br><span class="line">        [-0.7846],</span><br><span class="line">        [-1.3331],</span><br><span class="line">        [-1.0719],</span><br><span class="line">        [-2.6650],</span><br><span class="line">        [-2.6127],</span><br><span class="line">        [-2.8739],</span><br><span class="line">        [-2.7172]], grad_fn&#x3D;&lt;AddmmBackward&gt;)</span><br><span class="line">tensor([[6.7463],</span><br><span class="line">        [6.7463],</span><br><span class="line">        [6.7463],</span><br><span class="line">        [6.7463],</span><br><span class="line">        [6.7463],</span><br><span class="line">        [6.7463],</span><br><span class="line">        [6.7463],</span><br><span class="line">        [6.7463],</span><br><span class="line">        [6.7463],</span><br><span class="line">        [6.7463]], grad_fn&#x3D;&lt;AddmmBackward&gt;)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h1 id="CNN卷积神经网络"><a href="#CNN卷积神经网络" class="headerlink" title="CNN卷积神经网络"></a>CNN卷积神经网络</h1><h2 id="例子–数字识别"><a href="#例子–数字识别" class="headerlink" title="例子–数字识别"></a>例子–数字识别</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#超参数</span></span><br><span class="line">EPOCH=<span class="number">1</span></span><br><span class="line">BATCH_SIZE=<span class="number">50</span> <span class="comment">#每批有五组数据</span></span><br><span class="line">LR=<span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">DOWNLOAD_MINLIST=<span class="literal">False</span> <span class="comment">#未下载前设置为Ture</span></span><br><span class="line">train_data=torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">'./mnist'</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),</span><br><span class="line">    download=DOWNLOAD_MINLIST</span><br><span class="line">)</span><br><span class="line">print(train_data.data.size()) <span class="comment">#60000张，像素为28*28</span></span><br><span class="line">print(train_data.targets.size())</span><br><span class="line"><span class="comment">#imshow()接收一张图像，只是画出该图，并不会立刻显示出来,之后还可以进行其他draw操作比如scatter散点等,所有画完后使用plt.show()才能进行结果的显示</span></span><br><span class="line">plt.imshow(train_data.data[<span class="number">0</span>],cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'%d'</span> % train_data.targets[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#将数据分批处理</span></span><br><span class="line">train_loader=Data.DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=<span class="literal">True</span>) </span><br><span class="line">test_data=torchvision.datasets.MNIST(root=<span class="string">'./mnist'</span>,train=<span class="literal">False</span>)</span><br><span class="line">test_x = torch.unsqueeze(test_data.data, dim=<span class="number">1</span>).type(torch.float32)[:<span class="number">2000</span>] / <span class="number">255.</span></span><br><span class="line">test_y = test_data.targets[:<span class="number">2000</span>]</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([60000, 28, 28])</span><br><span class="line">torch.Size([60000])</span><br></pre></td></tr></table></figure>

<p><img src="/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C8.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义神经网络层,常见结构是先将图像经过卷积层--&gt;激活层--&gt;池化层,再经过一次卷积层--&gt;激活层--&gt;池化层，最后经过一个线性层</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(  <span class="comment"># (1, 28, 28)</span></span><br><span class="line">                in_channels=<span class="number">1</span>,<span class="comment">#第一层输入图片的高度</span></span><br><span class="line">                out_channels=<span class="number">16</span>,<span class="comment">#输出图片的高度（特征）</span></span><br><span class="line">                kernel_size=<span class="number">5</span>,<span class="comment">#kernel高和宽各为5个像素点</span></span><br><span class="line">                stride=<span class="number">1</span>,<span class="comment">#kernel每一次遍历跳一个pixel像素</span></span><br><span class="line">                padding=<span class="number">2</span>,  <span class="comment">#这样卷积后就可以得到相同大小的图片 if stride = 1, padding = (kernel_size - 1)/2 = (5-1)/2</span></span><br><span class="line">            ),  <span class="comment"># 卷积层（就是过滤器，filter） # -&gt; (16, 28, 28)</span></span><br><span class="line">            nn.ReLU(),  <span class="comment"># 激活函数（神经网络）    # -&gt; (16, 28, 28)</span></span><br><span class="line">            <span class="comment">#池化层 # -&gt; (16, 14, 14)，作用是将图片缩小（思路是用2*2的kernel遍历，每次遍历选取框内最大的数）</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>), </span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(  <span class="comment"># (16, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),  <span class="comment"># -&gt; (32, 14, 14)</span></span><br><span class="line">            nn.ReLU(),  <span class="comment"># -&gt; (32, 14, 14)</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>)  <span class="comment"># -&gt; (32, 7, 7)</span></span><br><span class="line">        )</span><br><span class="line">        self.out = nn.Linear(<span class="number">32</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)  <span class="comment"># (batch, 32, 7, 7)</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)<span class="comment">#相当于numpy中resize(),展开数据 (batch,32 * 7 * 7)</span></span><br><span class="line">        output = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">cnn=CNN()</span><br><span class="line">print(cnn)</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CNN(</span><br><span class="line">  (conv1): Sequential(</span><br><span class="line">    (0): Conv2d(1, 16, kernel_size&#x3D;(5, 5), stride&#x3D;(1, 1), padding&#x3D;(2, 2))</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0, dilation&#x3D;1, ceil_mode&#x3D;False)</span><br><span class="line">  )</span><br><span class="line">  (conv2): Sequential(</span><br><span class="line">    (0): Conv2d(16, 32, kernel_size&#x3D;(5, 5), stride&#x3D;(1, 1), padding&#x3D;(2, 2))</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0, dilation&#x3D;1, ceil_mode&#x3D;False)</span><br><span class="line">  )</span><br><span class="line">  (out): Linear(in_features&#x3D;1568, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># training and testing</span></span><br><span class="line">optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)  <span class="comment"># optimize all cnn parameters</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()  <span class="comment"># the target label is not one-hotted</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step,(b_x, b_y) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        output = cnn(b_x)</span><br><span class="line">        loss = loss_func(output, b_y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">50</span> == <span class="number">0</span>: <span class="comment">#每经过50个batch输出一次结果</span></span><br><span class="line">            test_output = cnn(test_x)</span><br><span class="line">            pred_y = torch.max(test_output,dim=<span class="number">1</span>)[<span class="number">1</span>].data.numpy()</span><br><span class="line">            accuracy = (sum(pred_y == np.array(test_y.data)).item()) / test_y.size(<span class="number">0</span>)</span><br><span class="line">            print(<span class="string">'Epoch:'</span>, epoch, <span class="string">'| train loss:%.4f'</span> % loss.item(), <span class="string">'| test accuracy:%.4f'</span> % accuracy)</span><br><span class="line">            </span><br><span class="line"><span class="comment"># print 10 predictions from test data</span></span><br><span class="line">test_output = cnn(test_x[:<span class="number">10</span>]) </span><br><span class="line">pred_y = torch.max(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.numpy().squeeze()</span><br><span class="line">print(pred_y, <span class="string">'prediction number'</span>)</span><br><span class="line">print(test_y[:<span class="number">10</span>].numpy(), <span class="string">'real number'</span>)</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 | train loss:2.3112 | test accuracy:0.1300</span><br><span class="line">Epoch: 0 | train loss:0.7076 | test accuracy:0.8125</span><br><span class="line">Epoch: 0 | train loss:0.2933 | test accuracy:0.8925</span><br><span class="line">Epoch: 0 | train loss:0.1877 | test accuracy:0.9265</span><br><span class="line">Epoch: 0 | train loss:0.0922 | test accuracy:0.9350</span><br><span class="line">Epoch: 0 | train loss:0.1003 | test accuracy:0.9540</span><br><span class="line">Epoch: 0 | train loss:0.3434 | test accuracy:0.9520</span><br><span class="line">Epoch: 0 | train loss:0.1386 | test accuracy:0.9560</span><br><span class="line">Epoch: 0 | train loss:0.0806 | test accuracy:0.9575</span><br><span class="line">Epoch: 0 | train loss:0.0409 | test accuracy:0.9605</span><br><span class="line">Epoch: 0 | train loss:0.0547 | test accuracy:0.9575</span><br><span class="line">Epoch: 0 | train loss:0.0700 | test accuracy:0.9650</span><br><span class="line">Epoch: 0 | train loss:0.1740 | test accuracy:0.9660</span><br><span class="line">Epoch: 0 | train loss:0.1522 | test accuracy:0.9665</span><br><span class="line">Epoch: 0 | train loss:0.0228 | test accuracy:0.9710</span><br><span class="line">Epoch: 0 | train loss:0.0173 | test accuracy:0.9665</span><br><span class="line">Epoch: 0 | train loss:0.0969 | test accuracy:0.9740</span><br><span class="line">Epoch: 0 | train loss:0.0521 | test accuracy:0.9740</span><br><span class="line">Epoch: 0 | train loss:0.0796 | test accuracy:0.9740</span><br><span class="line">Epoch: 0 | train loss:0.0387 | test accuracy:0.9715</span><br><span class="line">Epoch: 0 | train loss:0.0811 | test accuracy:0.9765</span><br><span class="line">Epoch: 0 | train loss:0.1526 | test accuracy:0.9765</span><br><span class="line">Epoch: 0 | train loss:0.0628 | test accuracy:0.9780</span><br><span class="line">Epoch: 0 | train loss:0.0181 | test accuracy:0.9775</span><br><span class="line">[7 2 1 0 4 1 4 9 5 9] prediction number</span><br><span class="line">[7 2 1 0 4 1 4 9 5 9] real number</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-id="ckn1t0idx000drgvtayu7ai74" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-决策树与随机森林" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" class="article-date">
  <time datetime="2020-07-30T06:54:09.000Z" itemprop="datePublished">2020-07-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习算法</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/">决策树与随机森林</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>既能用于分类，也能用于回归。这里是用于分类。</p>
<img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/1.png" style="zoom:67%;">

<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><p>如何构造决策树是最重要的。</p>
<p><img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%5C7.png" alt></p>
<img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/2.png" style="zoom:67%;">

<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/4.png" style="zoom:67%;">

<h4 id="选取根节点"><a href="#选取根节点" class="headerlink" title="选取根节点"></a>选取根节点</h4><img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/5.png" style="zoom:67%;">

<p>先计算最初样本的熵</p>
<img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/8.png" style="zoom:67%;">

<p>分别计算前四种划分后的熵</p>
<img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/9.png" style="zoom:67%;">

<p>熵最小的那个特征就选为根节点（用信息增益以及信息增益率判断）</p>
<p>信息增益率表示为信息增益/特征划分的样本的熵；越大说明这个特征作为节点越合适（例如添加一个特征ID编号从1到14，它的信息增益很大，然而这个特征是无用的）</p>
<img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/10.png" style="zoom:67%;">

<p>继续以这种方式构造子节点，最终得到一个决策树</p>
<img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/11.png" style="zoom:67%;">

<h4 id="熵与基尼系数"><a href="#熵与基尼系数" class="headerlink" title="熵与基尼系数"></a>熵与基尼系数</h4><p>熵指物体内部（这里我们可以指一个样本集）的混乱程度。因此一个集合中类别越多，熵就越大。</p>
<p>下图熵的表达式中Pi指样本集中第i个类的的概率（比如有10个样本分为两个类别，其中有4个1类，6个0类，那么1类的概率为2/5）</p>
<p>基尼系数（取值0-1）表达的含义与熵一样。</p>
<img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/3.png" style="zoom:67%;">



<h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>Nt指当前这个叶子节点有几个样本，H(t)指节点的熵或基尼系数。</p>
<img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/12.png" style="zoom:67%;">

<p>决策树太高太大分支太多会造成过拟合。我们要进行剪枝。</p>
<p>预剪枝包括将深度设置为三即最多三层，或者当叶子节点的样本数小于一个阈值（例如50个）时就停止。</p>
<p>后剪枝包括改变代价函数，|Tleaf|指叶子节点的个数。</p>
<img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/13.png" style="zoom:67%;">

<p>很明显决策树分到最后是可以实现叶子节点熵为0的，但我们还要考虑这个树是否分的太细过拟合了，就无法对新样本进行预测。</p>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>随机选择一些样本（有放回采样）以及特征，构造出一个决策树，再随机选一些样本以及特征，再构造一个决策树…得到一些决策树后，将分类结果用简单投票法得到最终分类，提高分类准确率。</p>
<p>随机森林属于集成学习（Ensemble Learning）中的bagging算法。<br>bagging的算法过程如下：</p>
<p>从原始样本集中使用Bootstraping方法(自助法，是一种有放回的抽样方法)随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复）</p>
<p>对于k个训练集，我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等）</p>
<p>对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同）</p>
<p>Bagging的特点：</p>
<p>样本选择上：Bagging采用的是Bootstrap随机有放回抽样；<br>样本权重：Bagging使用的是均匀取样，每个样本权重相等；<br>预测函数：Bagging所有的预测函数的权重相等；<br>并行计算：Bagging各个预测函数可以并行生成；</p>
<h2 id="用python构造决策树"><a href="#用python构造决策树" class="headerlink" title="用python构造决策树"></a>用python构造决策树</h2><h3 id="决策树的参数"><a href="#决策树的参数" class="headerlink" title="决策树的参数"></a>决策树的参数</h3><p>criterion：<br>特征选择标准，【entropy, gini】。默认gini，即CART算法。</p>
<p>splitter：<br>特征划分标准，【best, random】。best在特征的所有划分点中找出最优的划分点，random随机的在部分划分点中找局部最优的划分点。默认的‘best’适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐‘random’。</p>
<p>max_depth：<br>决策树最大深度，【int,  None】。默认值是‘None’。一般数据比较少或者特征少的时候可以不用管这个值，如果模型样本数量多，特征也多时，推荐限制这个最大深度，具体取值取决于数据的分布。常用的可以取值10-100之间，常用来解决过拟合。</p>
<p>min_samples_split：<br>内部节点（即判断条件）再划分所需最小样本数，【int, float】。默认值为2。如果是int，则取传入值本身作为最小样本数；如果是float，则取ceil(min_samples_split*样本数量)作为最小样本数。（向上取整）</p>
<p>min_samples_leaf：<br>叶子节点（即分类）最少样本数。如果是int，则取传入值本身作为最小样本数；如果是float，则取ceil(min_samples_leaf*样本数量)的值作为最小样本数。这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。</p>
<p>min_weight_fraction_leaf：<br>叶子节点（即分类）最小的样本权重和，【float】。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是0，就是不考虑权重问题，所有样本的权重相同。</p>
<p>一般来说如果我们有较多样本有缺失值或者分类树样本的分布类别偏差很大，就会引入样本权重，这时就要注意此值。</p>
<p>max_features：<br>在划分数据集时考虑的最多的特征值数量，【int值】。在每次split时最大特征数；【float值】表示百分数，即（max_features*n_features）</p>
<p>random_state：<br>【int, randomSate instance, None】，默认是None</p>
<p>max_leaf_nodes：<br>最大叶子节点数。【int, None】，通过设置最大叶子节点数，可以防止过拟合。默认值None，默认情况下不设置最大叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征多，可以加限制，具体的值可以通过交叉验证得到。</p>
<p>min_impurity_decrease：<br>节点划分最小不纯度，【float】。默认值为‘0’。限制决策树的增长，节点的不纯度（基尼系数，信息增益，均方差，绝对差）必须大于这个阈值，否则该节点不再生成子节点。</p>
<p>min_impurity_split（已弃用）：<br>信息增益的阀值。决策树在创建分支时，信息增益必须大于这个阈值，否则不分裂。（从版本0.19开始不推荐使用：min_impurity_split已被弃用，以0.19版本中的min_impurity_decrease取代。 min_impurity_split的默认值将在0.23版本中从1e-7变为0，并且将在0.25版本中删除。 请改用min_impurity_decrease。）</p>
<p>class_weight：<br>类别权重，【dict, list of dicts, balanced】，默认为None。（不适用于回归树，sklearn.tree.DecisionTreeRegressor）</p>
<p>指定样本各类别的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。balanced，算法自己计算权重，样本量少的类别所对应的样本权重会更高。如果样本类别分布没有明显的偏倚，则可以不管这个参数。</p>
<p>presort：<br>bool，默认是False，表示在进行拟合之前，是否预分数据来加快树的构建。</p>
<p>对于数据集非常庞大的分类，presort=true将导致整个分类变得缓慢；当数据集较小，且树的深度有限制，presort=true才会加速分类。</p>
<h3 id="随机森林的参数"><a href="#随机森林的参数" class="headerlink" title="随机森林的参数"></a>随机森林的参数</h3><p>（1） 决策树的个数</p>
<p>（2） 特征属性的个数</p>
<p>（3） 递归次数（即决策树的深度）</p>
<img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/14.png" style="zoom:67%;">



<p>对于回归问题：</p>
<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。<br>  GBDT中的树是回归树（不是分类树），GBDT用来做回归预测，调整后也可以用于分类。<br>  GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。业界中，Facebook使用其来自动发现有效的特征、特征组合，来作为LR模型中的特征，以提高 CTR预估（Click-Through Rate Prediction）的准确性；GBDT在淘宝的搜索及预测业务上也发挥了重要作用。</p>
<h3 id="Regression-Decision-Tree：回归树"><a href="#Regression-Decision-Tree：回归树" class="headerlink" title="Regression Decision Tree：回归树"></a>Regression Decision Tree：回归树</h3><p>回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化平方误差。也就是被预测出错的人数越多，错的越离谱，平方误差就越大，通过最小化平方误差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。</p>
<p><img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%5Cc.jpg" alt></p>
<h3 id="Boosting-Decision-Tree：提升树算法"><a href="#Boosting-Decision-Tree：提升树算法" class="headerlink" title="Boosting Decision Tree：提升树算法"></a>Boosting Decision Tree：提升树算法</h3><p>提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。提升树即是整个迭代过程生成的回归树的累加。</p>
<p>例子：训练一个提升树模型来预测年龄：<br>  训练集是4个人，A，B，C，D年龄分别是14，16，24，26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下：</p>
<p><img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%5Ca.jpg" alt></p>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>XGBoost是Exterme Gradient Boosting（极限梯度提升）的缩写，它是基于决策树的集成机器学习算法，它以梯度提升（Gradient Boost）为框架。XGBoost是由由GBDT发展而来，同样是利用加法模型与前向分步算法实现学习的优化过程，但与GBDT是有区别的。主要区别包括以下几点：</p>
<ul>
<li>目标函数：XGBoost的损失函数添加了正则化项，使用正则用以控制模型的复杂度，正则项里包含了树的叶子节点个数、每个叶子节点权重（叶结点的socre值）的平方和。</li>
<li>优化方法：GBDT在优化时只使用了一阶导数信息，XGBoost在优化时使用了一、二介导数信息。</li>
<li>缺失值处理：XBGoost对缺失值进行了处理，通过学习模型自动选择最优的缺失值默认切分方向。</li>
<li>防止过拟合: XGBoost除了增加了正则项来防止过拟合,还支持行列采样的方式来防止过拟合。</li>
<li>结果：它可以在最短时间内用更少的计算资源得到更好的结果。</li>
</ul>
<p>XGBoost的可以使用Regression Tree（CART）作为基学习器，也可以使用线性分类器作为基学习器。以CART作为基学习器时，其决策规则和决策树是一样的，但CART的每一个叶节点具有一个权重，也就是叶节点的得分或者说是叶节点的预测值。CART的示例如下图：</p>
<p><img src="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%5Cb.jpg" alt></p>
<p>图中为两颗回归树（左右两个），其中树下方的输出值即为叶节点的权重（得分），当输出一个样本进行预测时，根据每个内部节点的决策条件进行划分节点，最终被划分到的叶节点的权重即为该样本的预测输出值。例如对于小男孩的预测结果为2+0.9=2.9</p>
<h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a><strong>LightGBM</strong></h2><p>常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制。而GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。</p>
<p>LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业实践。</p>
<p><strong>（1）XGBoost的缺点</strong></p>
<p>在LightGBM提出之前，最有名的GBDT工具就是XGBoost了，它是基于预排序方法的决策树算法。这种构建决策树的算法基本思想是：首先，对所有特征都按照特征的数值进行预排序。其次，在遍历分割点的时候用O(#data)的代价找到一个特征上的最好分割点。最后，在找到一个特征的最好分割点后，将数据分裂成左右子节点。</p>
<p>这样的预排序算法的优点是能精确地找到分割点。但是缺点也很明显：首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。最后，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。</p>
<p><strong>（2）LightGBM的优化</strong></p>
<p>为了避免上述XGBoost的缺陷，并且能够在不损害准确率的条件下加快GBDT模型的训练速度，lightGBM在传统的GBDT算法上进行了如下优化：</p>
<ul>
<li>基于Histogram的决策树算法。</li>
<li>单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。</li>
<li>互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。</li>
<li>带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。</li>
<li>直接支持类别特征(Categorical Feature)</li>
<li>支持高效并行</li>
<li>Cache命中率优化</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" data-id="ckn1t0igj001krgvtf3gt0uvk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" rel="tag">决策树与随机森林</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Java基础篇学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2020-07-30T06:48:46.000Z" itemprop="datePublished">2020-07-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Java基础篇学习笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h2><h3 id="java程序的编译与执行"><a href="#java程序的编译与执行" class="headerlink" title="java程序的编译与执行"></a>java程序的编译与执行</h3><p>JDK/bin：该目录下存放了很多命令。如：</p>
<p>javac.exe负责编译</p>
<p>java.exe负责运行</p>
<p>编译java程序</p>
<p>写好helloworld.java程序后，打开cmd输入命令进行编译</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">javac D:\helloworld.java</span><br></pre></td></tr></table></figure>

<p>此时在这个程序文件路径下生成了一个class文件。</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C1.png" alt></p>
<p>运行java程序</p>
<p>进入class目录打开cmd。输入命令： java+类名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java helloworld</span><br></pre></td></tr></table></figure>

<p>就得到运行结果</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C2.png" alt></p>
<h3 id="注释规则"><a href="#注释规则" class="headerlink" title="注释规则"></a>注释规则</h3><p>单行注释</p>
<p>//</p>
<p>多行注释</p>
<p>/*</p>
<p>多行</p>
<p>*/</p>
<p>javadoc注释（我们可以通过 <strong>javadoc</strong> 命令从文档注释中提取内容，生成程序的 API 帮助文档。）</p>
<p>/**</p>
<p>*javadoc注释</p>
<p>*/</p>
<h3 id="Java的class"><a href="#Java的class" class="headerlink" title="Java的class"></a>Java的class</h3><p>一个java源文件可以定义多个class，对应会生成多个.class字节码文件</p>
<p>public的class不是必须的，如果有只能有一个，且该类名称必须与java源文件名称一致。</p>
<h2 id="第二章–Java语言基础"><a href="#第二章–Java语言基础" class="headerlink" title="第二章–Java语言基础"></a>第二章–Java语言基础</h2><h3 id="Java数据类型"><a href="#Java数据类型" class="headerlink" title="Java数据类型"></a>Java数据类型</h3><p>Java 语言是一种<strong>强类型</strong>语言。通俗点说就是，在 Java 中存储的数据都是有类型的，而且必须在编译时就确定其类型。 Java 中有两类数据类型：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C3.jpg" alt></p>
<p>在 Java 的领域里<strong>，基本数据类型变量</strong>存的是<strong>数据本身</strong>，而<strong>引用类型变量</strong>存的是保存<strong>数据的空间地址</strong>。说白了，基本数据类型变量里存储的是直接放在抽屉里的东西，而引用数据类型变量里存储的是这个抽屉的钥匙，钥匙和抽屉一一对应。</p>
<h4 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h4><p>int型可以自动转换为double型，但反之不行。double 类型长度为 8 字节， int 类型为 4 字节。</p>
<p>double型可以强制转换为int型，操作如下：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C4.jpg" alt></p>
<h4 id="标识符"><a href="#标识符" class="headerlink" title="标识符"></a>标识符</h4><p>例如类名，接口名，方法名，变量名…</p>
<p>方法名一般采用第一个单词首字母小写，其它单词首字母大写的形式</p>
<p>使用标识符时，需要遵守几条规则：</p>
<ol>
<li><p>标识符<strong>可以</strong>由<strong>字母、数字、下划线（_）、美元符（$）</strong>组成，但不能包含 @、%、空格等其它特殊字符<strong>，不能以数字开头</strong>。譬如：123name 就是不合法滴</p>
</li>
<li><p>标识符<strong>不能是 Java 关键字和保留字</strong>（ Java 预留的关键字，以后的升级版本中有可能作为关键字），但可以包含关键字和保留字。如：不可以使用 void 作为标识符，但是 Myvoid 可以</p>
</li>
<li><p>标识符是<strong>严格区分大小写</strong>的。</p>
</li>
</ol>
<h4 id="关键字"><a href="#关键字" class="headerlink" title="关键字"></a>关键字</h4><p>java中的一些常用关键字如public，class，void…、</p>
<h5 id="this关键字"><a href="#this关键字" class="headerlink" title="this关键字"></a>this关键字</h5><p>this关键字代表当前对象。经常用于封装中。</p>
<p>this.属性 操作当前对象的属性</p>
<p>this.方法 操作当前对象的属性</p>
<h4 id="字面值"><a href="#字面值" class="headerlink" title="字面值"></a>字面值</h4><p>字面值就是数据</p>
<p>如100(整数型)，3.14(浮点数型),”abc”(字符串型),true,false(布尔型)，’a’(字符型)</p>
<h4 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h4><p>Java中的变量需要先声明再使用。命名规则：</p>
<p>1、变量名由多单词组成时，<strong>第一个单词的首字母小写</strong>，<strong>其后单词的首字母大写</strong>，俗称骆驼式命名法（也称驼峰命名法），如 myAge</p>
<p>2、变量命名时，尽量简短且能清楚的表达变量的作用，做到<strong>见名知意</strong>。如：定义变量名 stuName 保存“学生姓名”信息</p>
<p>PS： Java 变量名的<strong>长度没有限制</strong>，但 Java 语言<strong>是区分大小写</strong>的，所以 price 和 Price 是两个完全不同的变量。</p>
<h4 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h4><p>所谓常量，我们可以理解为是一种特殊的变量，它的值被设定后，在<strong>程序运行过程中不允许改变</strong>。常量名一般使用<strong>大写字符</strong></p>
<p>语法：<strong>final 常量名 = 值;</strong></p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C5.jpg" alt></p>
<h4 id="枚举变量"><a href="#枚举变量" class="headerlink" title="枚举变量"></a>枚举变量</h4><p>枚举类型是一种特殊的数据类型（特殊的类），它允许变量成为一组变量中的一个。<br>预定义常量<br>常见示例：<br>指南针方向（北，南，东，西）<br>星期几，一年中的月份等</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public enum Day &#123;</span><br><span class="line">SUNDAY, MONDAY, TUESDAY, WEDNESDAY,</span><br><span class="line">THURSDAY, FRIDAY, SATURDAY,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>它有方法<br>内置的静态方法values（）返回所有值的数组<br>内置的静态方法valueOf（）将字符串解析为枚举常量<br>compareTo（），equals（），hashCode（），toString（）的适当定义<br>其他方法：<br>ordinal（）-返回此常量在列表中的位置<br>name（）-返回此常量的名称</p>
<h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><h4 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h4><p>a=1</p>
<p>b=++a表示将a+1后再赋值给b。</p>
<p>b=a++表示先将a赋值给b再+1。</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C6.jpg" alt></p>
<h4 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h4><p>1、  &gt; 、 &lt; 、 &gt;= 、 &lt;= 只支持左右两边操作数是数值类型</p>
<p>2、  == 、 != 两边的操作数既可以是数值类型，也可以是引用类型</p>
<h4 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h4><p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C7.jpg" alt></p>
<h4 id="条件运算符"><a href="#条件运算符" class="headerlink" title="条件运算符"></a>条件运算符</h4><p>条件运算符（ ? : ）也称为 “三元运算符”。</p>
<p>语法形式：<strong>布尔表达式 ？ 表达式1 ：表达式2</strong></p>
<p>运算过程：如果布尔表达式的值为 <strong>true</strong> ，则返回 <strong>表达式1</strong> 的值，否则返回 <strong>表达式2</strong> 的值</p>
<p>例如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C8.jpg" alt></p>
<p>因为，表达式 8&gt;5 的值为 true ，所以，返回： <strong>8大于5</strong></p>
<h3 id="Java条件语句之-if"><a href="#Java条件语句之-if" class="headerlink" title="Java条件语句之 if"></a>Java条件语句之 if</h3><h4 id="简单if语句"><a href="#简单if语句" class="headerlink" title="简单if语句"></a>简单if语句</h4><p>语法：</p>
<p>if(条件){</p>
<p>条件成立时执行的代码</p>
<p>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">String[] fruits &#x3D; new String[] &#123; “apple”, “banana”, “cherry” ];</span><br><span class="line">Standard Java idiom: for loop</span><br><span class="line">for (int i &#x3D; 0; i &lt; fruits.length; i++) &#123;</span><br><span class="line">System.out.println(fruits[i]);</span><br><span class="line">&#125;</span><br><span class="line">More efficient option: for-each loop (since Java 1.5) – similar to Python’s for loop</span><br><span class="line">for (String fruit : fruits) &#123;</span><br><span class="line">System.out.println (fruit);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意：如果 if 条件成立时的执行语句只有一条，是可以省略大括号的。但如果执行语句有多条，那么大括号不可或缺。</p>
<h4 id="if…else-语句"><a href="#if…else-语句" class="headerlink" title="if…else 语句"></a><strong>if…else</strong> 语句</h4><p><strong>if…else</strong> 语句的操作比 if 语句多了一步:  当条件成立时，则执行 if 部分的代码块； 条件不成立时，则进入 else 部分。</p>
<p>语法：</p>
<p>if(条件){</p>
<p>代码块1</p>
<p>}else{</p>
<p>代码块2</p>
<p>}</p>
<h4 id="多重-if-语句"><a href="#多重-if-语句" class="headerlink" title="多重 if 语句"></a>多重 if 语句</h4><p><strong>多重 if 语句</strong>，在条件 1 不满足的情况下，才会进行条件 2 的判断；当前面的条件均不成立时，才会执行 else 块内的代码。例如，如果考试成绩大于 90 分，则奖励一个 IPHONE 5S ，如果成绩介于 70 分至 90 分之间，则奖励一个红米，否则罚做 500 个俯卧撑。</p>
<p>语法：</p>
<p>if(条件1){</p>
<p>代码块1</p>
<p>}else if(条件2){</p>
<p>代码块2</p>
<p>}else{</p>
<p>代码块3</p>
<p>}</p>
<p>注意：条件里不能写成区间如40&gt;age&gt;=18</p>
<h3 id="Java条件语句之Switch"><a href="#Java条件语句之Switch" class="headerlink" title="Java条件语句之Switch"></a>Java条件语句之Switch</h3><p>当需要对选项进行等值判断时，使用 switch 语句更加简洁明了。例如：根据考试的名次，给予前 4 名不同的奖品。第一名，奖励笔记本一台；第二名，奖励 IPAD 2 一个；第三名，奖励移动电源一个；最后一名奖励 U 盘一个。</p>
<p>语法：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C9.jpg" alt></p>
<p>执行过程：当 switch 后表达式的值和 case 语句后的值相同时，从该位置开始向下执行，直到遇到 break 语句或者 switch 语句块结束；如果没有匹配的 case 语句则执行 default 块的代码。</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C10.jpg" alt></p>
<p>注意：</p>
<p>1、 switch 后面小括号中表达式的值必须是<strong>整型或字符型</strong></p>
<p>2、 case 后面的值<strong>可以是常量数值</strong>，如 1、2，字符，字符串等；也<strong>可以是一个常量表达式</strong>，如 2+2 ；但不能是变量或带有变量的表达式，如 a * 2</p>
<p>3、 case 匹配后，执行匹配块里的程序代码，如果没有遇见 <strong>break</strong> 会继续执行下一个的 case 块的内容，直到遇到 break 语句或者 switch 语句块结束 如</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C11.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C12.jpg" alt></p>
<p>4、 可以把功能相同的 case 语句合并起来，如</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C13.jpg" alt></p>
<p>5、 default 块可以出现在<strong>任意位置</strong>，也<strong>可以省略</strong></p>
<h3 id="Java循环语句"><a href="#Java循环语句" class="headerlink" title="Java循环语句"></a>Java循环语句</h3><h4 id="while"><a href="#while" class="headerlink" title="while"></a>while</h4><p>语法：</p>
<p>while(判断条件){</p>
<p>循环操作</p>
<p>}</p>
<p>执行过程：</p>
<p>&lt; 1 &gt;、 判断 while 后面的条件是否成立( true / false )</p>
<p>&lt; 2 &gt;、 当条件成立时，执行循环内的操作代码 ，然后重复执行&lt; 1 &gt;、&lt; 2 &gt;， 直到循环条件不成立为止</p>
<p>特点： <strong>先判断，后执行</strong></p>
<h4 id="do…while"><a href="#do…while" class="headerlink" title="do…while"></a>do…while</h4><p><strong>do…while 循环</strong>与 while 循环语法有些类似，但执行过程差别比较大。</p>
<p>语法：</p>
<p>do{</p>
<p>循环操作</p>
<p>}while(判断条件);</p>
<p>执行过程：</p>
<p>&lt;1&gt;、 先执行一遍循环操作，然后判断循环条件是否成立</p>
<p>&lt;2&gt;、 如果条件成立，继续执行&lt; 1 &gt; 、&lt; 2 &gt;，直到循环条件不成立为止</p>
<p>特点： <strong>先执行，后判断</strong></p>
<p>由此可见，do…while 语句保证循环<strong>至少被执行一次</strong>！</p>
<h4 id="for"><a href="#for" class="headerlink" title="for"></a>for</h4><p>Java 的循环结构中除了 while 和 do…while 外，还有 <strong>for 循环</strong>，三种循环可以相互替换。</p>
<p>语法： </p>
<p>for (循环变量初始化; 循环条件; 循环变量变化){</p>
<p>循环操作</p>
<p>}</p>
<p>执行过程：</p>
<p>&lt;1&gt;、 执行循环变量初始化部分，设置循环的初始状态，此部分在整个循环中只执行一次</p>
<p>&lt;2&gt;、 进行循环条件的判断，如果条件为 true ，则执行循环体内代码；如果为 false ，则直接退出循环</p>
<p>&lt;3&gt;、 执行循环变量变化部分，改变循环变量的值，以便进行下一次条件判断</p>
<p>&lt;4&gt;、 依次重新执行&lt; 2 &gt;、&lt; 3 &gt;、&lt; 4 &gt;，直到退出循环</p>
<p>特点：相比 while 和 do…while 语句<strong>结构更加简洁易读</strong></p>
<p>例如，输出 1000 遍“我爱慕课网”，使用 for 的实现代码为：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C14.jpg" alt></p>
<p>注意：</p>
<p>1、 for 关键字后面括号中的三个表达式必须用 “;” 隔开，三个表达式都可以省略，但 “;” 不能省略。</p>
<p>  a. 省略“循环变量初始化”，可以在 for 语句之前由赋值语句进行变量初始化操作，如:</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C15.jpg" alt></p>
<p> b. 省略“循环条件”，可能会造成循环将一直执行下去，也就是我们常说的“死循环”现象，可以在循环体中使用 break 强制跳出循环</p>
<p>c. 省略“循环变量变化”，可以在循环体中进行循环变量的变化，如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C16.jpg" alt></p>
<p>2、 for 循环变量初始化和循环变量变化部分，可以是使用 “,” 同时初始化或改变多个循环变量的值，如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C17.jpg" alt></p>
<p>3、 循环条件部分可以使用逻辑运算符组合的表达式，表示复杂判断条件，但一定注意运算的优先级，如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C18.jpg" alt></p>
<p>代码中，必须同时满足变量 i 小于 10 ，并且 i 不等于 5 时才会进行循环，输出变量 i 的值。</p>
<h3 id="Java循环跳转语句之-break"><a href="#Java循环跳转语句之-break" class="headerlink" title="Java循环跳转语句之 break"></a>Java循环跳转语句之 break</h3><p>例如，使用循环输出 1–10的数值，其中，如果数值大于 2 ，并且为 3 的倍数则停止输出。</p>
<p>实现代码：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C19.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C20.jpg" alt></p>
<h3 id="Java循环跳转语句之-continue"><a href="#Java循环跳转语句之-continue" class="headerlink" title="Java循环跳转语句之 continue"></a>Java循环跳转语句之 continue</h3><p><strong>continue</strong> 的作用是跳过循环体中剩余的语句执行下一次循环。</p>
<p>例如，打印 1–10 之间所有的偶数，使用 continue 语句实现代码为：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C21.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C22.jpg" alt></p>
<p>java的输入操作</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line">Scanner input=<span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> score=input.nextInt(); <span class="comment">//获取输入的成绩</span></span><br></pre></td></tr></table></figure>

<h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><h4 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a>创建数组</h4><p>数组可以理解为是一个巨大的“盒子”，里面可以按顺序存放多个类型相同的数据，比如可以定义 int 型的数组 scores 存储 4 名学生的成绩</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C23.jpg" alt></p>
<p>数组中的元素都可以通过下标来访问，下标从 0 开始。例如，可以通过 scores[0] 获取数组中的第一个元素 76 ，scores[2] 就可以取到第三个元素 92</p>
<p>Java 中操作数组只需要四个步骤：</p>
<p><strong>1、 声明数组</strong></p>
<p>语法： <strong>数据类型[ ] 数组名；</strong></p>
<p>或者  <strong>数据类型 数组名[ ]；</strong></p>
<p>其中，数组名可以是任意合法的变量名，如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C24.jpg" alt></p>
<p><strong>2、 分配空间</strong></p>
<p>简单地说，就是指定数组中最多可存储多少个元素</p>
<p>语法： <strong>数组名 = new  数据类型 [ 数组长度 ];</strong></p>
<p>其中，数组长度就是数组中能存放元素的个数，如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C25.jpg" alt></p>
<p>我们也可以将上面的两个步骤合并，在声明数组的同时为它分配空间，如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C29.jpg" alt></p>
<p><strong>3、 赋值</strong></p>
<p>分配空间后就可以向数组中放数据了，数组中元素都是通过下标来访问的，例如向 scores 数组中存放学生成绩</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C26.jpg" alt></p>
<p><strong>4、 处理数组中数据</strong></p>
<p>我们可以对赋值后的数组进行操作和处理.</p>
<p>在 Java 中还提供了另外一种直接创建数组的方式，它将声明数组、分配空间和赋值合并完成，如</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C27.jpg" alt></p>
<p>它等价于：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C28.jpg" alt></p>
<h4 id="使用循环操作-Java-中的数组"><a href="#使用循环操作-Java-中的数组" class="headerlink" title="使用循环操作 Java 中的数组"></a>使用循环操作 Java 中的数组</h4><p>我们经常使用循环控制数组成员的操作。如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C30.jpg" alt></p>
<h4 id="使用-Arrays-类操作-Java-中的数组"><a href="#使用-Arrays-类操作-Java-中的数组" class="headerlink" title="使用 Arrays 类操作 Java 中的数组"></a>使用 Arrays 类操作 Java 中的数组</h4><p>Arrays 类是 Java 中提供的一个工具类，在 java.util 包中。该类中包含了一些方法用来直接操作数组，比如可直接实现数组的排序、搜索等。</p>
<p>Arrays 中常用的方法：</p>
<h5 id="1、-排序"><a href="#1、-排序" class="headerlink" title="1、 排序"></a><strong>1、 排序</strong></h5><p>语法： <strong>Arrays.sort(数组名);</strong></p>
<p>可以使用 sort( ) 方法实现对数组的排序，只要将数组名放在 sort( ) 方法的括号中，就可以完成对该数组的排序（按升序排列），如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C31.jpg" alt></p>
<p>运行结果： </p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C32.jpg" alt></p>
<h5 id="2、-将数组转换为字符串"><a href="#2、-将数组转换为字符串" class="headerlink" title="2、 将数组转换为字符串"></a><strong>2、 将数组转换为字符串</strong></h5><p>语法： <strong>Arrays.toString(数组名);</strong></p>
<p>可以使用 toString( ) 方法将一个数组转换成字符串，该方法按顺序把多个数组元素连接在一起，多个元素之间使用逗号和空格隔开，如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C33.jpg" alt></p>
<p>运行结果为： </p>
<p><strong>输出数组nums中的元素：[25，7，126，53，14，86]</strong></p>
<h4 id="使用-foreach-操作数组"><a href="#使用-foreach-操作数组" class="headerlink" title="使用 foreach 操作数组"></a>使用 foreach 操作数组</h4><p>foreach 并不是 Java 中的关键字，是 for 语句的特殊简化版本，在遍历数组、集合时， foreach 更简单便捷。</p>
<p>我们分别使用 for 和 foreach 语句来遍历数组：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C34.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C35.jpg" alt></p>
<h3 id="二维数组"><a href="#二维数组" class="headerlink" title="二维数组"></a>二维数组</h3><p>所谓二维数组，可以简单的理解为是一种“特殊”的一维数组，它的每个数组空间中保存的是一个一维数组。</p>
<p>那么如何使用二维数组呢，步骤如下：</p>
<p><strong>1、 声明数组并分配空间</strong></p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C37.jpg" alt></p>
<p>或者</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C36.jpg" alt></p>
<p><strong>2、 赋值</strong></p>
<p>二维数组的赋值，和一维数组类似，可以通过下标来逐个赋值，注意索引从 0 开始</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C38.jpg" alt></p>
<p>也可以在声明数组的同时为其赋值</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C39.jpg" alt></p>
<p><strong>3、 处理数组</strong></p>
<p>二维数组的访问和输出同一维数组一样，只是多了一个下标而已。在循环输出时，需要里面再内嵌一个循环，即使用二重循环来输出二维数组中的每一个元素。如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C40.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C41.jpg" alt></p>
<p><strong>需要了解的：</strong>在定义二维数组时也可以只指定行的个数，然后再为每一行分别指定列的个数。如果每行的列数不同，则创建的是不规则的二维数组，如下所示：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C42.jpg" alt></p>
<p>运行结果为：</p>
<p>1</p>
<p>2</p>
<p>3</p>
<h2 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h2><h3 id="定义-Java-中的方法"><a href="#定义-Java-中的方法" class="headerlink" title="定义 Java 中的方法"></a>定义 Java 中的方法</h3><p>所谓方法，就是用来解决一类问题的</p>
<p>代码的有序组合，是一个功能模块。</p>
<p>一般情况下，定义一个方法的语法是：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C43.jpg" alt></p>
<p>其中：</p>
<h4 id="1、-访问修饰符"><a href="#1、-访问修饰符" class="headerlink" title="1、 访问修饰符"></a>1、 访问修饰符</h4><p><strong>1.访问控制修饰符</strong></p>
<p> 访问修饰符：方法允许被访问的权限范围， 可以是 public、protected、private 甚至可以省略 ，其中 public 表示该方法或变量可以被其他任何代码调用。</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C64.png" alt></p>
<p><strong>2.非访问控制修饰符</strong></p>
<p>   抽象方法控制符<strong>abstract</strong> 、静态方法控制符<strong>static</strong> 、最终方法控制符<strong>final</strong> 、本地方法控制符<strong>native</strong> 、同步方法控制符<strong>synchronized</strong></p>
<p>2、 返回值类型：方法返回值的类型，如果方法不返回任何值，则返回值类型指定为 void ；如果方法具有返回值，则需要指定返回值的类型，并且在方法体中使用 return 语句返回值</p>
<p>3、 方法名：定义的方法的名字，必须使用合法的标识符</p>
<p>4、 参数列表：传递给方法的参数列表，参数可以有多个，多个参数间以逗号隔开，每个参数由参数类型和参数名组成，以空格隔开 </p>
<h4 id="例1–无参无返回值（创建对象，调用方法）"><a href="#例1–无参无返回值（创建对象，调用方法）" class="headerlink" title="例1–无参无返回值（创建对象，调用方法）"></a>例1–无参无返回值（创建对象，调用方法）</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line"><span class="comment">//定义了一个方法名为 print 的方法，实现输出信息功能</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		System.out.println(<span class="string">"Hello World"</span>);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在 main 方法中调用 print 方法</span></span><br><span class="line">    <span class="comment">//创建对象，对象名为test</span></span><br><span class="line">    	HelloWorld test=<span class="keyword">new</span> HelloWorld();</span><br><span class="line">    <span class="comment">//调用之前定义的print方法</span></span><br><span class="line">    	test.print();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当需要调用方法执行某个操作时，可以先创建类的对象，然后通过  <strong>对象名.方法名();</strong> 来实现</p>
<h4 id="例2–无参带返回值"><a href="#例2–无参带返回值" class="headerlink" title="例2–无参带返回值"></a>例2–无参带返回值</h4><p>例如：下面的代码，定义了一个方法名为 calSum ，无参数，但返回值为 int 类型的方法，执行的操作为计算两数之和，并返回结果</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C44.jpg" alt></p>
<p>在 calSum( ) 方法中，返回值类型为 int 类型，因此在方法体中必须使用 return 返回一个整数值。</p>
<p>调用带返回值的方法时需要注意，由于方法执行后会返回一个结果，因此在调用带返回值方法时一般都会接收其返回值并进行处理。如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C45.jpg" alt></p>
<p>运行结果为： <strong>两数之和为：17</strong></p>
<p><strong>不容忽视的“小陷阱”：</strong></p>
<p>1、 如果方法的返回类型为 void ，则方法中不能使用 return 返回值！</p>
<p>2、 方法的返回值最多只能有一个，不能返回多个值</p>
<p>3、 方法返回值的类型必须兼容，例如，如果返回值类型为 int ，则不能返回 String 型值</p>
<h4 id="例3–带参无返回值"><a href="#例3–带参无返回值" class="headerlink" title="例3–带参无返回值"></a>例3–带参无返回值</h4><p>我们可以通过在方法中加入参数列表接收外部传入的数据信息，参数可以是任意的基本类型数据或引用类型数据。</p>
<p>我们先来看一个带参数，但没有返回值的方法：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C46.jpg" alt></p>
<p>上面的代码定义了一个 show 方法，带有一个参数 name ，实现输出欢迎消息。</p>
<p>调用带参方法与调用无参方法的语法类似，但在调用时必须传入实际的参数值</p>
<p>例如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C47.jpg" alt></p>
<p>运行结果为： <strong>欢迎您，爱慕课！</strong></p>
<p>很多时候，我们把定义方法时的参数称为<strong>形参</strong>，目的是用来定义方法需要传入的参数的个数和类型；把调用方法时的参数称为<strong>实参</strong>，是传递给方法真正被处理的值。</p>
<p><strong>一定不可忽视的问题：</strong></p>
<p>1、 调用带参方法时，必须保证实参的数量、类型、顺序与形参一一对应</p>
<p>2、 调用方法时，实参不需要指定数据类型</p>
<p>3、 方法的参数可以是基本数据类型，如 int、double 等，也可以是引用数据类型，如 String、数组等</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C48.jpg" alt></p>
<p>4、 当方法参数有多个时，多个参数间以逗号分隔</p>
<h4 id="例4–带参带返回值"><a href="#例4–带参带返回值" class="headerlink" title="例4–带参带返回值"></a>例4–带参带返回值</h4><p>如果方法既包含参数，又带有返回值，我们称为带参带返回值的方法。</p>
<p>例如：下面的代码，定义了一个 show 方法，带有一个参数 name ，方法执行后返回一个 String 类型的结果</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C49.jpg" alt></p>
<p>调用带参带返回值的方法：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C50.jpg" alt></p>
<p>运行结果为： <strong>欢迎您，爱慕课！</strong></p>
<h3 id="Java-中方法的重载"><a href="#Java-中方法的重载" class="headerlink" title="Java 中方法的重载"></a>Java 中方法的重载</h3><p> 如果同一个类中包含了两个或两个以上方法名相同、方法参数的个数、顺序或类型不同的方法，则称为方法的重载，也可称该方法被重载了。如下所示 4 个方法名称都为 show ，但方法的参数有所不同，因此都属于方法的重载：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C51.jpg" alt></p>
<p>当调用被重载的方法时， Java 会根据参数的个数和类型来判断应该调用哪个重载方法，参数完全匹配的方法将被执行。如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C52.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C53.jpg" alt></p>
<p><strong>判断方法重载的依据：</strong></p>
<p>1、 必须是在同一个类中</p>
<p>2、 方法名相同</p>
<p>3、 方法参数的个数、顺序或类型不同</p>
<p>4、 与方法的修饰符或返回值没有关系</p>
<h2 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h2><h3 id="类与对象"><a href="#类与对象" class="headerlink" title="类与对象"></a>类与对象</h3><p><strong>类</strong>：类是一个模板，它描述一类对象的行为（方法）和状态（属性），比如把狗作为一类，把猫作为另一类。</p>
<h3 id="类修饰符"><a href="#类修饰符" class="headerlink" title="类修饰符"></a>类修饰符</h3><p><strong><em>\</em>1.访问修饰符：公共类修饰符public**</strong></p>
<p>  公共类修饰符 public ： Java 语言中类的访问控制符只有 public 即公共的。每个 Java 程序的有且只有一个类是 public，它被称为主类 ，其他外部类无访问控制修饰符，具有包访问性。注意：一个类的内部类可以被其他访问控制修饰符protected、缺省默认(default、friendly)、private修饰，相当于类的成员。</p>
<p> 注意：Java类或属性如果缺省访问控制修饰符，就属于default/friendly类型修饰符，但是实际上Java中并没有名为default或者friendly的访问修饰符(即不能使用default或者friendly定义类或变量)，只是为了方便标识缺省访问控制符的情况。</p>
<p><strong><em>\</em>2.非访问控制符：抽象类修饰符 abstract 、最终类修饰符 final**</strong></p>
<p>  （1）抽象类修饰符 abstract ：用 abstract 修饰符修饰的类，被称为抽象类。</p>
<p>  （2）最终类修饰符 final ：当一个类不能被继承时可用修饰符 final修饰为最终类。被定义为 final 的类通常是一些有固定作用、用来完成某种标准功能的类。</p>
<p>  （3）类缺省访问控制符：如果一个类没有访问控制符，说明它具有缺省的访问控制符特性。此时，这个类只能被同一个包中的类访问或引用。这一访问特性又称为包访问性。</p>
<p>定义类的的步骤</p>
<p>a.定义类名</p>
<p>b.编写类的属性</p>
<p>c.编写类的方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class 类名&#123;</span><br><span class="line">属性1的类型 属性1；</span><br><span class="line">属性2的类型 属性2；</span><br><span class="line">...</span><br><span class="line">方法1；</span><br><span class="line">方法2；</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>对象</strong>：对象是类的一个实例，有状态和行为。例如，一条狗是一个对象，它的状态有：颜色、名字、品种；行为有：摇尾巴、叫、吃等。</p>
<p>软件对象也有状态和行为。软件对象的状态就是属性，行为通过方法体现。</p>
<p>先创建一个类（创建这个类时，类名设置为Student，package名设置为test）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test；</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span></span>&#123;</span><br><span class="line">    String name;</span><br><span class="line">    <span class="keyword">int</span> age;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Study</span><span class="params">()</span></span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>再创建一个类，在里面实例化一个Student类对象，对对象的属性赋值,或调用Student类方法。例如</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> test；<span class="comment">//导入刚才创建的test包</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InitialStudent</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        Student pupil1=<span class="keyword">new</span> Student();<span class="comment">//使用new+构造方法创建对象</span></span><br><span class="line">        pupil1.age=<span class="number">12</span>; <span class="comment">//对对象的属性赋值</span></span><br><span class="line">        pupil1.name=<span class="string">"XiaoHong"</span>;<span class="comment">//对对象的属性赋值</span></span><br><span class="line">        pupil1.Study();<span class="comment">//调用方法</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h3><p>构造方法是指定义在Java类中的一个用来初始化对象的方法。构造方法与类同名且没有返回值。 </p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C54.png" alt></p>
<p>一个类里可以有多个构造方法，只要它们不是一模一样。</p>
<h3 id="成员变量与局部变量"><a href="#成员变量与局部变量" class="headerlink" title="成员变量与局部变量"></a>成员变量与局部变量</h3><p>成员变量：在类中定义，用来描述对象将要有什么。能够被本类或其他类的方法使用。成员变量初始值默认为0。</p>
<p>局部变量：在类的方法中定义，用来在方法中临时保存数据。java没有给局部变量赋予初始值。</p>
<p>在不同的方法中，可以有同名局部变量。局部变量的优先级高于它的同名成员变量。</p>
<h3 id="static-之静态变量"><a href="#static-之静态变量" class="headerlink" title="static 之静态变量"></a>static 之静态变量</h3><p>我们可以基于一个类创建多个该类的对象，每个对象都拥有自己的成员，互相独立。然而在某些时候，我们更希望该类所有的对象共享同一个成员，此时 static 就可以发挥作用。</p>
<p>Java 中被 static 修饰的成员称为静态成员或类成员。它属于整个类所有，而不是某个对象所有，即被类的所有对象所共享。静态成员可以使用类名直接访问，也可以使用对象名进行访问。当然，鉴于他作用的特殊性更推荐用类名访问。</p>
<p>使用 static 可以修饰变量、方法和代码块。</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C54.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C55.jpg" alt></p>
<p>如果只是普通的成员变量，就不能通过类名来访问，必须先创建类的对象。</p>
<h3 id="static之静态方法"><a href="#static之静态方法" class="headerlink" title="static之静态方法"></a>static之静态方法</h3><p>与静态变量一样，我们也可以使用 static 修饰方法，称为静态方法或类方法。其实之前我们一直写的 main 方法就是静态方法。静态方法的使用如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C56.jpg" alt></p>
<p>需要注意：</p>
<p>1、 <strong>静态方法中可以直接调用同类中的静态成员，但不能直接调用非静态成员。</strong>如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C57.jpg" alt></p>
<p><strong>如果希望在静态方法中调用非静态变量，可以通过创建类的对象，然后通过对象来访问非静态变量</strong>。如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C58.jpg" alt></p>
<p>2、 <strong>在普通成员方法中，则可以直接访问同类的非静态变量和静态变量</strong>，如下所示：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C59.jpg" alt></p>
<p>3、 <strong>静态方法中不能直接调用非静态方法，需要通过对象来访问非静态方法。</strong>如：</p>
<p>总结一下，普通方法中可以直接访问同类的所有成员变量或方法。而静态方法只能直接访问静态变量和直接调用静态方法，不能直接调用非静态成员或方法。</p>
<h3 id="static之静态初始化块"><a href="#static之静态初始化块" class="headerlink" title="static之静态初始化块"></a>static之静态初始化块</h3><p>Java 中可以通过初始化块进行数据赋值。如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C60.jpg" alt></p>
<p>在类的声明中，可以包含多个初始化块，当创建类的实例时，就会依次执行这些代码块。如果使用 static 修饰初始化块，就称为静态初始化块。</p>
<p>需要特别注意：<strong>静态初始化块只在类加载时执行，且只会执行一次，同时静态初始化块只能给静态变量赋值，不能初始化普通的成员变量</strong>。</p>
<p>我们来看一段代码：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C61.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C62.jpg" alt></p>
<p>通过输出结果，我们可以看到，程序运行时静态初始化块最先被执行，然后执行普通初始化块，最后才执行构造方法。由于静态初始化块只在类加载时执行一次，所以当再次创建对象 hello2 时并未执行静态初始化块。</p>
<h2 id="第五章–封装"><a href="#第五章–封装" class="headerlink" title="第五章–封装"></a>第五章–封装</h2><p>面向对象的三大特性：封装，继承，多态。</p>
<p>将类的某些信息隐藏在类内部，不允许外部程序直接访问，而是通过该类提供的方法来实现对隐藏信息的操作和访问。</p>
<p>优点：</p>
<p>1.只能通过规定的方法访问数据。例如在其他类不能直接调用该类的private变量，可以在该类中定义一个方法获取这个变量，然后在其他类中通过调用这个方法间接获取那个私人变量。</p>
<p>2.隐藏类的实例细节，方便修改和实现。</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C63.png" alt></p>
<p>eg：在Telphone类里创建get和set方法</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C65.png" alt></p>
<h3 id="使用包管理java中的类"><a href="#使用包管理java中的类" class="headerlink" title="使用包管理java中的类"></a>使用包管理java中的类</h3><p>包的作用：管理Java文件；解决同名文件冲突。</p>
<p>定义包：package 包名</p>
<p>注：必须放在Java源程序的第一行。命名包可以用”.”隔开表示层次。</p>
<p>eg：apartment1.floor2.room3</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> apartment1.floor2.room3;</span><br></pre></td></tr></table></figure>

<p>包的使用：</p>
<p>在某个文件使用刚才创建的包的类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> apartment1.floor2.room3;</span><br></pre></td></tr></table></figure>

<h3 id="Java中的内部类"><a href="#Java中的内部类" class="headerlink" title="Java中的内部类"></a>Java中的内部类</h3><p>内部类（ Inner Class ）就是定义在另外一个类里面的类。与之对应，包含内部类的类被称为外部类。内部类也是一个类，可以有自己的成员方法和成员变量。</p>
<p>作用：</p>
<ol>
<li><p>内部类提供了更好的封装，可以把内部类隐藏在外部类之内，不允许同一个包中的其他类访问该类</p>
</li>
<li><p>内部类的方法可以直接访问外部类的所有数据，包括私有的数据</p>
</li>
<li><p>内部类所实现的功能使用外部类同样可以实现，只是有时使用内部类更方便</p>
</li>
</ol>
<p>内部类可分为以下几种：</p>
<ul>
<li>成员内部类</li>
<li>静态内部类</li>
<li>方法内部类</li>
<li>匿名内部类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//外部类HelloWorld</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line"><span class="comment">// 内部类Inner，类Inner在类HelloWorld的内部</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Inner</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 内部类的方法</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">show</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		System.out.println(<span class="string">"welcome to imooc!"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建外部类对象</span></span><br><span class="line">	HelloWorld hello = <span class="keyword">new</span> HelloWorld();</span><br><span class="line">    <span class="comment">// 创建内部类对象</span></span><br><span class="line">	Inner i = hello.<span class="keyword">new</span> Inner();</span><br><span class="line">    <span class="comment">// 调用内部类对象的方法</span></span><br><span class="line">	i.show();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="成员内部类"><a href="#成员内部类" class="headerlink" title="成员内部类"></a>成员内部类</h4><p>内部类中最常见的就是成员内部类，也称为普通内部类。我们来看如下代码：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C63.jpg" alt></p>
<p>运行结果为：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C64.jpg" alt></p>
<p>从上面的代码中我们可以看到，<strong>成员内部类的使用方法</strong>：</p>
<p>1、 Inner 类定义在 Outer 类的内部，相当于 Outer 类的一个成员变量的位置，Inner 类可以使用任意访问控制符，如 public 、 protected 、 private等</p>
<p>2、 Inner 类中定义的 test() 方法可以直接访问 Outer 类中的数据，而不受访问控制符的影响，如直接访问 Outer 类中的私有属性a</p>
<p>3、 定义了成员内部类后，必须使用外部类对象来创建内部类对象，而不能直接去 new 一个内部类对象，即：内部类 对象名 = 外部类对象.new 内部类( )</p>
<p>4、 编译上面的程序后，会发现产生了两个 .class 文件</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C65.jpg" alt></p>
<p>其中，第二个是外部类的 .class 文件，第一个是内部类的 .class 文件，即成员内部类的 .class 文件总是这样：外部类名$内部类名.class</p>
<p>注意：</p>
<p>1、 外部类是不能直接使用内部类的成员和方法</p>
<p>可先创建内部类的对象，然后通过内部类的对象来访问其成员变量和方法。</p>
<p>2、 如果外部类和内部类具有相同的成员变量或方法，内部类默认访问自己的成员变量或方法，如果要访问外部类的成员变量，可以使用 this 关键字。如：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C66.jpg" alt></p>
<h4 id="静态内部类"><a href="#静态内部类" class="headerlink" title="静态内部类"></a>静态内部类</h4><p>静态内部类是 static 修饰的内部类，这种内部类的特点是：</p>
<p>1、 静态内部类不能直接访问外部类的非静态成员，但可以通过 <strong>new 外部类().成员</strong> 的方式访问 </p>
<p>2、 如果外部类的静态成员与内部类的成员名称相同，可通过“类名.静态成员”访问外部类的静态成员；如果外部类的静态成员与内部类的成员名称不相同，则可通过“成员名”直接调用外部类的静态成员</p>
<p>3、 创建静态内部类的对象时，不需要外部类的对象，可以直接创建 <strong>内部类 对象名= new 内部类();</strong></p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C67.jpg" alt></p>
<p>运行结果</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C68.jpg" alt></p>
<h4 id="方法内部类"><a href="#方法内部类" class="headerlink" title="方法内部类"></a>方法内部类</h4><p>方法内部类就是内部类定义在外部类的方法中，方法内部类只在该方法的内部可见，即只在该方法内可以使用。</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C69.jpg" alt></p>
<p>方法内部类的对象应该要定义在方法中。</p>
<p><strong>注意：</strong>由于方法内部类不能在外部类的方法以外的地方使用，因此方法内部类不能使用访问控制符和 static 修饰符。</p>
<h2 id="第六章–继承"><a href="#第六章–继承" class="headerlink" title="第六章–继承"></a>第六章–继承</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>继承是类和类之间的一种关系。</p>
<p>Java中的继承是单继承，即一个类只有一个父类。</p>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><p>子类拥有父类的所有属性和方法，（修饰符不能是private）实现代码复用。</p>
<h4 id="语法规则"><a href="#语法规则" class="headerlink" title="语法规则"></a>语法规则</h4><p><strong>class</strong> 子类 <strong>extends</strong> 父类{</p>
<p>}</p>
<h4 id="继承的例子"><a href="#继承的例子" class="headerlink" title="继承的例子"></a>继承的例子</h4><p>创建一个动物–父类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mmm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> age;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eat</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"动物具有吃东西的能力"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>创建一个狗–子类，创建时继承动物类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mmm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        Dog dog1=<span class="keyword">new</span> Dog();</span><br><span class="line">        dog1.age=<span class="number">10</span>;</span><br><span class="line">        dog1.name=<span class="string">"duoduo"</span>;</span><br><span class="line">        dog1.eat();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="继承的初始化顺序"><a href="#继承的初始化顺序" class="headerlink" title="继承的初始化顺序"></a>继承的初始化顺序</h4><p>1 初始化父类之后再初始化子类（创建对象时，先创建父类对象再创建子类对象）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mmm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> age;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eat</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"动物具有吃东西的能力"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Animal</span><span class="params">()</span></span>&#123; <span class="comment">//父类构造方法</span></span><br><span class="line">        System.out.println(<span class="string">"Animal类执行了"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mmm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        Dog dog1=<span class="keyword">new</span> Dog();</span><br><span class="line">        dog1.age=<span class="number">10</span>;</span><br><span class="line">        dog1.name=<span class="string">"duoduo"</span>;</span><br><span class="line">        dog1.eat();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Dog</span><span class="params">()</span></span>&#123; <span class="comment">//子类构造方法</span></span><br><span class="line">        System.out.println(<span class="string">"Dog类执行了"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Animal类执行了</span><br><span class="line">Dog类执行了</span><br><span class="line">动物具有吃东西的能力</span><br></pre></td></tr></table></figure>

<p>2 先执行初始化对象中的属性，再执行构造方法中属性的初始化。</p>
<h3 id="方法重写"><a href="#方法重写" class="headerlink" title="方法重写"></a>方法重写</h3><p>子类在继承父类的方法时可以对父类的方法进行重写，当调用方法时会优先调用子类的方法。</p>
<p>重写时返回值类型，方法名，参数类型及个数都要与父类继承的方法相同才能重写成功。</p>
<p>修改刚刚Dog类的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mmm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eat</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"狗具有吃东西的能力"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="final关键字"><a href="#final关键字" class="headerlink" title="final关键字"></a>final关键字</h3><p>final可以修饰类，方法，属性和变量。</p>
<p>final修饰类时，该类就不能被继承。</p>
<p>修饰方法时，方法不能被重写，但可以被调用。</p>
<p>修饰属性时，如果只是声明了该属性而没有赋值，就必须在构造方法中赋值。如果修饰了该属性并同时赋了值，则不能再在构造方法或子类中进行修改。</p>
<p>修饰变量时，变量变常量。</p>
<h3 id="super关键字"><a href="#super关键字" class="headerlink" title="super关键字"></a>super关键字</h3><p>在对象的内部使用，可以代表父类对象。</p>
<p>如访问父类的属性：super.age</p>
<p>访问父类的方法：super.eat()</p>
<p>例子</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mmm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> age=<span class="number">5</span>;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eat</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"动物具有吃东西的能力"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mmm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eat</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"狗具有吃东西的能力"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        Dog dog1=<span class="keyword">new</span> Dog();</span><br><span class="line">        <span class="comment">//dog1.age=10; 如果加此行代码，执行结果的5会变为10</span></span><br><span class="line">        dog1.name=<span class="string">"duoduo"</span>;</span><br><span class="line">        <span class="comment">//dog1.eat();</span></span><br><span class="line">        dog1.method();</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//在子类中写一个方法调用父类的属性和方法。</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">method</span><span class="params">()</span></span>&#123;</span><br><span class="line">        eat();<span class="comment">//调用自己的方法</span></span><br><span class="line">        <span class="keyword">super</span>.eat();<span class="comment">//调用父类的方法</span></span><br><span class="line">        System.out.println(<span class="keyword">super</span>.age);<span class="comment">//调用父类的属性</span></span><br><span class="line">        System.out.println(age);<span class="comment">//调用子类继承的属性</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">狗具有吃东西的能力</span><br><span class="line">动物具有吃东西的能力</span><br><span class="line">5</span><br><span class="line">5</span><br></pre></td></tr></table></figure>

<p>super的应用</p>
<p>如果子类的构造方法中没有显示调用父类的构造方法，则系统默认调用父类无参的构造方法。如果显示调用构造方法，则必须在子类构造方法的第一行。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Dog</span><span class="params">()</span></span>&#123; <span class="comment">//子类构造方法</span></span><br><span class="line">    <span class="keyword">super</span>();<span class="comment">//这一行没有后台依然会隐性添加。</span></span><br><span class="line">    System.out.println(<span class="string">"Dog类执行了"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果父类没有无参的构造方法，则子类的构造方法应在第一行调用父类的有参构造方法。</p>
<p>例如现在我们在父类中写一个有参的构造方法 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Animal</span><span class="params">(<span class="keyword">int</span> age)</span></span>&#123; <span class="comment">//父类有参构造方法</span></span><br><span class="line">    <span class="keyword">this</span>.age=age;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>则在写子类的构造方法时</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Dog</span><span class="params">()</span></span>&#123; </span><br><span class="line">    <span class="keyword">super</span>(<span class="number">8</span>);<span class="comment">//必须先调用父类的有参构造方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="object类"><a href="#object类" class="headerlink" title="object类"></a>object类</h3><p>object类是所有类的父类。如果一个类没有使用extends关键字标识它继承哪个父类，拿它默认继承object类。</p>
<h4 id="object类的方法"><a href="#object类的方法" class="headerlink" title="object类的方法"></a>object类的方法</h4><h5 id="1-toString-方法"><a href="#1-toString-方法" class="headerlink" title="1.toString()方法"></a>1.toString()方法</h5><p>默认继承 Object 的 toString( ) 方法，输出对象地址</p>
<p>返回对象的哈希code码（对象地址字符串）</p>
<h5 id="2-equals-方法"><a href="#2-equals-方法" class="headerlink" title="2 equals()方法"></a>2 equals()方法</h5><p>比较对象的运用是否指向同一块内存地址。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dog dog&#x3D;new Dog();</span><br></pre></td></tr></table></figure>

<p>在这里dog其实不是一个对象，而是指指向一个对象。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mmm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Dog dog1 = <span class="keyword">new</span> Dog();</span><br><span class="line">        Dog dog2 = <span class="keyword">new</span> Dog();</span><br><span class="line">        <span class="keyword">if</span> (dog1.equals(dog2)) &#123;</span><br><span class="line">            System.out.println(<span class="string">"两个对象是相同的"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">"两个对象是不相同的"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">两个对象是不相同的</span><br></pre></td></tr></table></figure>

<p>如果我们要用equals()比较两个对象的值是否相等，那就要在Dog类里重写equals()方法。</p>
<h2 id="第七章–多态"><a href="#第七章–多态" class="headerlink" title="第七章–多态"></a>第七章–多态</h2><p>指对象的多种状态，继承是多态的实现基础。</p>
<h3 id="1-引用多态"><a href="#1-引用多态" class="headerlink" title="1.引用多态"></a>1.引用多态</h3><p>父类的引用可以指向本类的对象。</p>
<p>父类的引用可以指向子类的对象。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mmm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> age=<span class="number">5</span>;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eat</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"动物具有吃东西的能力"</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mmm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Animal obj1=<span class="keyword">new</span> Animal();</span><br><span class="line">        Animal obj2=<span class="keyword">new</span> Dog();<span class="comment">//父类的引用可以指向子类的对象</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="2-方法多态"><a href="#2-方法多态" class="headerlink" title="2.方法多态"></a>2.方法多态</h3><p>创建本类对象时，调用的方法为本类方法。</p>
<p>创建子类对象时，调用的方法为子类重写的方法或者继承的方法。</p>
<p>例如我们现在在Dog类中重写eat方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mmm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eat</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"狗吃骨头"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Animal obj1=<span class="keyword">new</span> Animal();</span><br><span class="line">        Animal obj2=<span class="keyword">new</span> Dog();<span class="comment">//父类的引用可以指向子类的对象</span></span><br><span class="line">        obj1.eat();</span><br><span class="line">        obj2.eat();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">动物具有吃东西的能力</span><br><span class="line">狗吃骨头</span><br></pre></td></tr></table></figure>

<h3 id="多态中的引用类型转换"><a href="#多态中的引用类型转换" class="headerlink" title="多态中的引用类型转换"></a>多态中的引用类型转换</h3><p>1.向上类型转换（隐式/自动类型转换），是小类型到大类型的转换。（无风险）</p>
<p>2.向下类型转换（强制类型转换），是大类型到小类型。（有风险，数据可能会溢出）</p>
<p>instanceof运算符，来解决引用对象的类型，避免类型转换的安全性问题。</p>
<p>例子（Animal父类，Dog子类和Cat子类）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eat</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"狗吃骨头"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Dog dog=<span class="keyword">new</span> Dog();</span><br><span class="line">        Animal animal=dog;<span class="comment">//向上类型转换，自动类型提升。</span></span><br><span class="line">        Dog dog2=(Dog)animal;<span class="comment">//向下类型转换，强制类型转换。</span></span><br><span class="line">        <span class="comment">//Cat cat=(Cat)animal; 不能再把属于Dog类的animal强制转换为属于Cat类的animal。</span></span><br><span class="line">        <span class="keyword">if</span>(animal <span class="keyword">instanceof</span> Cat)&#123;</span><br><span class="line">            Cat cat=(Cat)animal;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"无法进行类型转换"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">无法进行类型转换</span><br></pre></td></tr></table></figure>

<h3 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h3><p>应用场景：</p>
<p>1.在某些情况下，某个父类只是知道其子类应该包含怎样的方法，但无法准确知道这些子类如何实现这些方法。抽象类用来约束子类有哪些方法，但不关注具体实现。</p>
<p>2.从多个具有相同特征的类中抽象出一个抽象类，以这个抽象类作为子类的模板，从而避免了子类设计的随意性。</p>
<p>使用规则</p>
<p>用abstract修饰符定义抽象类。抽象类依然可以有构造函数，属性，非抽象类方法，静态属性和静态方法。</p>
<p>用abstract修饰符定义抽象方法，只有声明，不需要实现。</p>
<p>包含抽象方法的类是抽象类。</p>
<p>抽象类中可以包含普通的方法，也可以没有抽象方法。</p>
<p>抽象方法没有方法体以;结束。</p>
<p>If a subclass does not implement all abstract methods, it must also be marked abstract.</p>
<p>You cannot create instances of abstract classes – only concrete subclasses</p>
<p>例子</p>
<p>创建一个抽象Telphone父类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Telphone</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">message</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">bigscreen</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在创建CellPhone子类，子类自动实现父类里的方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CellPhone</span> <span class="keyword">extends</span> <span class="title">Telphone</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">message</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>现在我们可以在子类的两个方法体里添加代码了。比如：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CellPhone</span> <span class="keyword">extends</span> <span class="title">Telphone</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"用键盘拨打电话"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">message</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"用键盘发短信"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p>接口可以理解为一种特殊的类，由全局常量和公共的抽象方法所组成。You cannot directly create an instance of an interface.</p>
<p>Similar to a class, but : Declared with interface keyword ,All (non-static) methods are implicitly public abstract ,All fields are implicitly public static final</p>
<p>类是一种具体实现体，而接口定义了某一批类所需要遵守的规范，接口不关心这些类的内部数据，也不关心这些类里方法的实现细节，它只规定这些类里必须提供某些方法。</p>
<h4 id="定义接口"><a href="#定义接口" class="headerlink" title="定义接口"></a>定义接口</h4><p>基本语法：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C70.png" alt></p>
<p>public static final 常量名</p>
<p>public abstract 方法名</p>
<h4 id="使用接口"><a href="#使用接口" class="headerlink" title="使用接口"></a>使用接口</h4><p>一个类可以有一个或多个接口，实现接口使用implements关键字。弥补java中类只能继承一个父类的不足。</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C71.png" alt></p>
<p>例子</p>
<p>通过接口描述两个不同对象的共同特性。</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C72.png" alt></p>
<p>以抽象类中的例子为例，创建一个接口，接口名一般以I开头。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">IPlayGame</span> </span>&#123; <span class="comment">//接口可以不显式显现abstract关键字</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">playgame</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>让CellPhone类接入这个接口，CellPhone类就必须添加playgame那个方法。之后可以在这个方法中添加代码。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CellPhone</span> <span class="keyword">extends</span> <span class="title">Telphone</span> <span class="keyword">implements</span> <span class="title">IPlayGame</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"用键盘拨打电话"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">message</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"用键盘发短信"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">playgame</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"用键盘玩游戏"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>调用时</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">    CellPhone phone1=<span class="keyword">new</span> CellPhone();</span><br><span class="line">    phone1.playgame();</span><br><span class="line">    IPlayGame phone2=<span class="keyword">new</span> CellPhone();</span><br><span class="line">    phone2.playgame();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">用键盘玩游戏</span><br><span class="line">用键盘玩游戏</span><br></pre></td></tr></table></figure>



<p>接口有时还会与匿名内部类（没有名字的类，用于关注实现而不关注实现类的名称）配合使用。</p>
<p>利用匿名内部类的方式实现接口：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C73.png" alt></p>
<p>执行结果为：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C74.png" alt></p>
<h4 id="接口与抽象类对比"><a href="#接口与抽象类对比" class="headerlink" title="接口与抽象类对比"></a>接口与抽象类对比</h4><p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C115.png" alt></p>
<h2 id="第八章–异常"><a href="#第八章–异常" class="headerlink" title="第八章–异常"></a>第八章–异常</h2><p>异常包括：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/75.png" alt></p>
<p>检查异常包括文件异常和SQL异常。</p>
<p>处理异常</p>
<p>try-catch以及try-catch-finally</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line"><span class="comment">//一些抛出异常的方法</span></span><br><span class="line">&#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line"><span class="comment">//捕获某异常并处理</span></span><br><span class="line">&#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">    <span class="comment">//最终要执行的一些代码，进行善后工作。</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>例子1</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/76.png" alt></p>
<p>在执行完catch语句后最后一句代码依旧会被执行。</p>
<p>例子2</p>
<p>如果有try中出现多种异常，可以用catch语句分别取捕获：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/77.png" alt></p>
<p>catch块里的异常类型应该从子类编写到父类。</p>
<h4 id="Java中的异常抛出"><a href="#Java中的异常抛出" class="headerlink" title="Java中的异常抛出"></a>Java中的异常抛出</h4><p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/78.png" alt></p>
<p>捕获到的异常，可以在当前方法的 catch 块中处理，也可抛出给调用者去处理</p>
<h4 id="自定义异常"><a href="#自定义异常" class="headerlink" title="自定义异常"></a>自定义异常</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class 自定义异常类 extends 异常类型&#123;</span><br><span class="line">&#125;&#x2F;&#x2F;继承与意思相近的异常或直接继承Exception类</span><br></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/79.png" alt></p>
<h2 id="第九章–字符串"><a href="#第九章–字符串" class="headerlink" title="第九章–字符串"></a>第九章–字符串</h2><h3 id="创建-String-对象"><a href="#创建-String-对象" class="headerlink" title="创建 String 对象"></a>创建 String 对象</h3><p>在 Java 中，字符串被作为 String 类型的对象处理。 String 类位于 java.lang 包中。默认情况下，该包被自动导入所有的程序。</p>
<p>创建 String 对象的方法：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/80.jpg" alt></p>
<p>String 对象创建后则不能被修改，是不可变的，所谓的修改其实是创建了新的对象，所指向的内存空间不同。</p>
<p>每次 new 一个字符串就是产生一个新的对象，即便两个字符串的内容相同，使用 ”==” 比较时也为 ”false” ,如果只需比较内容是否相同，应使用 ”equals()” 方法。而如果直接创建的话，多次出现时会被编译器优化，只创建一个对象。</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/81.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/82.jpg" alt></p>
<h3 id="String-类的常用方法-Ⅰ"><a href="#String-类的常用方法-Ⅰ" class="headerlink" title="String 类的常用方法 Ⅰ"></a>String 类的常用方法 Ⅰ</h3><p>String 类的常用方法：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/83.jpg" alt></p>
<p>结合代码来熟悉一下方法的使用：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/84.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/85.jpg" alt></p>
<p>注意：</p>
<ol>
<li><p>字符串 str 中字符的索引从0开始，范围为 0 到 str.length()-1</p>
</li>
<li><p>使用 indexOf 进行字符或字符串查找时，如果匹配返回位置索引；如果没有匹配结果，返回 -1</p>
</li>
<li><p>使用 substring(beginIndex , endIndex) 进行字符串截取时，包括 beginIndex 位置的字符，不包括 endIndex 位置的字符</p>
</li>
</ol>
<h3 id="String-类常用方法-Ⅱ"><a href="#String-类常用方法-Ⅱ" class="headerlink" title="String 类常用方法 Ⅱ"></a>String 类常用方法 Ⅱ</h3><p>继续来看 String 类常用的方法，如下代码所示：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/86.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/87.jpg" alt></p>
<p><strong>“==” 和 equals() 的区别？</strong></p>
<p>==: 判断两个字符串在内存中首地址是否相同，即判断是否是同一个字符串对象</p>
<p>equals(): 比较存储在两个字符串对象中的内容是否一致</p>
<p>PS：字节是计算机存储信息的基本单位，<strong>1 个字节等于 8 位</strong>， <strong>gbk 编码中 1 个汉字字符存储需要 2 个字节</strong>，<strong>1 个英文字符存储需要 1 个字节</strong>。所以我们看到上面的程序运行结果中，每个汉字对应两个字节值，如“学”对应 “-47 -89” ，而英文字母 “J” 对应 “74” 。同时，我们还发现汉字对应的字节值为负数，原因在于每个字节是 8 位，最大值不能超过 127，而<strong>汉字转换为字节后超过 127，如果超过就会溢出，以负数的形式显示。</strong></p>
<h3 id="StringBuilder-类"><a href="#StringBuilder-类" class="headerlink" title="StringBuilder 类"></a>StringBuilder 类</h3><p>在Java中，除了可以使用 String 类来存储字符串，还可以使用 StringBuilder 类或 StringBuffer 类存储字符串</p>
<p>String 类具有是不可变性。如</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/88.jpg" alt></p>
<p>运行结果：  </p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/89.jpg" alt></p>
<p>从运行结果中我们可以看到，程序运行时会额外创建一个对象，保存 “helloworld”。当频繁操作字符串时，就会额外产生很多临时变量。使用 StringBuilder 或 StringBuffer 就可以避免这个问题。至于 StringBuilder 和StringBuffer ，它们基本相似，不同之处，StringBuffer 是线程安全的，而 StringBuilder 则没有实现线程安全功能，所以性能略高。因此一般情况下，如果需要创建一个内容可变的字符串对象，应优先考虑使用 StringBuilder 类。</p>
<p>如何定义 StringBuilder 类的对象：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/90.jpg" alt></p>
<h3 id="StringBuilder-类的常用方法"><a href="#StringBuilder-类的常用方法" class="headerlink" title="StringBuilder 类的常用方法"></a>StringBuilder 类的常用方法</h3><p>StringBuilder 类提供了很多方法来操作字符串：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/91.jpg" alt></p>
<p>例如：在下面的示例代码中，创建了 StringBuilder 对象，用来存储字符串，并对其做了追加和插入操作。这些操作修改了 str 对象的值，而没有创建新的对象，这就是 StringBuilder 和 String 最大的区别。</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/92.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/93.jpg" alt></p>
<h2 id="第十章–包装类"><a href="#第十章–包装类" class="headerlink" title="第十章–包装类"></a>第十章–包装类</h2><p>基本数据类型如 int、float、double、boolean、char 等。基本数据类型是不具备对象的特性的，比如基本类型不能调用方法、功能简单。为了让基本数据类型也具备对象的特性， Java 为每个基本数据类型都提供了一个包装类，这样我们就可以像操作对象那样来操作基本数据类型。 </p>
<p>基本类型和包装类之间的对应关系：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/94.jpg" alt></p>
<p>包装类主要提供了两大类方法：</p>
<ol>
<li><p>将本类型和其他基本类型进行转换的方法</p>
</li>
<li><p>将字符串和本类型及包装类互相转换的方法</p>
</li>
</ol>
<h3 id="Integer-包装类"><a href="#Integer-包装类" class="headerlink" title="Integer 包装类"></a>Integer 包装类</h3><p>我们以 Integer 包装类为例，来看下包装类的特性。</p>
<p>Integer 包装类的构造方法：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/95.jpg" alt></p>
<p>如下代码所示：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/96.jpg" alt></p>
<p>Integer包装类的常用方法：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/97.jpg" alt></p>
<p>public class HelloWorld {<br>    public static void main(String[] args) {</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义int类型变量，值为86</span></span><br><span class="line"><span class="keyword">int</span> score1 = <span class="number">86</span>; </span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建Integer包装类对象，表示变量score1的值</span></span><br><span class="line">Integer score2=<span class="keyword">new</span> Integer(score1);</span><br><span class="line">   </span><br><span class="line"><span class="comment">// 将Integer包装类转换为double类型</span></span><br><span class="line"><span class="keyword">double</span> score3=score2.doubleValue();</span><br><span class="line">   </span><br><span class="line"><span class="comment">// 将Integer包装类转换为float类型</span></span><br><span class="line"><span class="keyword">float</span> score4=score2.floatValue();</span><br><span class="line">   </span><br><span class="line"><span class="comment">// 将Integer包装类转换为int类型</span></span><br><span class="line"><span class="keyword">int</span> score5 =score2.intValue();</span><br></pre></td></tr></table></figure>

<h3 id="基本类型和包装类之间的转换"><a href="#基本类型和包装类之间的转换" class="headerlink" title="基本类型和包装类之间的转换"></a>基本类型和包装类之间的转换</h3><p>基本类型和包装类之间经常需要互相转换，以 Integer 为例</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C98.jpg" alt></p>
<p>在 JDK1.5 引入自动装箱和拆箱的机制后，包装类和基本类型之间的转换就更加轻松便利了。</p>
<p><strong>装箱：</strong>把基本类型转换成包装类，使其具有对象的性质，又可分为手动装箱和自动装箱</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C99.jpg" alt></p>
<p><strong>拆箱：</strong>和装箱相反，把包装类对象转换成基本类型的值，又可分为手动拆箱和自动拆箱</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C100.jpg" alt></p>
<h3 id="基本类型和字符串之间的转换"><a href="#基本类型和字符串之间的转换" class="headerlink" title="基本类型和字符串之间的转换"></a>基本类型和字符串之间的转换</h3><p>在程序开发中，我们经常需要在基本数据类型和字符串之间进行转换。</p>
<p>其中，基本类型转换为字符串有三种方法：</p>
<ol>
<li><p>使用包装类的 toString() 方法</p>
</li>
<li><p>使用String类的 valueOf() 方法</p>
</li>
<li><p>用一个空字符串加上基本类型，得到的就是基本类型数据对应的字符串</p>
</li>
</ol>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C101.jpg" alt></p>
<p>再来看，将字符串转换成基本类型有两种方法：</p>
<ol>
<li><p>调用包装类的 parseXxx 静态方法</p>
</li>
<li><p>调用包装类的 valueOf() 方法转换为基本类型的包装类，会自动拆箱</p>
</li>
</ol>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C102.jpg" alt></p>
<h3 id="Date-和-SimpleDateFormat-类表示时间"><a href="#Date-和-SimpleDateFormat-类表示时间" class="headerlink" title="Date 和 SimpleDateFormat 类表示时间"></a>Date 和 SimpleDateFormat 类表示时间</h3><p>使用的包</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.text.ParseException;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br></pre></td></tr></table></figure>

<p>在程序开发中，经常需要处理日期和时间的相关数据，此时我们可以使用 java.util 包中的 Date 类。这个类最主要的作用就是获取当前时间，我们来看下 Date 类的使用：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C103.jpg" alt></p>
<p>使用 Date 类的<strong>默认无参构造方法</strong>创建出的对象就代表<strong>当前时间</strong>，我们可以直接输出 Date 对象显示当前的时间。显示的结果如下：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C104.jpg" alt></p>
<p>其中， Wed 代表 Wednesday (星期三)， Jun 代表 June (六月)， 11 代表 11 号， CST 代表 China Standard Time (中国标准时间，也就是北京时间，东八区)。</p>
<p>如果想要按指定的格式进行显示，如 2014-06-11 09:22:30 ，可以使用 SimpleDateFormat 来对日期时间进行格式化，如可以将日期转换为指定格式的文本，也可将文本转换为日期。</p>
<ol>
<li>使用 format() 方法将日期转换为指定格式的文本</li>
</ol>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C105.jpg" alt></p>
<p>运行结果：<strong>2014-06-11  09:55:48</strong>  </p>
<ol start="2">
<li>使用 parse() 方法将文本转换为日期</li>
</ol>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C106.jpg" alt></p>
<p>运行结果</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C107.jpg" alt></p>
<h3 id="Calendar-类的应用"><a href="#Calendar-类的应用" class="headerlink" title="Calendar 类的应用"></a>Calendar 类的应用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Calendar;</span><br></pre></td></tr></table></figure>

<p>Date 类最主要的作用就是获得当前时间，同时这个类里面也具有设置时间以及一些其他的功能，但是由于本身设计的问题，这些方法却遭到众多批评，不建议使用，更推荐使用 Calendar 类进行时间和日期的处理。</p>
<p>java.util.Calendar 类是一个抽象类，可以通过调用 getInstance() 静态方法获取一个 Calendar 对象，此对象已由当前日期时间初始化，即默认代表当前时间，如 Calendar c = Calendar.getInstance();</p>
<p>那么如何使用 Calendar 获取年、月、日、时间等信息呢？我们来看下面的代码：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C108.jpg" alt></p>
<p>其中，调用 Calendar 类的 getInstance() 方法获取一个实例，然后通过调用 get() 方法获取日期时间信息，参数为需要获得的字段的值， Calendar.Year 等为 Calendar 类中定义的静态常量。</p>
<p>运行结果： </p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C109.jpg" alt></p>
<p>Calendar 类提供了 getTime() 方法，用来获取 Date 对象，完成 Calendar 和 Date 的转换，还可通过 getTimeInMillis() 方法，获取此 Calendar 的时间值，以毫秒为单位。如下所示：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C110.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C111.jpg" alt></p>
<h3 id="用-Math-类操作数据"><a href="#用-Math-类操作数据" class="headerlink" title="用 Math 类操作数据"></a>用 Math 类操作数据</h3><p>Math 类位于 java.lang 包中，包含用于执行基本数学运算的方法， Math 类的所有方法都是静态方法，所以使用该类中的方法时，可以直接使用类名.方法名，如： Math.round();</p>
<p>常用的方法：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C112.jpg" alt></p>
<p>代码</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C113.jpg" alt></p>
<p>运行结果：</p>
<p><img src="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C114.jpg" alt></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-id="ckn1t0idj0002rgvtgugkboje" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Java/" rel="tag">Java</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Scrapy-Re-动态爬虫实例" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/" class="article-date">
  <time datetime="2020-07-06T15:58:47.000Z" itemprop="datePublished">2020-07-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/">Scrapy&amp;Re--动态爬虫实例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>关于动态爬虫：</p>
<h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><img src="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/1.png" style="zoom: 67%;">

<h2 id="实例一：爬取影评网站中的视频链接，视频名，以及图片"><a href="#实例一：爬取影评网站中的视频链接，视频名，以及图片" class="headerlink" title="实例一：爬取影评网站中的视频链接，视频名，以及图片"></a>实例一：爬取影评网站中的视频链接，视频名，以及图片</h2><p><a href="https://www.1905.com/mtalk/" target="_blank" rel="noopener">https://www.1905.com/mtalk/</a></p>
<h3 id="方法一：用request"><a href="#方法一：用request" class="headerlink" title="方法一：用request"></a>方法一：用request</h3><p>打开网站：</p>
<img src="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/2.png" style="zoom: 33%;">



<p>由于这是一个动态网页，我们不能直接爬它的网址。打开XHR（F12），手动获取真正的URL。</p>
<img src="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/3.png" style="zoom:33%;">

<p>在之后写代码请求访问时，要把query string parameters 以键值对形式添加到我们的params中</p>
<img src="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/4.png" style="zoom: 33%;">



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">()</span>:</span></span><br><span class="line">    url=<span class="string">'http://www.1905.com/api/content/index.php'</span> </span><br><span class="line">    header=&#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'</span>&#125; <span class="comment">#这里头部不伪装成浏览器也可以访问</span></span><br><span class="line">    param=&#123;</span><br><span class="line"><span class="string">'callback'</span>: <span class="string">'reloadList'</span>,</span><br><span class="line"><span class="string">'m'</span>: <span class="string">'converged'</span>,</span><br><span class="line"><span class="string">'a'</span>: <span class="string">'info'</span>,</span><br><span class="line"><span class="string">'type'</span>: <span class="string">'jryp'</span>,</span><br><span class="line"><span class="string">'year'</span>: <span class="string">'2020'</span>,</span><br><span class="line"><span class="string">'month'</span>: <span class="string">'5'</span></span><br><span class="line">    &#125; <span class="comment">#从网站看到还有其他年份和月份的数据，我们可以直接修改param中的year和month。</span></span><br><span class="line">    r=requests.get(url,headers=header,params=param) </span><br><span class="line">    result=r.text</span><br><span class="line">find()</span><br></pre></td></tr></table></figure>

<p>我们发现多了一些数据 reloadList({“month”:{“6”:6,”5”:5,”4”:4,”3”:3,”2”:2,”1”:1}，导致返回的代码不是json字符串形式。这样会导致后续json.loads()将json字符串格式转为dict格式时出错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reloadList(&#123;&quot;month&quot;:&#123;&quot;6&quot;:6,&quot;5&quot;:5,&quot;4&quot;:4,&quot;3&quot;:3,&quot;2&quot;:2,&quot;1&quot;:1&#125;,&quot;info&quot;:[&#123;&quot;contentid&quot;:&quot;1458732&quot;,&quot;title&quot;:&quot;\u4e00\u5468\u70ed\u70b9\uff1a\u89e3\u8bfb\u201c\u4e24\u4f1a\u201d\u4e2d\u7684\u7535\u5f71\u4e4b\u58f0&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image11.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0529\&#x2F;thumb_1_224_122_20200529043833417252.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1458732.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-29\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1458506&quot;,&quot;title&quot;:&quot;\u4e24\u4f1a\u7279\u522b\u7b56\u5212\u8282\u76ee\uff1a\u63a8\u4ecb\u65b0\u7247\u300a\u5343\u9877\u6f84\u78a7\u7684\u65f6\u4ee3\u300b&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image14.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0529\&#x2F;thumb_1_224_122_20200529091151656904.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1458506.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-28\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1458290&quot;,&quot;title&quot;:&quot;\u4e24\u4f1a\u7279\u522b\u7b56\u5212\u8282\u76ee\uff1a\u63a8\u4ecb\u5f20\u827a\u8c0b\u65b0\u7247\u300a\u575a\u5982\u78d0\u77f3\u300b\u300a\u60ac\u5d16\u4e4b\u4e0a\u300b&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image14.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0527\&#x2F;thumb_1_224_122_20200527040952900837.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1458290.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-27\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1458145&quot;,&quot;title&quot;:&quot;\u4e24\u4f1a\u7279\u522b\u7b56\u5212\u8282\u76ee\uff1a\u63a8\u4ecb\u65b0\u7247\u300a\u5c01\u795e\u4e09\u90e8\u66f2\u300b&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image13.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0527\&#x2F;thumb_1_224_122_20200527091651592674.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1458145.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-26\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1457789&quot;,&quot;title&quot;:&quot;\u4e24\u4f1a\u7279\u522b\u7b56\u5212\u8282\u76ee\uff1a\u63a8\u4ecb\u65b0\u7247\u300a\u897f\u6e38\u8bb0\u771f\u5047\u7f8e\u7334\u738b\u300b&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image14.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0526\&#x2F;thumb_1_224_122_20200526085953497978.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1457789.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-25\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1457465&quot;,&quot;title&quot;:&quot;\u4e24\u4f1a\u7279\u522b\u7b56\u5212\u8282\u76ee\uff1a\u63a8\u4ecb\u65b0\u7247\u300a\u718a\u51fa\u6ca1\u00b7\u72c2\u91ce\u5927\u9646\u300b&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image13.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0522\&#x2F;thumb_1_224_122_20200522062257499752.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1457465.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-22\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1457310&quot;,&quot;title&quot;:&quot;\u4e24\u4f1a\u7279\u522b\u7b56\u5212\u8282\u76ee\uff1a\u63a8\u4ecb\u65b0\u7247\u300a\u6211\u548c\u6211\u7684\u5bb6\u4e61\u300b&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image14.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0522\&#x2F;thumb_1_224_122_20200522090219537956.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1457310.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-21\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1457055&quot;,&quot;title&quot;:&quot;\u4e24\u4f1a\u7279\u522b\u7b56\u5212\u8282\u76ee\uff1a\u63a8\u4ecb\u65b0\u7247\u300a\u79c0\u7f8e\u4eba\u751f\u300b&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image14.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0520\&#x2F;thumb_1_224_122_20200520041720924811.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1457055.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-20\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1456896&quot;,&quot;title&quot;:&quot;\u4e24\u4f1a\u7279\u522b\u7b56\u5212\u8282\u76ee\uff1a\u63a8\u4ecb\u65b0\u7247\u300a749\u5c40\u300b&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image11.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0521\&#x2F;thumb_1_224_122_20200521011124342128.png&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1456896.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-19\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1456602&quot;,&quot;title&quot;:&quot;\u767e\u4ee3\u5149\u5f71\u5386\u4e45\u5f25\u65b0 \u4e16\u754c\u535a\u7269\u9986\u65e5 \u611f\u53d7\u4e2d\u56fd\u7535\u5f71\u535a\u7269\u9986\u7684\u9b45\u529b&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image11.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0519\&#x2F;thumb_1_224_122_20200519091623915695.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1456602.shtml&quot;,&quot;duration&quot;:&quot;0&quot;,&quot;airtime&quot;:&quot;2020-05-18\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1456178&quot;,&quot;title&quot;:&quot;\u89e3\u8bfb\u201c\u514d\u7a0e\u5229\u597d\u201d\u53cc\u653f\u7b56\uff1a\u80fd\u4e3a\u7535\u5f71\u884c\u4e1a\u7684\u53d1\u5c55\u5e26\u6765\u54ea\u4e9b\u652f\u6301\uff1f&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image14.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0515\&#x2F;thumb_1_224_122_20200515041954797088.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1456178.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-15\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1455841&quot;,&quot;title&quot;:&quot;\u6ee1\u5730\u7686\u662f\u80e1\u516b\u4e00\uff01\u300a\u9b3c\u5439\u706f\u300bIP\u77e9\u9635\u4e00\u56e2\u4e71 \u4f5c\u8005\u80cc\u9505\u51a4\u4e0d\u51a4\uff1f&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image14.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0514\&#x2F;thumb_1_224_122_20200514031301735539.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1455841.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-14\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1455806&quot;,&quot;title&quot;:&quot;\u300a\u5029\u5973\u5e7d\u9b42\uff1a\u4eba\u95f4\u60c5\u300b\u7206\u706b\u51fa\u5708 \u662f\u6361\u6f0f\u8fd8\u662f\u7f51\u5927\u7684\u65b0\u91cc\u7a0b\u7891\uff1f&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image13.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0514\&#x2F;thumb_1_224_122_20200514090500861713.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1455806.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-13\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1455661&quot;,&quot;title&quot;:&quot;\u4ece\u7d27\u6025\u62a2\u6551\u751f\u547d\u5230\u5904\u7406\u65e5\u5e38\u7410\u788e \u56fd\u9645\u62a4\u58eb\u8282 \u5411\u6700\u7f8e\u62a4\u58eb\u81f4\u656c&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image14.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0513\&#x2F;thumb_1_224_122_20200513090430196613.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1455661.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-12\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1455507&quot;,&quot;title&quot;:&quot;\u7cbe\u7b97\u5e08\u738b\u6676\u518d\u5bfc\u7ecf\u5178 \u300a\u501a\u5929\u5c60\u9f99\u8bb0\u300b\uff1a\u8425\u9500\u4ece\u9009\u89d2\u5f00\u59cb\uff1f&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image11.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0512\&#x2F;thumb_1_224_122_20200512094841916354.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1455507.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-11\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1454917&quot;,&quot;title&quot;:&quot;\u4e00\u5468\u70ed\u70b9\uff1a\u89e3\u8bfb\u5f71\u89c6\u884c\u4e1a\u8054\u5408\u5021\u8bae \u4e3a\u201c\u81ea\u6551\u201d\u5efa\u8a00\u732e\u7b56&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image14.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0508\&#x2F;thumb_1_224_122_20200508044743904659.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1454917.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-08\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1454487&quot;,&quot;title&quot;:&quot;\u300a\u4eca\u65e5\u5f71\u8bc4\u00b7\u81f4\u656c\u52b3\u52a8\u8005\u300b\u4e4b\u7e41\u8302\uff1a\u75ab\u60c5\u4e4b\u4e2d\u6709\u62c5\u5f53\u7684\u4f01\u4e1a\u5bb6\u4eec&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image11.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0506\&#x2F;thumb_1_224_122_20200506052833620891.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1454487.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-07\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1454002&quot;,&quot;title&quot;:&quot;\u300a\u4eca\u65e5\u5f71\u8bc4\u00b7\u81f4\u656c\u52b3\u52a8\u8005\u300b\u4e4b\u4f59\u9999\uff1a\u75ab\u60c5\u671f\u95f4\u575a\u5b88\u5c97\u4f4d\u7684\u6559\u5e08\u4eec&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image14.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0430\&#x2F;thumb_1_224_122_20200430101201175534.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1454002.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-06\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1454000&quot;,&quot;title&quot;:&quot;\u300a\u4eca\u65e5\u5f71\u8bc4\u00b7\u81f4\u656c\u52b3\u52a8\u8005\u300b\u4e4b\u77d7\u7acb\uff1a\u5411\u4e2d\u56fd\u519b\u4eba\u4e0e\u4eba\u6c11\u8b66\u5bdf\u81f4\u656c&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image11.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0430\&#x2F;thumb_1_224_122_20200430101135678150.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1454000.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-05\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1453999&quot;,&quot;title&quot;:&quot;\u300a\u4eca\u65e5\u5f71\u8bc4\u00b7\u81f4\u656c\u52b3\u52a8\u8005\u300b\u4e4b\u7480\u74a8\uff1a\u5e73\u51e1\u4e14\u4f1f\u5927\u7684\u533b\u62a4\u4eba\u5458\u4eec&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image13.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0430\&#x2F;thumb_1_224_122_20200430101103363111.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1453999.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-04\u671f&quot;&#125;,&#123;&quot;contentid&quot;:&quot;1453998&quot;,&quot;title&quot;:&quot;\u300a\u4eca\u65e5\u5f71\u8bc4\u00b7\u81f4\u656c\u52b3\u52a8\u8005\u300b\u4e4b\u751f\u5982\u590f\u82b1\uff1a\u4e3a\u65b0\u65f6\u671f\u7684\u52b3\u52a8\u8005\u4eec\u70b9\u8d5e&quot;,&quot;thumb&quot;:&quot;https:\&#x2F;\&#x2F;image11.m1905.cn\&#x2F;uploadfile\&#x2F;2020\&#x2F;0430\&#x2F;thumb_1_224_122_20200430101037817743.jpg&quot;,&quot;url&quot;:&quot;https:\&#x2F;\&#x2F;www.1905.com\&#x2F;video\&#x2F;play\&#x2F;1453998.shtml&quot;,&quot;duration&quot;:&quot;480&quot;,&quot;airtime&quot;:&quot;2020-05-01\u671f&quot;&#125;]&#125;)</span><br></pre></td></tr></table></figure>

<p>从preview里可以看到info是我们想要的数据。</p>
<img src="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/5.png" style="zoom:33%;">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用replace()将代码中最前面reloadList(和最后面的)剪掉</span></span><br><span class="line">result=result.replace(<span class="string">'reloadList('</span>,<span class="string">''</span>).replace(<span class="string">')'</span>,<span class="string">''</span>) </span><br><span class="line">    <span class="comment">#json.loads()将json字符串格式转为dict格式,json.dumps()则相反。</span></span><br><span class="line">    final=json.loads(result)</span><br><span class="line">    print(final) <span class="comment">#final就是我们最终的可以进行爬取的数据</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> final[<span class="string">'info'</span>]:</span><br><span class="line">        print(i[<span class="string">'url'</span>],i[<span class="string">'title'</span>],i[<span class="string">'thumb'</span>])</span><br><span class="line">        <span class="comment">#读取图片，使用content返回一个二进制数据</span></span><br><span class="line">        img=requests.get(i[<span class="string">'thumb'</span>]).content<span class="comment">#,返回图片的二进制内容，i['thumb']就是图片的链接</span></span><br><span class="line">        <span class="comment">#存储照片</span></span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'D:/photos/%s.jpg'</span>%i[<span class="string">'title'</span>],<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(img)</span><br><span class="line">        <span class="keyword">break</span> <span class="comment">#循环一次试试</span></span><br><span class="line">find()<span class="comment">#正常返回的数据不会有reloadList，只有后面的info</span></span><br></pre></td></tr></table></figure>

<p>out:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;month&#39;: &#123;&#39;6&#39;: 6, &#39;5&#39;: 5, &#39;4&#39;: 4, &#39;3&#39;: 3, &#39;2&#39;: 2, &#39;1&#39;: 1&#125;, &#39;info&#39;: [&#123;&#39;contentid&#39;: &#39;1458732&#39;, &#39;title&#39;: &#39;一周热点：解读“两会”中的电影之声&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image11.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0529&#x2F;thumb_1_224_122_20200529043833417252.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1458732.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-29期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1458506&#39;, &#39;title&#39;: &#39;两会特别策划节目：推介新片《千顷澄碧的时代》&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image14.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0529&#x2F;thumb_1_224_122_20200529091151656904.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1458506.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-28期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1458290&#39;, &#39;title&#39;: &#39;两会特别策划节目：推介张艺谋新片《坚如磐石》《悬崖之上》&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image14.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0527&#x2F;thumb_1_224_122_20200527040952900837.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1458290.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-27期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1458145&#39;, &#39;title&#39;: &#39;两会特别策划节目：推介新片《封神三部曲》&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image13.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0527&#x2F;thumb_1_224_122_20200527091651592674.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1458145.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-26期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1457789&#39;, &#39;title&#39;: &#39;两会特别策划节目：推介新片《西游记真假美猴王》&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image14.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0526&#x2F;thumb_1_224_122_20200526085953497978.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1457789.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-25期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1457465&#39;, &#39;title&#39;: &#39;两会特别策划节目：推介新片《熊出没·狂野大陆》&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image13.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0522&#x2F;thumb_1_224_122_20200522062257499752.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1457465.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-22期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1457310&#39;, &#39;title&#39;: &#39;两会特别策划节目：推介新片《我和我的家乡》&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image14.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0522&#x2F;thumb_1_224_122_20200522090219537956.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1457310.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-21期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1457055&#39;, &#39;title&#39;: &#39;两会特别策划节目：推介新片《秀美人生》&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image14.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0520&#x2F;thumb_1_224_122_20200520041720924811.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1457055.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-20期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1456896&#39;, &#39;title&#39;: &#39;两会特别策划节目：推介新片《749局》&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image11.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0521&#x2F;thumb_1_224_122_20200521011124342128.png&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1456896.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-19期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1456602&#39;, &#39;title&#39;: &#39;百代光影历久弥新 世界博物馆日 感受中国电影博物馆的魅力&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image11.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0519&#x2F;thumb_1_224_122_20200519091623915695.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1456602.shtml&#39;, &#39;duration&#39;: &#39;0&#39;, &#39;airtime&#39;: &#39;2020-05-18期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1456178&#39;, &#39;title&#39;: &#39;解读“免税利好”双政策：能为电影行业的发展带来哪些支持？&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image14.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0515&#x2F;thumb_1_224_122_20200515041954797088.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1456178.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-15期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1455841&#39;, &#39;title&#39;: &#39;满地皆是胡八一！《鬼吹灯》IP矩阵一团乱 作者背锅冤不冤？&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image14.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0514&#x2F;thumb_1_224_122_20200514031301735539.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1455841.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-14期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1455806&#39;, &#39;title&#39;: &#39;《倩女幽魂：人间情》爆火出圈 是捡漏还是网大的新里程碑？&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image13.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0514&#x2F;thumb_1_224_122_20200514090500861713.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1455806.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-13期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1455661&#39;, &#39;title&#39;: &#39;从紧急抢救生命到处理日常琐碎 国际护士节 向最美护士致敬&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image14.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0513&#x2F;thumb_1_224_122_20200513090430196613.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1455661.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-12期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1455507&#39;, &#39;title&#39;: &#39;精算师王晶再导经典 《倚天屠龙记》：营销从选角开始？&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image11.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0512&#x2F;thumb_1_224_122_20200512094841916354.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1455507.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-11期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1454917&#39;, &#39;title&#39;: &#39;一周热点：解读影视行业联合倡议 为“自救”建言献策&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image14.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0508&#x2F;thumb_1_224_122_20200508044743904659.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1454917.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-08期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1454487&#39;, &#39;title&#39;: &#39;《今日影评·致敬劳动者》之繁茂：疫情之中有担当的企业家们&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image11.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0506&#x2F;thumb_1_224_122_20200506052833620891.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1454487.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-07期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1454002&#39;, &#39;title&#39;: &#39;《今日影评·致敬劳动者》之余香：疫情期间坚守岗位的教师们&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image14.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0430&#x2F;thumb_1_224_122_20200430101201175534.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1454002.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-06期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1454000&#39;, &#39;title&#39;: &#39;《今日影评·致敬劳动者》之矗立：向中国军人与人民警察致敬&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image11.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0430&#x2F;thumb_1_224_122_20200430101135678150.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1454000.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-05期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1453999&#39;, &#39;title&#39;: &#39;《今日影评·致敬劳动者》之璀璨：平凡且伟大的医护人员们&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image13.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0430&#x2F;thumb_1_224_122_20200430101103363111.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1453999.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-04期&#39;&#125;, &#123;&#39;contentid&#39;: &#39;1453998&#39;, &#39;title&#39;: &#39;《今日影评·致敬劳动者》之生如夏花：为新时期的劳动者们点赞&#39;, &#39;thumb&#39;: &#39;https:&#x2F;&#x2F;image11.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0430&#x2F;thumb_1_224_122_20200430101037817743.jpg&#39;, &#39;url&#39;: &#39;https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1453998.shtml&#39;, &#39;duration&#39;: &#39;480&#39;, &#39;airtime&#39;: &#39;2020-05-01期&#39;&#125;]&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;www.1905.com&#x2F;video&#x2F;play&#x2F;1458732.shtml 一周热点：解读“两会”中的电影之声 https:&#x2F;&#x2F;image11.m1905.cn&#x2F;uploadfile&#x2F;2020&#x2F;0529&#x2F;thumb_1_224_122_20200529043833417252.jpg</span><br></pre></td></tr></table></figure>



<p>图片已经保存到我们的指定目录下，成功！</p>
<p><img src="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/6.png" alt></p>
<p>当然我们可以循环遍历出所有年份月份的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">()</span>:</span></span><br><span class="line">    param=&#123;</span><br><span class="line"><span class="string">'callback'</span>: <span class="string">'reloadList'</span>,</span><br><span class="line"><span class="string">'m'</span>: <span class="string">'converged'</span>,</span><br><span class="line"><span class="string">'a'</span>: <span class="string">'info'</span>,</span><br><span class="line"><span class="string">'type'</span>: <span class="string">'jryp'</span></span><br><span class="line">&#125; <span class="comment">#从网站看到还有其他年份和月份的数据，我们可以直接修改param中的year和month</span></span><br><span class="line">    url=<span class="string">'http://www.1905.com/api/content/index.php'</span></span><br><span class="line">    header=&#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'</span>&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2016</span>,<span class="number">2021</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">13</span>):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                param[<span class="string">'year'</span>]=str(i)</span><br><span class="line">                param[<span class="string">'month'</span>]=str(j)</span><br><span class="line">                r=requests.get(url,headers=header,params=param) <span class="comment">#头部不伪装成浏览器也可以访问</span></span><br><span class="line">                result=r.text</span><br><span class="line">                a=result.replace(<span class="string">'reloadList('</span>,<span class="string">''</span>).replace(<span class="string">')'</span>,<span class="string">''</span>)</span><br><span class="line">                <span class="comment">#json.loads()将json字符串格式转为dict格式,json.dumps()则相反。</span></span><br><span class="line">                final=json.loads(a)</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> final[<span class="string">'info'</span>]:</span><br><span class="line">                    print(k[<span class="string">'url'</span>],k[<span class="string">'title'</span>],k[<span class="string">'thumb'</span>])</span><br><span class="line">             <span class="keyword">except</span>:</span><br><span class="line">                 print(<span class="string">'non'</span>)</span><br><span class="line">find()<span class="comment">#正常返回的数据不会有reloadList，只有后面的info</span></span><br></pre></td></tr></table></figure>

<p>out：</p>
<p><img src="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/10.png" alt></p>
<p>出现了个问题，网站上2016年前几个月是没有数据的，但结果输出了这年12月份的数据。2020年的后几个月没有数据，但也输出了这年1月的数据。我猜测是由于他们的后台设置。解决方法可以把2016和2020单独遍历。但我觉得还有更好的办法。（比如在代码中把年份和月份取出来进行遍历）</p>
<pre><code>except:
     print(&apos;non&apos;)</code></pre><p>find()#正常返回的数据不会有reloadList，只有后面的info</p>
<h3 id="方法二：用scrapy"><a href="#方法二：用scrapy" class="headerlink" title="方法二：用scrapy"></a>方法二：用scrapy</h3><p>如何创建工程我们前面已经说过，现在不再累述。直接打开我们创建的爬虫文件movietalk.py</p>
<p>完整代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MovietalkSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'movietalk'</span></span><br><span class="line"><span class="comment">#    allowed_domains = ['1905.com']</span></span><br><span class="line">    start_urls = [<span class="string">'https://www.1905.com/api/content/index.php?callback=reloadList&amp;m=converged&amp;a=info&amp;type=jryp&amp;year=2020&amp;month=5'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 用replace()将代码中最前面reloadList(和最后面的)剪掉</span></span><br><span class="line">        result = response.text.replace(<span class="string">'reloadList('</span>, <span class="string">''</span>).replace(<span class="string">')'</span>, <span class="string">''</span>)</span><br><span class="line">        <span class="comment"># json.loads()将json字符串格式转为dict格式,json.dumps()则相反。</span></span><br><span class="line">        final = json.loads(result)</span><br><span class="line">        print(final)  <span class="comment"># final就是我们最终的可以进行爬取的数据</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> final[<span class="string">'info'</span>]:</span><br><span class="line">            items=&#123;<span class="string">'url'</span>:i[<span class="string">'url'</span>],<span class="string">'title'</span>:i[<span class="string">'title'</span>],<span class="string">'thumb'</span>:i[<span class="string">'thumb'</span>]&#125; <span class="comment">#保存到一个csv文件</span></span><br><span class="line">            <span class="keyword">yield</span> items</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>这里要注意的是，由于我们不是用request库去请求访问，因此不能请求访问时把参数放到params中。由于我们知道设置params的效果就相当于直接在url后面添加上这些参数。</p>
<p>因此我们直接将下图中整个url（包括？后面的）复制下来。</p>
<p><img src="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/3.png" alt></p>
<p>还要注意的是，响应内容中的title是中文，如果直接保存成csv文件会乱码。我们打开settings.py文件，加入一句代码即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FEED_EXPORT_ENCODING &#x3D; &#39;gb18030&#39;</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/7.png" alt></p>
<p>如果要保存为json或txt格式，则加这行代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FEED_EXPORT_ENCODING &#x3D;&#39;utf-8&#39;</span><br></pre></td></tr></table></figure>

<p>查看是否保存成功：</p>
<p><img src="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/8.png" alt></p>
<p><img src="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/9.png" alt></p>
<p>成功！</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/" data-id="ckn1t0igf001hrgvt6k005jb6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Request/" rel="tag">Request</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Scrapy/" rel="tag">Scrapy</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB/" rel="tag">动态爬虫</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-机器学习之监督学习算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" class="article-date">
  <time datetime="2020-07-06T15:05:38.000Z" itemprop="datePublished">2020-07-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习算法</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习之监督学习算法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文是一个关于常用的监督学习算法的学习笔记，以及与其相关的一些知识点。</p>
<p>在监督学习中，我们已经被告知了什么是所谓的正确答案。即下面例子中的良性和恶性。</p>
<h3 id="回归问题（预测连续的数值输出）"><a href="#回归问题（预测连续的数值输出）" class="headerlink" title="回归问题（预测连续的数值输出）"></a>回归问题（预测连续的数值输出）</h3><p>例子：房屋价格预测</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/1.png" alt></p>
<h4 id="线性回归算法"><a href="#线性回归算法" class="headerlink" title="线性回归算法"></a>线性回归算法</h4><p>也就是根据已有的样本拟合一条直线，用线性方程（一次函数）表示（称为假设函数）。如何拟合（设置参数）这个假设函数是重点。方程中的参数称为模型参数。</p>
<img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/5.png">

<p>m represents Number of training examples</p>
<p>x represents input variable/features</p>
<p>y represents output variable/‘target’ variable</p>
<p>代价函数（总是一个凹形函数）</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/6.png" alt></p>
<p>我们通过改变这两个参数，让代价函数收敛(取到最小值)，此时的假设函数能最好拟合实际情况。</p>
<h5 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h5><p>我们用梯度下降算法来改变参数，最终将代价函数最小化，公式为：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/7.png" alt></p>
<p>a称为学习速率。用来控制当梯度下降时，我们迈出多大的步子。a越大，梯度下降就越迅速。 化简上式得：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/8.png" alt></p>
<h4 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h4><p>实际情况当我们的参数很多时（比如在预测房价时，参数不只有面积，还有房屋年龄，卧室数量，地理位置…我们就要进行多元线性回归，这部分会用到线性代数的知识）</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/9.png" alt></p>
<p>Tips 1</p>
<p>将x均值归一化（特征收敛），使这些x都处于某个较小的范围，这样后面当我们用梯度下降算法时就可以更快的找到那个最佳点。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/10.png" alt></p>
<p>Tips 2</p>
<p>如何选择学习率a</p>
<p>a过小会导致迭代次数过多</p>
<p>a过大会导致代价函数不收敛，甚至发散。</p>
<p>a取0.001，0.003，0.01，0.03，0.1，0.3，1，3…十倍的取，去看代价函数的值随迭代次数变化的情况（当我们判断代价函数此时收敛时，迭代次数越少，说明此时选择的a就越适合）找到合适的a。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/11.png" alt></p>
<p>上图当迭代次数为300左右时，代价函数就已经很好的收敛了。</p>
<h5 id="用正规方程求参数（不同于梯度下降算法）"><a href="#用正规方程求参数（不同于梯度下降算法）" class="headerlink" title="用正规方程求参数（不同于梯度下降算法）"></a>用正规方程求参数（不同于梯度下降算法）</h5><p>这种方法不需要进行tips 1 .</p>
<p>例子：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/14.png" alt></p>
<p>当X的转置*X不可逆时，我们就不能算出参数。通常出现这种情况是由于以下两个原因：</p>
<p>1.两个成倍数的特征值（删掉其中一个）</p>
<p>2.特征值太多，数量多于样本数（删掉一些）</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/15.png" alt></p>
<p>对比</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/13.png" alt></p>
<h4 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h4><p>有时我们的假设函数为二次函数或多次函数才能很好拟合：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/12.png" alt></p>
<h3 id="分类问题（预测一个离散的数值输出）"><a href="#分类问题（预测一个离散的数值输出）" class="headerlink" title="分类问题（预测一个离散的数值输出）"></a>分类问题（预测一个离散的数值输出）</h3><p>例子：预测肿瘤是良性还是恶性</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/2.png" alt></p>
<p>当有多个特征值（age，size）时：</p>
<p>（o代表良性，x代表恶性）</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/3.png" alt></p>
<p>实际上机器学习甚至可以设计一种算法处理无穷多个特征值的算法。</p>
<h4 id="logistic回归（一种分类算法）"><a href="#logistic回归（一种分类算法）" class="headerlink" title="logistic回归（一种分类算法）"></a>logistic回归（一种分类算法）</h4><p>算法的输出值会一直介于0到1之间</p>
<p>如下图</p>
<p>Sigmoid/Logistic 函数：g(z)</p>
<p>在logistic回归中假设函数的表示方法：</p>
<p>假设函数也可看作输入以θ为参数的x时，假设输出y=1的概率。</p>
<p>当g(z)大于等于0.5（z大于等于0）时，假设函数预测为1。</p>
<p>当g(z)小于0.5（z小于0）时，假设函数预测为0。</p>
<p>z其实就是我们之前回归问题中的假设函数。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/16.png" alt></p>
<p>假如现在我们已经拟合出了参数θ1  θ2   …</p>
<p>下图中的那条粉色线被称为决策边界，粉线右边输出为1，左边为0；它是假设函数的一个属性。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/17.png" alt></p>
<p>如果我们的数据集是这样的：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/18.png" alt></p>
<p>我们可以在假设函数中添加高阶多项式。假如现在我们已经拟合出了参数θ1  θ2   …，可以看到现在的决策边界是一个圆。这说明决策边界可以是更复杂的曲线，取决于我们假设函数。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/19.png" alt></p>
<p>如何拟合参数θ</p>
<p>先介绍一下代价函数。在logistic回归中，如果只有一个训练样本，我们的代价函数是这样的：</p>
<p>当y=1时，如下图，假如假设函数算出来的值也是1，则把1代到代价函数-log(1)中，此时代价为0。若假设函数趋于0，则代价为无穷大。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/20.png" alt></p>
<p>代价函数图象的推导：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/21.png" alt></p>
<p>红线部分 自变量x(这里就是我们的代价函数)范围0到1，y再取负值则沿x轴翻转，就得到代价函数的图像。</p>
<p>当y=0时，如下图</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/22.png" alt></p>
<p>我们把单个样本代价函数用一个式子表示:</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/23.png" alt></p>
<p>现在，当有多个样本时，我们真正的代价函数可以表示为：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/24.png" alt></p>
<p>与之前一样，用梯度下降算法，我们不断改变θ（改变θ要同时改变θ1，θ2…)，找到代价函数的最小值，这时的θ就是最佳的。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/7.png" alt></p>
<p>可以看到求出来的更新θj的式子与线性回归中的一样！</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/25.png" alt></p>
<p>同样的，我们也可以把特征缩放的方法用在这里，让梯度下降收敛更快。</p>
<h4 id="多类别问题"><a href="#多类别问题" class="headerlink" title="多类别问题"></a>多类别问题</h4><p>如果有多种类别，例如三种时，如下图所示：</p>
<p>这时我们的解决方法是把最初的训练集分成三种情况，求出这三个训练集各自的假设函数。我们给出一个新的x值，代到这三个假设函数中，得到的值（概率）最大的那个就是我们应该选择的类别。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/26.png" alt></p>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>过拟合是指我们的假设函数能够很好的拟合现有的数据集，但我们用了太多的特征（x）去拟合它，当有新的样本时并不能推测出一个合适的输出值。在回归问题和分类问题中都有可能会出现。如下图所示，右一就是过拟合的情况</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/27.png" alt></p>
<h4 id="解决方法（对于线性回归问题）："><a href="#解决方法（对于线性回归问题）：" class="headerlink" title="解决方法（对于线性回归问题）："></a>解决方法（对于线性回归问题）：</h4><ol>
<li><p>减少特征数量（人工剔除或用算法自动剔除）</p>
</li>
<li><p>正则化：保留所有的特征，但把其中一些参数θj的值减小，让一些特征值的作用变小。</p>
</li>
</ol>
<h5 id="将正则化用于梯度下降算法中"><a href="#将正则化用于梯度下降算法中" class="headerlink" title="将正则化用于梯度下降算法中"></a>将正则化用于梯度下降算法中</h5><p>   右边的那项为正则化项 ，λ为正则化参数。</p>
<p>   <img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/28.png" alt></p>
<p>对于θ0，按我们前面的说法是一个常数，因为x0我们默认为1。没有必要进行正则化。</p>
<p>对于其他的θj，每次变化的表达式依然是求偏导，只不过现在多了一项。整理式子后我们发现θj的表达式中，（1-a*λ/m）是略小于1的，因为一般m很大。这说明在添加正则项后，每次θ都会再变小一点点。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/29.png" alt></p>
<h5 id="将正则化用于正规方程中："><a href="#将正则化用于正规方程中：" class="headerlink" title="将正则化用于正规方程中："></a>将正则化用于正规方程中：</h5><p>θ新的表达式如下：（这里不再具体推导多的那一项是怎么来的，矩阵为(n+1)行*(n+1)列）</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/30.png" alt></p>
<p>如果λ&gt;0，我们可以确保括号里的那一项是可逆的。</p>
<h4 id="解决方法（对于逻辑回归问题）："><a href="#解决方法（对于逻辑回归问题）：" class="headerlink" title="解决方法（对于逻辑回归问题）："></a>解决方法（对于逻辑回归问题）：</h4><p>与在线性回归问题中处理梯度下降算法的过拟合类似</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/31.png" alt></p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/32.png" alt></p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>当特征值很多时，前面学的算法会难以处理，因为计算量过大。此时我们会用神经网络模型。</p>
<p>神经网络模型会模拟人类大脑处理信息的方式，一个神经元接收一些信息（多个输入）并进行处理后，将输出传递给下一个神经元。输出用一个假设函数表示。假设函数中的参数在一些文献中也叫做weights权重。</p>
<p>下图中x0称为偏执单元，视情况决定是否添加。橙圈为一个神经元。<img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/33.png" alt></p>
<p>神经网络：</p>
<p>第一层叫输入层（input layer），第二层叫隐藏层（hidden layer，可以有多层，隐藏层每一项称为单元，它们的输出称为激活项），第三层叫输出层（output layer）。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/34.png" alt></p>
<p>前向传播</p>
<p>下图中g(z)就是sigmoid函数。权重矩阵为三行四列。一般来说，如果一个网络第j层有sj个单元，在j+1层有s(j+1)个单元，则θj的维度为s(j+1)行sj+1列。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/35.png" alt></p>
<h4 id="例1（非线性分类）"><a href="#例1（非线性分类）" class="headerlink" title="例1（非线性分类）"></a>例1（非线性分类）</h4><p>如果我们想要得到这样一个假设函数（实现非线性分类问题），当两个输入相同时，分为一类，当两个输入不同时，分为另一类。如下图所示</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/40.png" alt></p>
<h5 id="感知机原理"><a href="#感知机原理" class="headerlink" title="(感知机原理)"></a>(感知机原理)</h5><p>首先先用神经网络算法实现与操作</p>
<p>如果有三个特征值（x0默认为1），设参数为-30，20，20。x1和x2取值范围为0或1，假设函数就是g(-30+20X1+20X2)，现在进行检验，输入四种组合，我们的假设函数都能得到正确的结果。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/36.png" alt></p>
<p>或操作：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/37.png" alt></p>
<p>非操作：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/38.png" alt></p>
<p>将前面几种操作组合起来，就得到我们想要的实现。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/39.png" alt></p>
<h4 id="例2（多元分类）"><a href="#例2（多元分类）" class="headerlink" title="例2（多元分类）"></a>例2（多元分类）</h4><p>我们想要判断一张图片是行人，汽车，摩托还是货车。理想情况下，我们希望假设函数输出四种结果，每种结果分别对应行人，汽车…如下图</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/41.png" alt></p>
<h5 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h5><p>根据训练集，从第一层到最后一层去构造每层假设方程的参数。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/42.png" alt></p>
<h5 id="表示代价函数"><a href="#表示代价函数" class="headerlink" title="表示代价函数"></a>表示代价函数</h5><p>我们用的是逻辑回归的代价函数的一般形式（输出有多种可能时）。</p>
<p>如下图所示。K个输出单元（可以看作假设函数的输出是一个k维向量），这里有四个；h(x)i指输出向量中的第几个；m个样本；L层，这里有四层；Sl表示第l层有S个单元。正则项可以理解为第l层中，第j个单元第i个参数θji的平方（一列一列的把参数依次加起来）。从前文可知，在第l层，θ参数可以看作一个矩阵，由i和j来标识。这里我们依然不加入偏差项(i=0项)</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/43.png" alt></p>
<h5 id="用反向传播算法求代价函数的偏导项"><a href="#用反向传播算法求代价函数的偏导项" class="headerlink" title="用反向传播算法求代价函数的偏导项"></a>用反向传播算法求代价函数的偏导项</h5><p>（有关反向传播算法的具体解释）<a href="https://www.jianshu.com/p/964345dddb70" target="_blank" rel="noopener">https://www.jianshu.com/p/964345dddb70</a></p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/45.png" alt></p>
<p>现在先来看看只有一个样本集的情况，a表示激活值（一个向量）：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/46.png" alt></p>
<p>我们先表示一个激活值（不考虑第一层，每个激活值都是最开始的输入通过一或多次的g(z)函数得出来的输出）的代价（误差），<img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/47.png" alt>代表第l层第j个节点的误差。</p>
<p>从最后一层反向将前一层的代价（最后一层为假设的输出值与样本y值之差）表示出来。所有的代价相加就等于代价函数对θ求偏导的值。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/48.png" alt></p>
<p>当有m个样本集时：</p>
<p>用循环把每个样本的误差加一起。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/49.png" alt></p>
<p>注意！对于反向传播，我们用随机取值的方法将参数初始化。如下图：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/52.png" alt></p>
<p>反向传播算法的缺点：</p>
<p>容易出现一些无法被程序识别的小bug，即使我们观察到代价函数是逐渐减小的，但最终的结果可能会比正确的结果高出一个量级。</p>
<p>解决办法：</p>
<h5 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h5><p>当θ为一个实数时，让对θ的导数近似等于对θ的双侧差分</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/51.png" alt></p>
<p>当θ为一个n维向量时，让对每个θ的偏导等于它们各自的双侧差分<img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/50.png" alt></p>
<p>之后我们验证通过梯度检验算出的代价函数的偏导是否近似等于反向传播算出来的偏导。如果很接近则说明反向传播是正确运行的，在正式运行代码时要关掉梯度检验，因为它梯度检验的运行速度很慢。</p>
<h5 id="神经网络模型总述"><a href="#神经网络模型总述" class="headerlink" title="神经网络模型总述"></a>神经网络模型总述</h5><p>1.选择一个合适的神经网络结构</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/53.png" alt></p>
<p>2.将参数初始化。</p>
<p>3.用前向传播获得假设函数。</p>
<p>4.通过代码计算代价函数。</p>
<p>5.用反向传播算法计算各个参数的偏导</p>
<p>6.用梯度检验确定反向传播算法没问题后，关闭梯度检验（如下图）</p>
<p>7.知道偏导后就可以用梯度下降算法或其他高级算法更新θ，逐渐找到代价函数的局部最小值。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/55.png" alt></p>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><h4 id="选择多项式（选择模型）"><a href="#选择多项式（选择模型）" class="headerlink" title="选择多项式（选择模型）"></a>选择多项式（选择模型）</h4><p>将样本集分为训练集，(交叉)验证集和测试集（比例为3:1:1）：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/57.png" alt></p>
<p>三个集合各自的代价函数的表示：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/58.png" alt></p>
<p>如下图所示，假如有10个待选模型。我们先用验证集的数据确定参数，再算出这10个代价函数的值（比较大小），选择哪个假设函数是最优的，假如第五个模型是最优的。以这个最优模型的参数为参考，去人工调整训练集模型的参数。</p>
<p>在不断调整训练集的模型的过程中，我们用测试集（相当于新样本）去测试模型的泛化误差。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/59.png" alt></p>
<h4 id="如何判断偏差欠拟合与方差过拟合"><a href="#如何判断偏差欠拟合与方差过拟合" class="headerlink" title="如何判断偏差欠拟合与方差过拟合"></a>如何判断偏差欠拟合与方差过拟合</h4><p>当偏差过大时为欠拟合，当方差过大时为过拟合。</p>
<p>一开始，训练误差与验证误差一开始都很大，此时为欠拟合。随着假设函数多项式次数的增加，训练误差变得很小，而验证误差远远大于训练误差，此时为过拟合。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/60.png" alt></p>
<h4 id="正则化与欠拟合、过拟合的关系"><a href="#正则化与欠拟合、过拟合的关系" class="headerlink" title="正则化与欠拟合、过拟合的关系"></a>正则化与欠拟合、过拟合的关系</h4><p>λ越大，对参数的惩罚程度越大，欠拟合程度越高。</p>
<p>λ越小，对参数的惩罚程度越小，过拟合程度越高。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/61.png" alt></p>
<p>如何选择正则化参数λ：</p>
<p>尝试一组λ，找出其中使验证误差最小的λ。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/62.png" alt></p>
<p>随λ的变化，训练误差与验证误差的变化如下：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/63.png" alt></p>
<h4 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h4><p>判断模型是否存在偏差或方差问题</p>
<p>m就找训练集中的大概10个20个就可以了。</p>
<p>正常情况下，我们假设模型是一个二次函数（这个训练集比较符合二次函数的特征）：</p>
<p>随着训练样本的增加，训练误差会越来越大。（样本少时能很好拟合，但样本多时二次函数可能就已经不足以拟合了）</p>
<p>而验证误差会越来越小，因为随着样本的增大，能获得更好的泛化表现。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/64.png" alt></p>
<p>高偏差时的学习曲线：</p>
<p>如果我们用一个一次函数作为假设函数：</p>
<p>随着训练样本的增加，训练误差会不断增大直到趋近一个稳定值。因为我们知道一次函数并不能很好的拟合这些样本，即使样本量越来越大。而验证误差会从一个很高的值逐渐减小直到趋近一个稳定值。最终两种误差值会比较接近。<img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/65.png" alt></p>
<p>高方差时的学习曲线：</p>
<p>当我们用一个高阶函数作为（模型）假设函数时：</p>
<p>随着样本量不断的增大，模型会变得越来越好。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/66.png" alt></p>
<p>因此如果出现过拟合问题，增大训练样本数量对改进算法是有帮助的。</p>
<h4 id="如何修正算法"><a href="#如何修正算法" class="headerlink" title="如何修正算法"></a>如何修正算法</h4><p>在了解过拟合和欠拟合之后。当得到一个线性回归模型但它的误差很大时，我们可以从以下方面去修正这个模型（修正欠拟合或过过拟合）：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/56.png" alt></p>
<h4 id="对于神经网络的欠拟合和过拟合"><a href="#对于神经网络的欠拟合和过拟合" class="headerlink" title="对于神经网络的欠拟合和过拟合"></a>对于神经网络的欠拟合和过拟合</h4><p>通常隐藏层越多，每层单元越多（计算量也就越大），就意味着参数越多，更容易出现过拟合。我们可以根据之前所说的将样本集划分为训练集，验证集，测试集来找最适合的那个模型结构。</p>
<h3 id="机器学习系统设计"><a href="#机器学习系统设计" class="headerlink" title="机器学习系统设计"></a>机器学习系统设计</h3><h4 id="执行的优先级"><a href="#执行的优先级" class="headerlink" title="执行的优先级"></a>执行的优先级</h4><p>例子：垃圾邮件分类器</p>
<p>如何让这个分类器错误率更小（随机选择以下的方法，因为不能确定哪一个效果更好）：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/68.png" alt></p>
<p>一开始时，先用一个尽量简单粗暴的模型去实现，之后再根据学习曲线去调整这个模型。同时，对于那些经常被错误分类的邮件，我们看看这些邮件内容它们有没有什么共同特征（误差分析），看是否还需要添加新的特征。</p>
<p>在决定要不要使用词干提取器（如Porter Stemmer，会将discount，discounts，discounting…看成一个）时，我们可以用交叉验证集的数据验证在使用它前后误差率的变化。</p>
<h4 id="不对称性分类的误差评估（度量率-召回率）"><a href="#不对称性分类的误差评估（度量率-召回率）" class="headerlink" title="不对称性分类的误差评估（度量率/召回率）"></a>不对称性分类的误差评估（度量率/召回率）</h4><p>例如我们想要设计一个模型判断是否是癌症，由于我们知道患病率本来就是很低的，所以对于那些验证集样本，我们不能接受一个相对高的误差率。（比如误差率是1%，而验证集样本的患病率是0.5%，说明这个模型不好。因为即使我们把所有样本都当成未患病的，误差率也只有0.5%。）</p>
<p>对于这种偏斜类，我们可以用度量率或召回率（越高越好）来度量误差</p>
<p>度量率：在验证集中，对于我们预测的那些患有癌症的病人，真正有多少个患有癌症的比率。</p>
<p>召回率：在验证集中，对于那些患有癌症的病人，我们预测正确的比率。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/69.png" alt></p>
<p>度量率和召回率的权衡</p>
<p>假如我们将假设函数的临界值设为0.7。假设值&gt;=0.7时预测为1，此时的度量率高，而召回率低。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/70.png" alt></p>
<p>用F1 Score值（0-1之间，越大越好）来度量：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/71.png" alt></p>
<p>其中一种解决办法</p>
<p>采用欠采样</p>
<p>如果有100个正样本，1w个负样本，那在模型训练时只采用100个正样本，100个负样本。</p>
<p>或过采样</p>
<p>在模型训练时采用100个正样本重复100次，1w个负样本。</p>
<h3 id="支持向量机（SVM）-大间距分类器"><a href="#支持向量机（SVM）-大间距分类器" class="headerlink" title="支持向量机（SVM）/大间距分类器"></a>支持向量机（SVM）/大间距分类器</h3><h4 id="推导代价函数"><a href="#推导代价函数" class="headerlink" title="推导代价函数"></a>推导代价函数</h4><p>在逻辑函数中，一个样本的代价函数表示为</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/72.png" alt></p>
<p>（图左）当y=1时，代价函数替换为品红色线，用cost1(z)表示。</p>
<p>（图右）当y=0时，代价函数替换为品红色线，用cost0(z)表示。</p>
<p>用上面的两项代替逻辑回归代价函数函数表达式的那两项，去掉1/m（习惯），不再用λ参数而是C去权衡代价项和正则项的权重，就得到支持向量机的代价函数：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/73.png" alt></p>
<h4 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h4><p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/74.png" alt></p>
<h4 id="分析代价函数"><a href="#分析代价函数" class="headerlink" title="分析代价函数"></a>分析代价函数</h4><p>现在我们要使代价函数最小，根据图形可知</p>
<p>如果y=1时，z&gt;=1代价函数就最小（我们知道根据假设函数z&gt;=0时就能够正确分类）。（通常参数C会选取的很大）</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/75.png" alt></p>
<p>SVM决策边界：</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/76.png" alt></p>
<p>举例</p>
<p>黑色线就是向量机的决策边界（黑色线到正/负样本的距离称为间距，它比起其他线间距是最大的），相比其他的决策边界，它的鲁棒性更强，适用性也更好。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/77.png" alt></p>
<h4 id="高斯核函数exp"><a href="#高斯核函数exp" class="headerlink" title="高斯核函数exp()"></a>高斯核函数exp()</h4><p>用于在向量机中定义新的特征变量，获得更加复杂精确的决策边界。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/80.png" alt></p>
<p>假如我们用特征值x1，x2构造三个新特征f1，f2，f3。手动选择三个标记点。这里的exp()就是核函数，确切来说是高斯核函数。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/81.png" alt></p>
<p>σ的作用：</p>
<p>如果标记点在[3,5]，当x为[3,5]时，f1位于最高点</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/78.png" alt></p>
<p>每个标记点l对应一个f</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/82.png" alt></p>
<p>来看看核函数是怎么预测输出的：</p>
<p>这里假设x是二维（两个特征值x1，x2）</p>
<p>三个特征变量f1，f2，f3，假如现在我们有三个标记点，且已经知道参数的值（下图所示）。与之前一样，当假设函数&gt;=0时，预测输出值为1。</p>
<p>现在如果有一个样本（x1,x2），靠近l1（品红色部分），那么根据核函数定义假设函数的值&gt;=0，预测输出为1。如果一个样本靠近上面两个标记点的任意一个，它都会被预测为1。通过这种方式我们就可以得到一条橘红的非线性的决策边界，内部预测为1，外部预测为0。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/79.png" alt></p>
<p>如何选取标记点l</p>
<p>将标记点直接选取为样本的位置</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/83.png" alt></p>
<p>（对于特征值x1，x2…，如果x1是房屋面积(例如1000平米)，x2是卧室的数量，它们的值差异很大，我们需要把x1缩放，以免||x-l||^2都由房屋面积决定，让SVM能考虑到所有特征变量f。）</p>
<p>f作为新的特征向量</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/84.png" alt></p>
<p>引入核函数之后的向量机的代价函数：</p>
<p>需要注意的是，在实际操作中，如果参数特别多时，我们计算θ^2(等同于[θ]^T[θ])时，会改为计算[θ]^TM[θ]，M为一个矩阵，与标记点有关。这样是为了简化计算。</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/85.png" alt></p>
<p>参数大小的选择</p>
<p><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/86.png" alt></p>
<h4 id="线性核函数"><a href="#线性核函数" class="headerlink" title="线性核函数"></a>线性核函数</h4><p>线性核函数为K(x,z)=xTz。</p>
<h4 id="向量机vs逻辑回归vs神经网络"><a href="#向量机vs逻辑回归vs神经网络" class="headerlink" title="向量机vs逻辑回归vs神经网络"></a>向量机vs逻辑回归vs神经网络</h4><h2 id><a href="#" class="headerlink" title></a><img src="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/87.png" alt></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def sgd_learn(shapes, inputs, outputs, sigma&#x3D;0.1, step&#x3D;0.1, iters&#x3D;10000):    </span><br><span class="line">    # YOUR CODE HERE</span><br><span class="line"> </span><br><span class="line">    init_theta&#x3D;initial_conditions(shapes, sigma)</span><br><span class="line">    theta, unflatten &#x3D; flatten(init_theta)</span><br><span class="line">#     face_loss(theta, unflatten, x, y)</span><br><span class="line">    hist_loss_500&#x3D;0</span><br><span class="line">    for j in range(iters):</span><br><span class="line">        rand_index&#x3D;np.random.choice(0,698,1)</span><br><span class="line">       # sing_output&#x3D;predict(inputs[rand_index], theta, unflatten)</span><br><span class="line">        grad_loss&#x3D;grad_face_loss(theta,unflatten,inputs[rand_index,:] ,outputs[rand_index,:])</span><br><span class="line">        flatten_loss&#x3D;flatten(grad_loss)</span><br><span class="line">        theta&#x3D;theta-flatten_loss*step</span><br><span class="line">        if j%500&#x3D;&#x3D;0:</span><br><span class="line">            print(&#39;%s iterations&#39; %j)</span><br><span class="line">        if j&lt;500:</span><br><span class="line">            hist_loss_500+&#x3D;total_face_loss(theta, unflatten)</span><br><span class="line">    w&#x3D;theta</span><br><span class="line">    return w, unflatten, hist_loss_500</span><br><span class="line"># flat, unflatten &#x3D; flatten(np.array([1,2,3]))</span><br><span class="line"># test_network &#x3D; [np.array([[1,0.5,-0.5], [0.0, 2.0, -1.0]]).T, </span><br><span class="line">#                 np.array([[2.0, 1.0], [1.0, -1.0]]).T]</span><br><span class="line"># test_theta, test_unflatten &#x3D; flatten(test_network)</span><br><span class="line"></span><br><span class="line"># with tick.marks(4):</span><br><span class="line">#     assert(abs(np.sum(grad_face_loss(test_theta, test_unflatten, np.array([1,2,3]), np.array([-1, 1])))-4.479)&lt;0.2)</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" data-id="ckn1t0igp001trgvte46odcjn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" rel="tag">支持向量机</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">监督学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Linux操作学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/05/Linux%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2020-07-05T15:03:12.000Z" itemprop="datePublished">2020-07-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/05/Linux%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Linux操作学习笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="快捷操作"><a href="#快捷操作" class="headerlink" title="快捷操作"></a>快捷操作</h3><p>ls:查看当前目录下的文件和目录。</p>
<p>cd:使用 cd 命令可进入其他目录(如cd code  表示进入code目录)</p>
<p><code>cd ..</code> 可以回到上一级目录，类似 Windows 的「向上」。</p>
<p><code>cd -</code> 表示回到上一次所在的目录，类似 Windows 的「后退」。</p>
<p><code>cd ~</code> 表示回到当前用户的主目录，类似 Windows 的「回到桌面」。</p>
<p><code>cd /</code> 表示进入根目录，它是一切目录的父目录</p>
<h3 id="查看目录结构：tree"><a href="#查看目录结构：tree" class="headerlink" title="查看目录结构：tree"></a>查看目录结构：tree</h3><p>使用 tree 命令，可以列出一个文件夹下的所有子文件夹和文件（以树形结构来进行列出）。按下 <code>ctrl + c</code> 键即可停止遍历。</p>
<h3 id="绝对路径"><a href="#绝对路径" class="headerlink" title="绝对路径"></a>绝对路径</h3><p>使用 pwd 命令可以获取当前目录的绝对路径（指一个以根目录 / 为起点的完整路径）</p>
<p><strong>提示：</strong>如果忘记了目录名、文件名或命令，可使用 Tab 键自动补全，还可避免输入错误；连续按两次 Tab 可以显示全部候选结果。</p>
<h3 id="新建目录"><a href="#新建目录" class="headerlink" title="新建目录"></a>新建目录</h3><p>使用 <code>mkdir</code> 命令可创建目录，<code>mkdir mycode</code> 的意思就是新建一个名为 <code>mycode</code> 的目录。</p>
<p>还可以在 <code>mkdir</code> 后加入 <code>-p</code> 参数，一次性创建多级目录，如：</p>
<p><img src="/2020/07/05/Linux%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.png" alt="img"></p>
<h3 id="Linux-文件操作"><a href="#Linux-文件操作" class="headerlink" title="Linux 文件操作"></a>Linux 文件操作</h3><p>使用 <code>touch</code> 命令可以新建文件，比如我想在新建一个名为 “hello” 的文件，可输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:~&#x2F; $ **touch hello**</span><br></pre></td></tr></table></figure>

<p>（该命令不会覆盖已有同名文件）</p>
<p>使用 <code>cp</code> 命令（Copy）复制文件到指定目录下，比如要把 <code>hello</code> 文件复制到 <code>one/two</code> 这个目录下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:~&#x2F; $ **cp hello one&#x2F;two&#x2F;**</span><br></pre></td></tr></table></figure>

<p>如果要复制目录，需要在 <code>cp</code> 后加上 <code>-r</code> ，然后接上 <code>目录名 目标目录名</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:~&#x2F; $ mkdir test</span><br><span class="line">shiyanlou:~&#x2F; $ cp -r test one&#x2F;two </span><br><span class="line">shiyanlou:~&#x2F; $ tree one </span><br><span class="line">one</span><br><span class="line"> └── two</span><br><span class="line">      ├── hello</span><br><span class="line">      ├── test</span><br><span class="line">      └── three</span><br><span class="line"></span><br><span class="line">3 directories, 1 file</span><br></pre></td></tr></table></figure>



<p>下面是 Linux 中对文件的常用操作，包含新建、复制、删除等。</p>
<h4 id="新建空白文件"><a href="#新建空白文件" class="headerlink" title="新建空白文件"></a>新建空白文件</h4><p>使用 <code>touch</code> 命令可以新建文件，比如我想在新建一个名为 “hello” 的文件，可输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:~&#x2F; $ touch hello</span><br></pre></td></tr></table></figure>

<p>“hello” 文件就被创建出来了，用 <code>ls</code> 命令查看一下：</p>
<p><img src="/2020/07/05/Linux%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Linux%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5C2.png" alt></p>
<p>（该命令不会覆盖已有同名文件）</p>
<h4 id="编辑文件"><a href="#编辑文件" class="headerlink" title="编辑文件"></a>编辑文件</h4><p>使用vim命令进该文件，点击insert可进行编辑。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:~&#x2F; $ vim hello</span><br></pre></td></tr></table></figure>

<p>编辑完成后按退出esc键，再输入 :wq  即可实现保存且退出。</p>
<h4 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h4><p>使用 <code>cp</code> 命令（Copy）复制文件到指定目录下，比如要把 <code>hello</code> 文件复制到 <code>one/two</code> 这个目录下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:~&#x2F; $ cp hello one&#x2F;two&#x2F;  </span><br><span class="line">shiyanlou:~&#x2F; $ tree one   </span><br><span class="line">one</span><br><span class="line"> └── two</span><br><span class="line">      ├── hello</span><br><span class="line">      └── three</span><br><span class="line"></span><br><span class="line">2 directories, 1 file</span><br><span class="line">shiyanlou:~&#x2F; $</span><br></pre></td></tr></table></figure>

<p>如果要复制目录，需要在 <code>cp</code> 后加上 <code>-r</code> ，然后接上 <code>目录名 目标目录名</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:~&#x2F; $ mkdir test</span><br><span class="line">shiyanlou:~&#x2F; $ cp -r test one&#x2F;two</span><br></pre></td></tr></table></figure>

<h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><p>使用 <code>rm</code> 命令删除文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:~&#x2F; $ ls</span><br><span class="line">Code  Desktop  hello  one</span><br><span class="line">shiyanlou:~&#x2F; $ rm hello</span><br><span class="line">shiyanlou:~&#x2F; $ ls</span><br><span class="line">Code  Desktop  one</span><br></pre></td></tr></table></figure>

<p>删除目录要加上 <code>-r</code> 选项，类似 <code>cp -r</code> 拷贝目录，会删除目录和目录下的所有内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:~&#x2F; $ mkdir test</span><br><span class="line">shiyanlou:~&#x2F; $ ls</span><br><span class="line">Code  Desktop  one  test</span><br><span class="line">shiyanlou:~&#x2F; $ rm -r test</span><br><span class="line">shiyanlou:~&#x2F; $ ls</span><br><span class="line">Code  Desktop  one</span><br></pre></td></tr></table></figure>

<h4 id="移动文件-目录与重命名"><a href="#移动文件-目录与重命名" class="headerlink" title="移动文件 / 目录与重命名"></a>移动文件 / 目录与重命名</h4><p>使用 <code>mv</code> 命令可以移动文件或目录。</p>
<p>首先，我们进入到 <code>/home/shiyanlou</code> 目录，使用 <code>touch</code> 创建空文件 <code>test1</code>：</p>
<p>然后，我们用mkdir创建一个新目录 <code>dir1</code></p>
<p>使用 <code>mv</code> 命令 将 <code>test1</code> 移动到 <code>dir1</code> 目录，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:~&#x2F; $ mv test1 dir1</span><br><span class="line">shiyanlou:~&#x2F; $ cd dir1</span><br><span class="line">shiyanlou:dir1&#x2F; $ ls</span><br><span class="line">test1</span><br></pre></td></tr></table></figure>

<p><code>mv</code> 命令还可以用来重命名，如 <code>mv test1 test2</code>， 会把 <code>test1</code> 重命名为 <code>test2</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:dir1&#x2F; $ ls</span><br><span class="line">test1</span><br><span class="line">shiyanlou:dir1&#x2F; $ mv test1 test2</span><br><span class="line">shiyanlou:dir1&#x2F; $ ls</span><br><span class="line">test2</span><br></pre></td></tr></table></figure>

<h4 id="cat打印"><a href="#cat打印" class="headerlink" title="cat打印"></a>cat打印</h4><p>使用 <code>cat</code> 命令，可以将文件中的内容打印到屏幕上，使用方法是 <code>cat 文件路径</code>。</p>
<p>现在还没有文件，我们先从其他地方复制过来一个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:dir1&#x2F; $ cp &#x2F;etc&#x2F;passwd passwd</span><br><span class="line">shiyanlou:dir1&#x2F; $ ls</span><br><span class="line">passwd  test2</span><br></pre></td></tr></table></figure>

<p>这样就把 <code>passwd</code> 文件从 <code>/etc</code> 目录拷贝到了当前目录中，然后我们用 <code>cat passwd</code> 显示文件中的内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:dir1&#x2F; $ cat passwd</span><br><span class="line">root:x:0:0:root:&#x2F;root:&#x2F;bin&#x2F;bash</span><br><span class="line">daemon:x:1:1:daemon:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;sbin&#x2F;nologin</span><br><span class="line">bin:x:2:2:bin:&#x2F;bin:&#x2F;usr&#x2F;sbin&#x2F;nologin</span><br><span class="line">sys:x:3:3:sys:&#x2F;dev:&#x2F;usr&#x2F;sbin&#x2F;nologin</span><br><span class="line">sync:x:4:65534:sync:&#x2F;bin:&#x2F;bin&#x2F;sync</span><br><span class="line">games:x:5:60:games:&#x2F;usr&#x2F;games:&#x2F;usr&#x2F;sbin&#x2F;nologin</span><br></pre></td></tr></table></figure>

<p>使用 <code>cat -n</code> 可以带行号地打印文件内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">shiyanlou:dir1&#x2F; $ cat -n passwd</span><br><span class="line">     1    root:x:0:0:root:&#x2F;root:&#x2F;bin&#x2F;bash</span><br><span class="line">     2    daemon:x:1:1:daemon:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;sbin&#x2F;nologin</span><br><span class="line">     3    bin:x:2:2:bin:&#x2F;bin:&#x2F;usr&#x2F;sbin&#x2F;nologin</span><br><span class="line">     4    sys:x:3:3:sys:&#x2F;dev:&#x2F;usr&#x2F;sbin&#x2F;nologin</span><br><span class="line">     5    sync:x:4:65534:sync:&#x2F;bin:&#x2F;bin&#x2F;sync</span><br><span class="line">     6    games:x:5:60:games:&#x2F;usr&#x2F;games:&#x2F;usr&#x2F;sbin&#x2F;nologin</span><br><span class="line">     7    man:x:6:12:man:&#x2F;var&#x2F;cache&#x2F;man:&#x2F;usr&#x2F;sbin&#x2F;nologin</span><br></pre></td></tr></table></figure>

<h4 id="帮助命令man"><a href="#帮助命令man" class="headerlink" title="帮助命令man"></a>帮助命令man</h4><p>在 Linux 环境中，如果你遇到困难，可以使用 <code>man</code> 命令，它是 <code>Manual pages</code> 的缩写。例如输入 <code>man cat</code> ，可以获取 cat 命令的详细的帮助文件。进入到 man 的页面后，按 <code>q</code> 可以退出 man。</p>
<p>有些命令可以使用 <code>--help</code> 选项查看帮助文档。如输入man –help </p>
<p>ubantu账号：yi-wang </p>
<p>密码：12345689</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/05/Linux%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-id="ckn1t0idc0000rgvtgkxf6966" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Matplotlib学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2020-07-05T15:00:10.000Z" itemprop="datePublished">2020-07-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%93/">数据分析库</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Matplotlib学习笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>echarts.baidu.com(这个网站提供很多前端框架，当我们把数据处理好之后，可以直接把这些数据放到框架中得到很多炫酷的展示,模仿MATLAB 构建，主要做数据可视化图表。</p>
<h2 id="折线图"><a href="#折线图" class="headerlink" title="折线图"></a>折线图</h2><h3 id="实例1（10到12点的温度变化图）"><a href="#实例1（10到12点的温度变化图）" class="headerlink" title="实例1（10到12点的温度变化图）"></a>实例1（10到12点的温度变化图）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt <span class="comment">#plt用来绘折线图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#以下代码为绘制一天内每隔2小时显示当前的气温图</span></span><br><span class="line"></span><br><span class="line">x=range(<span class="number">2</span>,<span class="number">26</span>,<span class="number">2</span>)</span><br><span class="line">y=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">2</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">8</span>),dpi=<span class="number">80</span>) <span class="comment">#设置图片大小，如果没有这行，则为默认大小，dpi指像素</span></span><br><span class="line">plt.plot(x,y) <span class="comment">#绘图</span></span><br><span class="line"><span class="comment">#plt.axis([0,5,0,20]) 设置x轴y轴范围 会自己生成刻度</span></span><br><span class="line">plt.xticks(x) <span class="string">'''设置x轴刻度，为刚才x的格式（从2到24，每间隔2），若不设置则为默认的样式。也可按自己的要求设置如plt.xticks(range(2,25)) '''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#若要实现x轴每隔0.5取一个呢？则应该这样：i=[i/2 for i in range(4,49)],plt.xticks(i);另一种方式可以用while循环。</span></span><br><span class="line">plt.yticks(range(min(y),max(y)+<span class="number">1</span>)) <span class="comment">#设置y轴的刻度范围</span></span><br><span class="line"><span class="string">''' plt.savefig('C:/Users/Administrator/t1.png') #保存图片到指定路径</span></span><br><span class="line"><span class="string">plt.savefig('C:/Users/Administrator/sig_size.png') #指以矢量图的形式保存图片到指定路径，矢量图放大不会有锯齿 ??此行代码有问题  '''</span></span><br><span class="line">plt.show() <span class="comment">#展示图形,jupyter notebook中只用plt.plot()就可显示出图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#实现10-12点显示这120分钟的温度</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">b=[random.randint(<span class="number">20</span>,<span class="number">35</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">120</span>)] <span class="comment">#产生从20到35（包括20和35）的整随机数</span></span><br><span class="line">a=range(<span class="number">120</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">8</span>),dpi=<span class="number">80</span>)</span><br><span class="line">plt.plot(a,b)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#如下图所示</span></span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.png" alt></p>
<p>#但我们希望x轴的刻度能以时钟的形式展示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#调整x轴的刻度，这里先让x轴以字符串形式标记</span></span><br><span class="line">_x=a</span><br><span class="line">_xtick_labels = [<span class="string">'hello,&#123;&#125;'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> _x]</span><br><span class="line"><span class="comment">#等价于上一句  _xtick_labels = ['hello,%s'%i for i in _x]</span></span><br><span class="line">plt.xticks(_x,_xtick_labels) <span class="comment">#将传入的x（数字）和字符串一一对应。实现x轴坐标以字符串的方式显示</span></span><br><span class="line">plt.plot(_x,b) </span><br><span class="line"><span class="comment">#如下图所示，x轴的坐标点名会变为hello，0  hello,1 hello,2 ...hello,119,但太过密集，我们用以下方式调</span></span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">_x&#x3D;list(a)[::10] #将a转换成列表型，让之前x轴上的点每隔10个取一个，达到放大的目的。</span><br><span class="line">_xtick_labels &#x3D; [&#39;hello,&#123;&#125;&#39;.format(i) for i in _x]</span><br><span class="line">plt.xticks(_x,_xtick_labels) #将传入的x（数字）和字符串一一对应。实现x轴坐标以字符串的方式显示</span><br><span class="line">plt.plot(_x,b) </span><br><span class="line">#如下图所示，x轴的坐标点名会变为hello,0  hello,10 hello,20 ...hello,110,共12个点</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.png" alt></p>
<p>#现在我们想把横坐标以小时分钟的形式表示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">8</span>),dpi=<span class="number">80</span>)</span><br><span class="line">plt.plot(a,y)</span><br><span class="line">_x=list(a)[::<span class="number">4</span>]</span><br><span class="line">_xtick_labels = [<span class="string">'10点&#123;&#125;分'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">60</span>)]</span><br><span class="line">_xtick_labels += [<span class="string">'11点&#123;&#125;分'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">60</span>)]</span><br><span class="line">plt.xticks(_x,_xtick_labels[::<span class="number">4</span>])</span><br><span class="line">plt.savefig(<span class="string">'C:/Users/Administrator/Desktop/计算机学习note/Matplotlib学习笔记/4.png'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4.png" alt></p>
<p>现在 我们将x轴的表达式旋转90度，使得能够看清楚。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接在刚刚设置x轴的代码中添加rotation</span></span><br><span class="line"></span><br><span class="line">plt.xticks(_x,_xtick_labels[::<span class="number">4</span>],rotation=<span class="number">90</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#对我们的图添加描述信息，图的标题，x，y轴的标签。在刚才的代码中加入三行：</span><br><span class="line">plt.xticks(_x,_xtick_labels[::4],rotation&#x3D;90)</span><br><span class="line">plt.xlabel(&#39;时间&#39;)</span><br><span class="line">plt.ylabel(&#39;温度，摄氏度&#39;)</span><br><span class="line">plt.title(&#39;10点到12点每分钟温度的变化情况&#39;)</span><br><span class="line">#plt.savefig(&#39;C:&#x2F;Users&#x2F;Administrator&#x2F;Desktop&#x2F;计算机学习note&#x2F;Matplotlib学习笔记&#x2F;6.png&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.grid(alpha=<span class="number">0.1</span>) <span class="string">'''增加网格，alpha指网格透明度，值从0-1，以改变x,y轴的疏密来改变网格的疏密,还可传入linestyle等'''</span></span><br></pre></td></tr></table></figure>

<h3 id="实例2（从11到30岁自己和同桌的恋爱次数变化图）"><a href="#实例2（从11到30岁自己和同桌的恋爱次数变化图）" class="headerlink" title="实例2（从11到30岁自己和同桌的恋爱次数变化图）"></a>实例2（从11到30岁自己和同桌的恋爱次数变化图）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">x=range(<span class="number">11</span>,<span class="number">31</span>)</span><br><span class="line">y_1=[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">y_2=[<span class="number">1</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">8</span>),dpi=<span class="number">80</span>)</span><br><span class="line">plt.grid(alpha=<span class="number">0.1</span>)<span class="comment">#增加网格，alpha指网格透明度</span></span><br><span class="line">plt.xlabel(<span class="string">'年龄'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'恋爱次数'</span>)</span><br><span class="line">plt.xticks(x)</span><br><span class="line"><span class="comment">#可以直接将两个甚至多个y放在plt.plot()中</span></span><br><span class="line"><span class="comment">#plt.plot(x,y_1,x,y_2)</span></span><br><span class="line">plt.plot(x,y_1,label=<span class="string">'自己'</span>)<span class="comment">#在这里还可以设置线条的颜色；如用linestyle==‘--’设置线条风格，用linewidth=5设置线条粗细，用alpha=0.5设置透明度</span></span><br><span class="line">plt.plot(x,y_2,label=<span class="string">'同桌'</span>,color=<span class="string">'pink'</span>)</span><br><span class="line">plt.legend() <span class="comment">#前两行中的label和legend联合使用，添加图例，实现图中右上的效果。可传入参数log改变位置</span></span><br><span class="line"><span class="comment">#plt.savefig('C:/Users/Administrator\Desktop/计算机学习note/Matplotlib学习笔记/7.png')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/7.png" alt></p>
<h3 id="思维导图（折线图）"><a href="#思维导图（折线图）" class="headerlink" title="思维导图（折线图）"></a>思维导图（折线图）</h3><p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/8.jpg" alt></p>
<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/9.jpg" alt></p>
<h2 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h2><p>技术要点：plt.scatter(x,y)</p>
<h3 id="实例1（3月与10月每天的气温散点图）"><a href="#实例1（3月与10月每天的气温散点图）" class="headerlink" title="实例1（3月与10月每天的气温散点图）"></a>实例1（3月与10月每天的气温散点图）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">x_3=range(<span class="number">1</span>,<span class="number">32</span>)</span><br><span class="line">x_10=range(<span class="number">51</span>,<span class="number">82</span>)</span><br><span class="line">y_3=[<span class="number">11</span>,<span class="number">17</span>,<span class="number">16</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">12</span>,<span class="number">15</span>,<span class="number">14</span>,<span class="number">17</span>,<span class="number">18</span>,<span class="number">21</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">20</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">15</span>,<span class="number">15</span>,<span class="number">19</span>,<span class="number">21</span>,<span class="number">22</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">23</span>,<span class="number">24</span>,<span class="number">25</span>]</span><br><span class="line">y_10=[<span class="number">26</span>,<span class="number">26</span>,<span class="number">28</span>,<span class="number">19</span>,<span class="number">21</span>,<span class="number">17</span>,<span class="number">16</span>,<span class="number">19</span>,<span class="number">18</span>,<span class="number">20</span>,<span class="number">20</span>,<span class="number">19</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">17</span>,<span class="number">20</span>,<span class="number">21</span>,<span class="number">20</span>,<span class="number">22</span>,<span class="number">15</span>,<span class="number">11</span>,<span class="number">15</span>,<span class="number">5</span>,<span class="number">13</span>,<span class="number">17</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">10</span>,<span class="number">9</span>]</span><br><span class="line"><span class="comment">#设置图片大小</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">8</span>),dpi=<span class="number">80</span>)</span><br><span class="line"><span class="comment">#设置网格</span></span><br><span class="line">plt.grid(alpha=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment">#生成散点图</span></span><br><span class="line">plt.scatter(x_3,y_3,label=<span class="string">'三月份'</span>)</span><br><span class="line">plt.scatter(x_10,y_10,label=<span class="string">'五月份'</span>)</span><br><span class="line"><span class="comment">#调整x轴的刻度</span></span><br><span class="line">_x=list(x_3)+list(x_10)</span><br><span class="line">_xtick_labels=[<span class="string">'3月&#123;&#125;号'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> x_3]</span><br><span class="line">_xtick_labels+=[<span class="string">'10月&#123;&#125;号'</span>.format(i<span class="number">-50</span>) <span class="keyword">for</span> i <span class="keyword">in</span> x_10]</span><br><span class="line">plt.xticks(_x,_xtick_labels,rotation=<span class="number">45</span>)</span><br><span class="line"><span class="comment">#调整y轴的刻度</span></span><br><span class="line">_y=range(min(y_3),max(y_10)+<span class="number">1</span>)</span><br><span class="line">plt.yticks(_y)</span><br><span class="line"><span class="comment">#添加图例</span></span><br><span class="line">plt.legend()</span><br><span class="line"><span class="comment">#添加描述信息</span></span><br><span class="line">plt.xlabel(<span class="string">'日期'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'温度（摄氏度）'</span>)</span><br><span class="line"><span class="comment">#保存此散点图</span></span><br><span class="line">plt.savefig(<span class="string">'C:\\Users\\Administrator\\Desktop\\计算机学习note\\Matplotlib\\10.png'</span>)</span><br><span class="line"><span class="comment">#展示</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.png" alt></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>除了在绘制散点图时我们用plt.scatter()进行绘制，其他方法与折线图没有任何区别。</p>
<h2 id="柱状图"><a href="#柱状图" class="headerlink" title="柱状图"></a>柱状图</h2><h3 id="实例1（某日的电影票房）"><a href="#实例1（某日的电影票房）" class="headerlink" title="实例1（某日的电影票房）"></a>实例1（某日的电影票房）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">y=[<span class="number">2.91</span>,<span class="number">7.88</span>,<span class="number">11.97</span>,<span class="number">11.72</span>,<span class="number">0.1</span>,<span class="number">3.5</span>]</span><br><span class="line">x=[<span class="string">'雷霆沙赞！'</span>,<span class="string">'反贪风暴4'</span>,<span class="string">'误杀'</span>,<span class="string">'叶问4'</span>,<span class="string">'士兵顺溜：兵王争锋'</span>,<span class="string">'老师·好'</span>]</span><br><span class="line">plt.bar(range(len(x)),y,width=<span class="number">0.3</span>)<span class="comment">#width表示柱子的宽度</span></span><br><span class="line">plt.xticks(range(len(x)),x,rotation=<span class="number">45</span>)</span><br><span class="line">plt.ylabel(<span class="string">'票房（亿）'</span>)</span><br><span class="line"><span class="comment">#plt.savefig('C:\\Users\\Administrator\\Desktop\\计算机学习note\\Matplotlib\\11.png')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/11.png" alt></p>
<p>此时我们发现x轴坐标显示不完全，我们可以用plt.barh()将柱子横过来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">y=[<span class="number">2.91</span>,<span class="number">7.88</span>,<span class="number">11.97</span>,<span class="number">11.72</span>,<span class="number">0.1</span>,<span class="number">3.5</span>]</span><br><span class="line">x=[<span class="string">'雷霆沙赞！'</span>,<span class="string">'反贪风暴4'</span>,<span class="string">'误杀'</span>,<span class="string">'叶问4'</span>,<span class="string">'士兵顺溜：兵王争锋'</span>,<span class="string">'老师·好'</span>]</span><br><span class="line">plt.barh(range(len(x)),y,height=<span class="number">0.3</span>,color=<span class="string">'orange'</span>)<span class="comment">#由于柱子是横着的，要用height表示柱子的宽度</span></span><br><span class="line">plt.yticks(range(len(x)),x)<span class="comment">#之前的x变成y；</span></span><br><span class="line">plt.xlabel(<span class="string">'票房（亿）'</span>)<span class="comment">#之前的x变成y</span></span><br><span class="line"><span class="comment">#plt.savefig('C:\\Users\\Administrator\\Desktop\\计算机学习note\\Matplotlib\\12.png')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/12.png" alt></p>
<h3 id="实例2–绘制多次条形图（某三天的4部电影票房）"><a href="#实例2–绘制多次条形图（某三天的4部电影票房）" class="headerlink" title="实例2–绘制多次条形图（某三天的4部电影票房）"></a>实例2–绘制多次条形图（某三天的4部电影票房）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>),dpi=<span class="number">80</span>)</span><br><span class="line">y_14=[<span class="number">2358</span>,<span class="number">399</span>,<span class="number">2358</span>,<span class="number">362</span>]</span><br><span class="line">y_15=[<span class="number">12357</span>,<span class="number">156</span>,<span class="number">2045</span>,<span class="number">168</span>]</span><br><span class="line">y_16=[<span class="number">15746</span>,<span class="number">312</span>,<span class="number">4497</span>,<span class="number">319</span>]</span><br><span class="line">x=[<span class="string">'猩球崛起3：终极之战'</span>,<span class="string">'敦刻尔克'</span>,<span class="string">'蜘蛛侠：英雄归来'</span>,<span class="string">'战狼2'</span>]</span><br><span class="line"><span class="comment">#我们将15号的x轴坐标的后移一个柱子宽度0.2，将16号的x轴坐标的后移2个柱子宽度0.2，就可以将这三天的票房信息都放在一张表里</span></span><br><span class="line">bar_width=<span class="number">0.2</span></span><br><span class="line">x_14=list(range(len(x)))</span><br><span class="line">x_15=[i+bar_width <span class="keyword">for</span> i <span class="keyword">in</span> x_14]</span><br><span class="line">x_16=[i+bar_width*<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> x_14]</span><br><span class="line">plt.bar(range(len(x)),y_14,width=bar_width,label=<span class="string">'14号'</span>)</span><br><span class="line">plt.bar(x_15,y_15,width=bar_width,label=<span class="string">'15号'</span>)</span><br><span class="line">plt.bar(x_16,y_16,width=bar_width,label=<span class="string">'16号'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xticks(x_15,x)<span class="comment">#我们将电影名放到三个柱子中间，所以是x_15,更美观</span></span><br><span class="line">plt.ylabel(<span class="string">'票房（万）'</span>)</span><br><span class="line">plt.savefig(<span class="string">'C:\\Users\\Administrator\\Desktop\\计算机学习note\\Matplotlib\\13.png'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.png" alt></p>
<h2 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h2><h3 id="实例1（根据电影时长统计电影频数）"><a href="#实例1（根据电影时长统计电影频数）" class="headerlink" title="实例1（根据电影时长统计电影频数）"></a>实例1（根据电影时长统计电影频数）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#a为250部电影的时长集合</span></span><br><span class="line">a=[<span class="number">131</span>,  <span class="number">98</span>, <span class="number">125</span>, <span class="number">131</span>, <span class="number">124</span>, <span class="number">139</span>, <span class="number">131</span>, <span class="number">117</span>, <span class="number">128</span>, <span class="number">108</span>, <span class="number">135</span>, <span class="number">138</span>, <span class="number">131</span>, <span class="number">102</span>, <span class="number">107</span>, <span class="number">114</span>, <span class="number">119</span>, <span class="number">128</span>, <span class="number">121</span>, <span class="number">142</span>, <span class="number">127</span>, <span class="number">130</span>, <span class="number">124</span>, <span class="number">101</span>, <span class="number">110</span>, <span class="number">116</span>, <span class="number">117</span>, <span class="number">110</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">115</span>,  <span class="number">99</span>, <span class="number">136</span>, <span class="number">126</span>, <span class="number">134</span>,  <span class="number">95</span>, <span class="number">138</span>, <span class="number">117</span>, <span class="number">111</span>,<span class="number">78</span>, <span class="number">132</span>, <span class="number">124</span>, <span class="number">113</span>, <span class="number">150</span>, <span class="number">110</span>, <span class="number">117</span>,  <span class="number">86</span>,  <span class="number">95</span>, <span class="number">144</span>, <span class="number">105</span>, <span class="number">126</span>, <span class="number">130</span>,<span class="number">126</span>, <span class="number">130</span>, <span class="number">126</span>, <span class="number">116</span>, <span class="number">123</span>, <span class="number">106</span>, <span class="number">112</span>, <span class="number">138</span>, <span class="number">123</span>,  <span class="number">86</span>, <span class="number">101</span>,  <span class="number">99</span>, <span class="number">136</span>,<span class="number">123</span>, <span class="number">117</span>, <span class="number">119</span>, <span class="number">105</span>, <span class="number">137</span>, <span class="number">123</span>, <span class="number">128</span>, <span class="number">125</span>, <span class="number">104</span>, <span class="number">109</span>, <span class="number">134</span>, <span class="number">125</span>, <span class="number">127</span>,<span class="number">105</span>, <span class="number">120</span>, <span class="number">107</span>, <span class="number">129</span>, <span class="number">116</span>, <span class="number">108</span>, <span class="number">132</span>, <span class="number">103</span>, <span class="number">136</span>, <span class="number">118</span>, <span class="number">102</span>, <span class="number">120</span>, <span class="number">114</span>,<span class="number">105</span>, <span class="number">115</span>, <span class="number">132</span>, <span class="number">145</span>, <span class="number">119</span>, <span class="number">121</span>, <span class="number">112</span>, <span class="number">139</span>, <span class="number">125</span>, <span class="number">138</span>, <span class="number">109</span>, <span class="number">132</span>, <span class="number">134</span>,<span class="number">156</span>, <span class="number">106</span>, <span class="number">117</span>, <span class="number">127</span>, <span class="number">144</span>, <span class="number">139</span>, <span class="number">139</span>, <span class="number">119</span>, <span class="number">140</span>,  <span class="number">83</span>, <span class="number">110</span>, <span class="number">102</span>,<span class="number">123</span>,<span class="number">107</span>, <span class="number">143</span>, <span class="number">115</span>, <span class="number">136</span>, <span class="number">118</span>, <span class="number">139</span>, <span class="number">123</span>, <span class="number">112</span>, <span class="number">118</span>, <span class="number">125</span>, <span class="number">109</span>, <span class="number">119</span>, <span class="number">133</span>,<span class="number">112</span>, <span class="number">114</span>, <span class="number">122</span>, <span class="number">109</span>, <span class="number">106</span>, <span class="number">123</span>, <span class="number">116</span>, <span class="number">131</span>, <span class="number">127</span>, <span class="number">115</span>, <span class="number">118</span>, <span class="number">112</span>, <span class="number">135</span>,<span class="number">115</span>, <span class="number">146</span>, <span class="number">137</span>, <span class="number">116</span>, <span class="number">103</span>, <span class="number">144</span>,  <span class="number">83</span>, <span class="number">123</span>, <span class="number">111</span>, <span class="number">110</span>, <span class="number">111</span>, <span class="number">100</span>, <span class="number">154</span>,<span class="number">136</span>, <span class="number">100</span>, <span class="number">118</span>, <span class="number">119</span>, <span class="number">133</span>, <span class="number">134</span>, <span class="number">106</span>, <span class="number">129</span>, <span class="number">126</span>, <span class="number">110</span>, <span class="number">111</span>, <span class="number">109</span>, <span class="number">141</span>,<span class="number">120</span>, <span class="number">117</span>, <span class="number">106</span>, <span class="number">149</span>, <span class="number">122</span>, <span class="number">122</span>, <span class="number">110</span>, <span class="number">118</span>, <span class="number">127</span>, <span class="number">121</span>, <span class="number">114</span>, <span class="number">125</span>, <span class="number">126</span>,<span class="number">114</span>, <span class="number">140</span>, <span class="number">103</span>, <span class="number">130</span>, <span class="number">141</span>, <span class="number">117</span>, <span class="number">106</span>, <span class="number">114</span>, <span class="number">121</span>, <span class="number">114</span>, <span class="number">133</span>, <span class="number">137</span>,  <span class="number">92</span>,<span class="number">121</span>, <span class="number">112</span>, <span class="number">146</span>,  <span class="number">97</span>, <span class="number">137</span>, <span class="number">105</span>,  <span class="number">98</span>, <span class="number">117</span>, <span class="number">112</span>,  <span class="number">81</span>,  <span class="number">97</span>, <span class="number">139</span>, <span class="number">113</span>,<span class="number">134</span>, <span class="number">106</span>, <span class="number">144</span>, <span class="number">110</span>, <span class="number">137</span>, <span class="number">137</span>, <span class="number">111</span>, <span class="number">104</span>, <span class="number">117</span>, <span class="number">100</span>, <span class="number">111</span>, <span class="number">101</span>, <span class="number">110</span>,<span class="number">105</span>, <span class="number">129</span>, <span class="number">137</span>, <span class="number">112</span>, <span class="number">120</span>, <span class="number">113</span>, <span class="number">133</span>, <span class="number">112</span>,  <span class="number">83</span>,  <span class="number">94</span>, <span class="number">146</span>, <span class="number">133</span>, <span class="number">101</span>,<span class="number">131</span>, <span class="number">116</span>, <span class="number">111</span>,  <span class="number">84</span>, <span class="number">137</span>, <span class="number">115</span>, <span class="number">122</span>, <span class="number">106</span>, <span class="number">144</span>, <span class="number">109</span>, <span class="number">123</span>, <span class="number">116</span>, <span class="number">111</span>,<span class="number">111</span>, <span class="number">133</span>, <span class="number">150</span>]</span><br><span class="line">d=max(a)-min(a)</span><br><span class="line">print(d)<span class="comment">#我们算出极差是78，能被78整除的数有2，3...我们这里取3作为组距，并把x轴的刻度设置为3的间距，与组距一致，方便观察</span></span><br><span class="line">num_bins=int(d/<span class="number">3</span>)<span class="comment">#组数</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">8</span>),dpi=<span class="number">120</span>)</span><br><span class="line">plt.hist(a,num_bins)<span class="comment">#生成直方图，传入组数</span></span><br><span class="line">plt.xticks(range(min(a),max(a)+<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">plt.ylabel(<span class="string">'电影个数）'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'电影时长（分钟）'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.savefig(<span class="string">'C:\\Users\\Administrator\\Desktop\\计算机学习note\\Matplotlib\\14.png'</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/14.png" alt></p>
<p>如果我们想要不同组别的频率而不是频数，直接修改上述代码中的一行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.hist(a,num_bins,density&#x3D;True)#传入density&#x3D;True</span><br></pre></td></tr></table></figure>

<p>同时把y的标签改成电影频率：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.ylabel(&#39;电影频率&#39;)</span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/15.png" alt></p>
<p>一般来说plt.hist()的使用对象是没有进行统计过的数据。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-id="ckn1t0ido0005rgvtdmoeffpi" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Matplotlib/" rel="tag">Matplotlib</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%93/">数据分析库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/" rel="tag">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/" rel="tag">Matplotlib</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mini-Batch%E4%B8%8B%E9%99%8D/" rel="tag">Mini-Batch下降</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Numpy/" rel="tag">Numpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/" rel="tag">Pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Re/" rel="tag">Re</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Request/" rel="tag">Request</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scrapy/" rel="tag">Scrapy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bs4/" rel="tag">bs4</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" rel="tag">决策树与随机森林</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB/" rel="tag">动态爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/" rel="tag">异常检测算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" rel="tag">排序算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%97%E6%B3%95/" rel="tag">推荐系统算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" rel="tag">支持向量机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">无监督学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">监督学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95K-means/" rel="tag">聚类算法K-means</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95PCA/" rel="tag">降维算法PCA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" rel="tag">随机梯度下降</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB/" rel="tag">静态爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Mini-Batch%E4%B8%8B%E9%99%8D/" style="font-size: 10px;">Mini-Batch下降</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Python/" style="font-size: 20px;">Python</a> <a href="/tags/Re/" style="font-size: 10px;">Re</a> <a href="/tags/Request/" style="font-size: 10px;">Request</a> <a href="/tags/Scrapy/" style="font-size: 16.67px;">Scrapy</a> <a href="/tags/bs4/" style="font-size: 10px;">bs4</a> <a href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" style="font-size: 10px;">决策树与随机森林</a> <a href="/tags/%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB/" style="font-size: 10px;">动态爬虫</a> <a href="/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/" style="font-size: 10px;">异常检测算法</a> <a href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" style="font-size: 10px;">排序算法</a> <a href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%97%E6%B3%95/" style="font-size: 10px;">推荐系统算法</a> <a href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" style="font-size: 10px;">支持向量机</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 10px;">数据库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">无监督学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 13.33px;">机器学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 10px;">爬虫</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">监督学习</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">神经网络</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95K-means/" style="font-size: 10px;">聚类算法K-means</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">逻辑回归</a> <a href="/tags/%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95PCA/" style="font-size: 10px;">降维算法PCA</a> <a href="/tags/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 10px;">随机梯度下降</a> <a href="/tags/%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB/" style="font-size: 10px;">静态爬虫</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/11/03/kaggle%E4%B9%8B%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E9%A1%B9%E7%9B%AE/">kaggle之房价预测项目</a>
          </li>
        
          <li>
            <a href="/2020/09/27/YOLOv5-%E5%8F%A3%E7%BD%A9%E4%B8%8E%E5%B8%BD%E5%AD%90%E8%AF%86%E5%88%AB/">YOLOv5--口罩与帽子识别</a>
          </li>
        
          <li>
            <a href="/2020/09/16/Kaggle%E4%B9%8B%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB%E9%A1%B9%E7%9B%AE(%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)/">Kaggle之猫狗识别项目(神经网络)</a>
          </li>
        
          <li>
            <a href="/2020/09/10/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">PyTorch学习笔记</a>
          </li>
        
          <li>
            <a href="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/">决策树与随机森林</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>