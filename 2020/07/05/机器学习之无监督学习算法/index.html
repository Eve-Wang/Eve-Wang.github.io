<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>机器学习之无监督学习算法 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本文是一个关于常用的无监督学习算法的学习笔记，以及与其相关的一些知识点。 无监督学习指我们只知道benwe有一堆数据（x），但并不了解什么是正确答案（输出y）。 无监督学习算法的作用如聚类算法可以判定这个数据集包含两个簇  例子：谷歌新闻收集某个事件所有报道，就是搜索成千上万个新闻，把与这个事件有关的新闻都分为一簇。 聚类算法–K-means步骤：1.随机生成两个点–聚类中心 2.簇分配 遍历每个">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习之无监督学习算法">
<meta property="og:url" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="本文是一个关于常用的无监督学习算法的学习笔记，以及与其相关的一些知识点。 无监督学习指我们只知道benwe有一堆数据（x），但并不了解什么是正确答案（输出y）。 无监督学习算法的作用如聚类算法可以判定这个数据集包含两个簇  例子：谷歌新闻收集某个事件所有报道，就是搜索成千上万个新闻，把与这个事件有关的新闻都分为一簇。 聚类算法–K-means步骤：1.随机生成两个点–聚类中心 2.簇分配 遍历每个">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/4.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/88.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/89.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/90.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/91.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/92.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/93.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/94.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/95.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/96.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/97.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/98.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/99.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/100.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/101.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/104.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/102.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/103.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/105.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/106.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/110.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/109.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/107.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/108.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/111.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/112.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/113.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/114.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/115.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/116.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/117.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/118.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/119.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/120.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/121.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/122.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/123.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/124.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/125.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/126.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/127.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/128.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/129.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/130.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/131.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/132.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/133.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/134.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/135.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/136.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/137.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/138.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/139.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/140.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/141.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/142.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/143.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/144.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/145.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/146.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/153.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/147.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/148.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/151.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/152.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/149.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/150.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/154.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/155.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/156.png">
<meta property="article:published_time" content="2020-07-05T11:30:52.000Z">
<meta property="article:modified_time" content="2020-07-30T07:08:20.612Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="无监督学习">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="聚类算法K-means">
<meta property="article:tag" content="降维算法PCA">
<meta property="article:tag" content="异常检测算法">
<meta property="article:tag" content="推荐系统算法">
<meta property="article:tag" content="随机梯度下降">
<meta property="article:tag" content="Mini-Batch下降">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/4.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-机器学习之无监督学习算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" class="article-date">
  <time datetime="2020-07-05T11:30:52.000Z" itemprop="datePublished">2020-07-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习算法</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习之无监督学习算法
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文是一个关于常用的无监督学习算法的学习笔记，以及与其相关的一些知识点。</p>
<p>无监督学习指我们只知道benwe有一堆数据（x），但并不了解什么是正确答案（输出y）。</p>
<p>无监督学习算法的作用如聚类算法可以判定这个数据集包含两个簇</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/4.png" alt></p>
<p>例子：谷歌新闻收集某个事件所有报道，就是搜索成千上万个新闻，把与这个事件有关的新闻都分为一簇。</p>
<h3 id="聚类算法–K-means"><a href="#聚类算法–K-means" class="headerlink" title="聚类算法–K-means"></a>聚类算法–K-means</h3><h4 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h4><p>1.随机生成两个点–聚类中心</p>
<p>2.簇分配</p>
<p>遍历每个样本，离哪个聚类中心更近就把它们分为哪一簇</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/88.png" alt></p>
<p>3.移动聚类中心</p>
<p>将红色的聚类中心移至红点的均值处，蓝色的聚类中心作相同操作。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/89.png" alt></p>
<p>4.重复操作</p>
<p>移动聚类中心后，再次检查这些样本，看它们离哪个聚类中心更近，就分配给那个簇。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/90.png" alt></p>
<p>之后再次计算这两个簇的均值，并将各自的聚类中心移至新的均值处。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/91.png" alt></p>
<p>按照新的聚类中心，重新将样本分簇。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/92.png" alt></p>
<p>现在k均值已经聚合了，即聚类中心和簇已经固定了。</p>
<h4 id="K均值算法的一般表示"><a href="#K均值算法的一般表示" class="headerlink" title="K均值算法的一般表示"></a>K均值算法的一般表示</h4><p>两个输入：</p>
<p>一是簇的个数，K个</p>
<p>二是样本（m个样本，每个样本的特征值为n个）</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/93.png" alt></p>
<p>随机初始化K个聚类中心</p>
<p>c^(i)=2表示第i个样本属于簇2</p>
<p>遍历m个样本，对于第i个样本，找出它离哪个聚类中心最近，就把它分配给这个聚类中心（如果某个聚类中心没有被分配到样本，我们一般会直接移除，或者重新分配一个聚类中心）。之后再把K个簇的样本分别求均值，作为新的聚类中心。重复以上步骤直到结果聚合为止。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/94.png" alt></p>
<h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>样本到它们各自的聚类中心距离的平方的均值的最小值。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/95.png" alt></p>
<p>簇分配与移动聚类中心的过程其实就是在最小化代价函数J</p>
<h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>如何初始化聚类中心，使算法避开局部最优：</p>
<p>1，K应该&lt;M</p>
<p>2，一开始时，随机选择K个样本作为聚类中心。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/96.png" alt></p>
<p>聚类中心初始化的不同与导致最终的结果不同，可能会落在局部最优。如下图所示</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/97.png" alt></p>
<p>为了让最终的结果是全局最优或者一个比较好的局部最优，我们可以尝试多次随机初始化，比较最终的结果。</p>
<p>具体步骤：</p>
<p>例如我们进行100次聚类算法操作，比较最终得到的100个代价函数，选择最小的那个。通常簇的数量小时多次随机初始化会更有效。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/98.png" alt></p>
<p>选择簇的数量</p>
<p>1.运用肘部法则</p>
<p>缺点：一些情况可能并不能得到一个清晰的拐点</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/99.png" alt></p>
<p>2.根据我们的目的来决定数量</p>
<p>例如我们可以将人们衣服的尺寸分为3类（S,M,L)或者是5类（XS,S,M,L,XL)，这就意味着簇的数量为3个或者5个。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/100.png" alt></p>
<h3 id="降维算法–PCA"><a href="#降维算法–PCA" class="headerlink" title="降维算法–PCA"></a>降维算法–PCA</h3><p>1.用于数据压缩（减小内存，加速学习算法）</p>
<p>有时如果两个特征高度相关（例如厘米和英寸，可能由于四舍五入的原因它们没有完全成正比），则将这个二维特征降为一维（变成一个新的特征值）。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/101.png" alt>2.用于可视化数据</p>
<p>通常将数据降为2维或3维</p>
<h4 id="PCA–主成分分析算法"><a href="#PCA–主成分分析算法" class="headerlink" title="PCA–主成分分析算法"></a>PCA–主成分分析算法</h4><p>首先将x1,x2，…xn进行均值归一化（使n个特征值的均值相加为0）和特征规范化。</p>
<p>Sj是特征j的标准偏差</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/104.png" alt></p>
<p>找到一个投影平面（一维或多维，看需求）对数据进行投影，使这些数据到这个平面的距离最短。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/102.png" alt></p>
<p>PCA不是线性回归</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/103.png" alt></p>
<p>左边是线性回归，右边是PCA.。</p>
<h4 id="获取降维后的特征值"><a href="#获取降维后的特征值" class="headerlink" title="获取降维后的特征值"></a>获取降维后的特征值</h4><p>如何将n维特征值减少到k维</p>
<p>1.首先计算sigma矩阵即协方差矩阵（n*n），不要X0</p>
<p>2.将sigma矩阵进行奇异值分解(svd表示奇异值分解)</p>
<p>3.在U矩阵n<em>n中提取前k个向量,减小后的U矩阵为n</em>k</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/105.png" alt></p>
<p>4.得到映射后的k维特征值Z(i)（k行*1列）。最初的X为n维。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/106.png" alt></p>
<p>（这里不进行数学证明为什么通过上述步骤就可以进行数据降维，以为证明过程超出了目前的学习范围）</p>
<h4 id="如何选取参数K"><a href="#如何选取参数K" class="headerlink" title="如何选取参数K"></a>如何选取参数K</h4><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/110.png" style="zoom:67%;">

<p>原始数据的重构(reconstruction),将z还原到x：</p>
<img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/109.png" style="zoom:67%;">

<p>调整k使得99%的方差被保留。0.01可以按照需求进行调整。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/107.png" alt></p>
<p>下图左方框中的值等价于图右中进行svd后对S矩阵的操作</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/108.png" alt></p>
<h4 id="用PCA加速监督学习算法"><a href="#用PCA加速监督学习算法" class="headerlink" title="用PCA加速监督学习算法"></a>用PCA加速监督学习算法</h4><p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/111.png" alt></p>
<p>PCA只能在训练集上运行，当定义了x到z的映射后，才可以把这个映射运用到验证集和测试集中。</p>
<h4 id="PCA不能用于处理过拟合"><a href="#PCA不能用于处理过拟合" class="headerlink" title="PCA不能用于处理过拟合"></a>PCA不能用于处理过拟合</h4><p>尽管PCA似乎减少了特征值，但这个过程并没有使用到y值，可能会舍弃一些有价值的信息。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/112.png" alt></p>
<p>此外，在设计一个机器学习系统时，也不要过多的滥用PCA,即使一开始有大量的特征值，先考虑能否直接实现</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/113.png" alt></p>
<h3 id="异常检测算法"><a href="#异常检测算法" class="headerlink" title="异常检测算法"></a>异常检测算法</h3><p>假设那些红色标记的引擎是正常的，现在检测一个新的飞机引擎是否有异常：</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/114.png" alt></p>
<p>更普遍来说</p>
<p>根据训练样本(都是正常的)，建立概率模型p(x)。如果测试样本的概率p(X test)小于某个设定的阈值，就认为这个测试样本是异常的。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/115.png" alt></p>
<h4 id="正态分布（高斯分布）"><a href="#正态分布（高斯分布）" class="headerlink" title="正态分布（高斯分布）"></a>正态分布（高斯分布）</h4><p>根据满足正态分布的这些样本计算两个参数：均值和方差。知道这两个参数就能画出图。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/116.png" alt></p>
<h4 id="用正态分布推异常检测算法"><a href="#用正态分布推异常检测算法" class="headerlink" title="用正态分布推异常检测算法"></a>用正态分布推异常检测算法</h4><p>现在有m个样本，n个特征量，每个特征量（独立的，不是也没关系）都服从高斯分布。计算出每个特征量各自的的高斯分布（也就是计算均值，方差），最后根据p(x)=p(x1,x2,…xn)=p(x1)p(x2)…p(xn)计算p(x)。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/117.png" alt></p>
<p>假如现在有两个特征量</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/118.png" alt></p>
<h4 id="例子–飞机引擎异常检测"><a href="#例子–飞机引擎异常检测" class="headerlink" title="例子–飞机引擎异常检测"></a>例子–飞机引擎异常检测</h4><p>假设有10000个好的引擎，40个坏的。（这里其实这些样本都是有标签的），像以前一样把它们分成三种样本。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/119.png" alt></p>
<p>用训练集得到这个异常检验算法后，用验证集进行检验，由于数据很倾斜(y=0的结果很常见)，我们用精确度或召回率（前文有）来判断这是不是一个好的模型。</p>
<p>在验证集上选择ε（找一组），看哪个ε对应的F1-score值最大；或者决定要不要舍弃某个特征值。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/120.png" alt></p>
<p>最后再用测试集进行最终的测试。</p>
<h4 id="异常检测-vs-监督学习"><a href="#异常检测-vs-监督学习" class="headerlink" title="异常检测 vs 监督学习"></a>异常检测 vs 监督学习</h4><p>既然我们的样本都是带有标签的，那为什么不能用逻辑回归或者神经网络去检测异常。</p>
<p>因为对于一些例子positive样本很少很少，而negative样本很多（比如飞机引擎10000个可能只有20个坏的），我们很难通过学习监督学习的算法去找到那些positive样本的规律。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/121.png" alt></p>
<p>选择特征</p>
<p>当某个特征值x不满足正太分布时，将它们高斯化（log(x)或x^1/2…）。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/122.png" alt></p>
<h4 id="如何选取特征"><a href="#如何选取特征" class="headerlink" title="如何选取特征"></a>如何选取特征</h4><p>与之前一样，当用训练样本得到异常检测模型后，用验证集输入到模型中，检查那些检测出错的样本，观察如何改动现有的特征值去更好的。</p>
<h4 id="多元高斯分布"><a href="#多元高斯分布" class="headerlink" title="多元高斯分布"></a>多元高斯分布</h4><p>一些情况下，高斯分布不能很好的检测异常。</p>
<p>如下图，我们知道蓝圈中的都是正常的样本，而高斯分布会使得粉圈上的样本概率都相同，这样本来一些异常的样本如果用高斯分布检测就会出错。（我认为这是因为x1，x2存在正相关的关系，并不是相互独立的）</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/123.png" alt></p>
<p>此时的p(x)表示为</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/124.png" alt></p>
<p>改变协方差矩阵</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/125.png" alt></p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/126.png" alt></p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/127.png" alt></p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/128.png" alt></p>
<p>改变均值</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/129.png" alt></p>
<p>参数拟合：</p>
<p>协方差矩阵与在PCA中的表示一样。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/130.png" alt></p>
<p>如何用多元高斯分布进行异常检测</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/131.png" alt></p>
<h4 id="多元高斯-vs-高斯"><a href="#多元高斯-vs-高斯" class="headerlink" title="多元高斯 vs 高斯"></a>多元高斯 vs 高斯</h4><p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/132.png" alt></p>
<h3 id="推荐系统算法"><a href="#推荐系统算法" class="headerlink" title="推荐系统算法"></a>推荐系统算法</h3><p>例子：预测电影评分</p>
<p>对于那些已经评分的电影（r(i,j)=1表示用户j评价了电影i，y^(i,j)=1表示评分为1分。），我们想要预测用户如何对没看过的电影评分</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/133.png" alt></p>
<h4 id="基于内容的推荐算法"><a href="#基于内容的推荐算法" class="headerlink" title="基于内容的推荐算法"></a>基于内容的推荐算法</h4><p>x1表示一部电影为爱情片的程度</p>
<p>x2表示一部电影为动作片的程度</p>
<p>每一部电影就可以用一组特征值表示（x0=1）（基于内容的含义）</p>
<p>每个用户各自的喜好用参数θ1，θ2…表示</p>
<p>假设函数（这里用线性方程）表示为θ1^T*X^(3)（如果预测用户1对电影3的评价）</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/134.png" alt></p>
<p>m^(j)表示用户j评价的电影的数量</p>
<p>如何计算用户j的参数：</p>
<p>写出代价函数（与线性回归一样），为了方便这里不要m（不影响最终结果）</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/135.png" alt></p>
<p>对于所有用户，代价函数为</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/136.png" alt></p>
<h4 id="自行学习特征算法"><a href="#自行学习特征算法" class="headerlink" title="自行学习特征算法"></a>自行学习特征算法</h4><p>假如现在我们知道每个用户的喜好（θ），知道A和B喜好爱情电影，C和D不喜欢，喜换动作电影。那么根据他们对于电影的评分，我们就能够推测出那些电影是什么类型的（x）。例如对于第一部电影：</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/137.png" alt></p>
<p>对于第i部电影，通过最小化代价函数找到它的特征值：</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/138.png" alt></p>
<p>对于所有电影，代价函数为 ：</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/139.png" alt></p>
<h4 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h4><p>将上文说到的两种算法结合起来。即我们只知道一些用户评分，就可以将用户的参数和电影的特征值都学习出来。</p>
<p>思想：我们先将参数随机初始化，然后算出特征值，再根据这些特征值，拟合出更好的参数…不断循环，直到算法已经可以很好的进行预测。</p>
<p>用于得到协调过滤算法的一种更好的方法，将两个代价函数结合起来，对所有有评分的用户-电影对代价进行求和，同时解出x和θ（不需要反复计算），这里不要x0：</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/140.png" alt></p>
<h5 id="算法步骤总结"><a href="#算法步骤总结" class="headerlink" title="算法步骤总结"></a>算法步骤总结</h5><p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/141.png" alt></p>
<p>第三步中用户j和电影i都是我们学习过程中的对象，也就是我们只能对前面表格中的那些问号进行预测。</p>
<h5 id="协同过滤算法的应用"><a href="#协同过滤算法的应用" class="headerlink" title="协同过滤算法的应用"></a>协同过滤算法的应用</h5><p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/142.png" alt></p>
<p>写出预测矩阵</p>
<p>低秩矩阵（Xθ^T）分解</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/143.png" alt></p>
<p>如果用户正在看电影i，如何找到与i相似的电影并推荐给用户：</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/144.png" alt></p>
<h5 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h5><p>如果有一个新用户Eve，她没有对任何电影进行评价，如果按照常规的协同过滤算法算出的Eve的参数是一个0向量，那她的所有预测评分都是0 ，这样就不能给她推荐电影。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/145.png" alt></p>
<p>解决方法：</p>
<p>均值归一化指算出每一部电影分数的均值，客户对这部电影的评分减去均值，这样每一行（相加为0）。用户评分均值归一化后，再用协调过滤算法来学习特征值和参数。现在对某部电影的预测评分就还要加上那部电影评分的均值。因此Eve的预测评分就不再是0，而变成了均值。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/146.png" alt></p>
<h3 id="处理大数据集"><a href="#处理大数据集" class="headerlink" title="处理大数据集"></a>处理大数据集</h3><p>如果有几亿十几亿样本时，一般的梯度下降算法的计算量将会非常大。所以我们现在学习一些新的梯度下降算法</p>
<h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h4><p>每次迭代只需要一个样本，得到的代价函数会迭代到接近全局最小值，并在那里反复徘徊。一般我们将学习速率设置为一个常数（偶尔有些人会让a逐渐变小，这样越迭代越能精确到全局最小）</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/153.png" alt></p>
<h5 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h5><p>1.随机打乱所有数据：将m个训练样本随机重新排列（能够更快收敛）</p>
<p>2.对训练样本一个一个的进行遍历，这样在更新θj时，我们可以把图左橘框中的求和项拆开，一次一次的改变θj而不是先把几亿个加起来。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/147.png" alt></p>
<p>如果m很大很大，很有可能只需要进行一次随机梯度下降就收敛了。一般Repeat的次数为1-10次。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/148.png" alt></p>
<h5 id="确保能正确收敛"><a href="#确保能正确收敛" class="headerlink" title="确保能正确收敛"></a>确保能正确收敛</h5><p>对于一般的梯度下降，如前文所说，我们通过绘制出代价函数随迭代次数变化的图像观察代价函数是否收敛。</p>
<p>对于随机梯度下降，我们计算1000次迭代的代价函数的平均值，绘制出代价函数的随着次数变化的图像观察。</p>
<p>!(机器学习（基础篇）\151.png)</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/151.png" alt></p>
<p>如下图所示，</p>
<p>左上方橘线表示学习速率a更大时</p>
<p>右上方橘线表示算平均代价函数时选取的样本量更大时</p>
<p>左下方表示有时候噪声太大，选取1000个样本取平均函数看不出是在收敛，如果选取5000个（橘）时能看出是在缓慢收敛的</p>
<p>右下方表示代价函数发散了，要考虑选用更小的学习速率</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/152.png" alt></p>
<h4 id="Mini-Batch梯度下降"><a href="#Mini-Batch梯度下降" class="headerlink" title="Mini-Batch梯度下降"></a>Mini-Batch梯度下降</h4><p>介于一般梯度下降和随机梯度下降之间，每次用b个样本进行迭代。b一般为2-100之间。例如b=10时：</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/149.png" alt></p>
<p>如果有好的向量化方法同时计算10个样本，mini-batch梯度下降会比随机梯度下降更快。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/150.png" alt></p>
<p>在线学习机制</p>
<p>随机梯度下降的衍生</p>
<h5 id="例1"><a href="#例1" class="headerlink" title="例1"></a>例1</h5><p>例如我们有一个运输货物网站，有连续的数据流（客户访问）。客户输入起始地和目的地，我们给他们一个价格，看他们是否接受我们的运输服务。之后通过学习（这个例子使用逻辑回归，神经网络也可以）改变价格使其最优。</p>
<p>x包括起始地，目的地，价格，y为0或1（接受这个价格）</p>
<p>在得到一个样本(x,y)后，使用这个样本来更新参数，之后丢弃这个样本</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/154.png" alt></p>
<p>所以在线学习算法可以适应用户偏好的变动</p>
<h5 id="例2-CTR–点击率学习问题"><a href="#例2-CTR–点击率学习问题" class="headerlink" title="例2 CTR–点击率学习问题"></a>例2 CTR–点击率学习问题</h5><p>学习用户点击某一个你提供给他们链接的概率。</p>
<p>当用户输入关键词时，返回10个搜索结果，即我们就得到了10个样本，然后用在线学习算法更新参数。之后丢弃这些样本。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/155.png" alt></p>
<p>Map-Reduce</p>
<p>当随机梯度下降都不能很好的减小计算时</p>
<p>假如我们有四台电脑，我们把样本（贤假设样本量只有400）分成4份。分别计算再将结果传给一个中心服务器进行整合。</p>
<p><img src="/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/156.png" alt></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" data-id="ckeidjumz001578vt0w5dclvz" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Mini-Batch%E4%B8%8B%E9%99%8D/" rel="tag">Mini-Batch下降</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/" rel="tag">异常检测算法</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%97%E6%B3%95/" rel="tag">推荐系统算法</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">无监督学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95K-means/" rel="tag">聚类算法K-means</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95PCA/" rel="tag">降维算法PCA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" rel="tag">随机梯度下降</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/07/05/Python%E4%B9%8B%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Python之排序算法学习笔记
        
      </div>
    </a>
  
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%93/">数据分析库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/" rel="tag">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/" rel="tag">Matplotlib</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mini-Batch%E4%B8%8B%E9%99%8D/" rel="tag">Mini-Batch下降</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Numpy/" rel="tag">Numpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/" rel="tag">Pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Re/" rel="tag">Re</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Request/" rel="tag">Request</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scrapy/" rel="tag">Scrapy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bs4/" rel="tag">bs4</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" rel="tag">决策树与随机森林</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB/" rel="tag">动态爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/" rel="tag">异常检测算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" rel="tag">排序算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%97%E6%B3%95/" rel="tag">推荐系统算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" rel="tag">支持向量机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">无监督学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">监督学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95K-means/" rel="tag">聚类算法K-means</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95PCA/" rel="tag">降维算法PCA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" rel="tag">随机梯度下降</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB/" rel="tag">静态爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Mini-Batch%E4%B8%8B%E9%99%8D/" style="font-size: 10px;">Mini-Batch下降</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Python/" style="font-size: 20px;">Python</a> <a href="/tags/Re/" style="font-size: 10px;">Re</a> <a href="/tags/Request/" style="font-size: 10px;">Request</a> <a href="/tags/Scrapy/" style="font-size: 16.67px;">Scrapy</a> <a href="/tags/bs4/" style="font-size: 10px;">bs4</a> <a href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/" style="font-size: 10px;">决策树与随机森林</a> <a href="/tags/%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB/" style="font-size: 10px;">动态爬虫</a> <a href="/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/" style="font-size: 10px;">异常检测算法</a> <a href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" style="font-size: 10px;">排序算法</a> <a href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%97%E6%B3%95/" style="font-size: 10px;">推荐系统算法</a> <a href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" style="font-size: 10px;">支持向量机</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 10px;">数据库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">无监督学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 13.33px;">机器学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 10px;">爬虫</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">监督学习</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">神经网络</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95K-means/" style="font-size: 10px;">聚类算法K-means</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">逻辑回归</a> <a href="/tags/%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95PCA/" style="font-size: 10px;">降维算法PCA</a> <a href="/tags/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 10px;">随机梯度下降</a> <a href="/tags/%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB/" style="font-size: 10px;">静态爬虫</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/07/30/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/">决策树与随机森林</a>
          </li>
        
          <li>
            <a href="/2020/07/30/Java%E5%9F%BA%E7%A1%80%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Java基础篇学习笔记</a>
          </li>
        
          <li>
            <a href="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/">Scrapy&amp;Re--动态爬虫实例</a>
          </li>
        
          <li>
            <a href="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习之监督学习算法</a>
          </li>
        
          <li>
            <a href="/2020/07/05/Linux%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Linux操作学习笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>