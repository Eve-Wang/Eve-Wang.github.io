<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Scrapy静态爬虫实例-爬取ip和port | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="地址：https:&#x2F;&#x2F;www.xicidaili.com&#x2F;nn&#x2F; 1新建项目scrapy startproject xxx(项目名称) 1D:\pycode&gt;scrapy startproject xicidailiSpider  2 创建爬虫scrapy genspider 爬虫名称 域名 1D:\pycode\xicidailiSpider&gt;scrapy genspider xic">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy静态爬虫实例-爬取ip和port">
<meta property="og:url" content="http://yoursite.com/2020/07/05/Scrapy%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="地址：https:&#x2F;&#x2F;www.xicidaili.com&#x2F;nn&#x2F; 1新建项目scrapy startproject xxx(项目名称) 1D:\pycode&gt;scrapy startproject xicidailiSpider  2 创建爬虫scrapy genspider 爬虫名称 域名 1D:\pycode\xicidailiSpider&gt;scrapy genspider xic">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Scrapy%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport/Scrapy-%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport%5C1.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Scrapy%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport/Scrapy-%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport%5C2.png">
<meta property="article:published_time" content="2020-07-05T14:41:15.000Z">
<meta property="article:modified_time" content="2020-07-06T15:54:28.044Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Scrapy">
<meta property="article:tag" content="静态爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/07/05/Scrapy%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport/Scrapy-%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport%5C1.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Scrapy静态爬虫实例-获取ip和port" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/05/Scrapy%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport/" class="article-date">
  <time datetime="2020-07-05T14:41:15.000Z" itemprop="datePublished">2020-07-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Scrapy静态爬虫实例-爬取ip和port
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>地址：<a href="https://www.xicidaili.com/nn/" target="_blank" rel="noopener">https://www.xicidaili.com/nn/</a></p>
<h2 id="1新建项目"><a href="#1新建项目" class="headerlink" title="1新建项目"></a>1新建项目</h2><p>scrapy startproject xxx(项目名称)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D:\pycode&gt;scrapy startproject xicidailiSpider</span><br></pre></td></tr></table></figure>

<h2 id="2-创建爬虫"><a href="#2-创建爬虫" class="headerlink" title="2 创建爬虫"></a>2 创建爬虫</h2><p>scrapy genspider 爬虫名称 域名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D:\pycode\xicidailiSpider&gt;scrapy genspider xicidaili xicidaili.com</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p>   *上面是项目名称，下面是爬虫名称，不要弄成一样的了。</p>
<p>   *网站域名是允许爬虫采集的域名</p>
<p>打开项目，我们就可以发现spider文件夹下有xicidaili.py文件了，这就是我们的爬虫文件！！！</p>
<h2 id="3-Robots协议"><a href="#3-Robots协议" class="headerlink" title="3 Robots协议"></a>3 Robots协议</h2><p>在settings.py文件中，找到下面这行代码</p>
<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="keyword">True</span></span><br></pre></td></tr></table></figure>

<p>将True改为False： 不然爬虫没法工作</p>
<p>可以到网站根目录添加后缀 robots.txt查看网站的robots协议</p>
<h2 id="4-爬虫文件介绍"><a href="#4-爬虫文件介绍" class="headerlink" title="4 爬虫文件介绍"></a>4 爬虫文件介绍</h2><p>打开xicidaili.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy   <span class="comment">#导入scrapy包</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建爬虫类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XicidailiSpider</span><span class="params">(scrapy.Spider)</span>:</span>  <span class="comment">#继承自scrapy.Spider类</span></span><br><span class="line">    name = <span class="string">'xicidaili'</span>   <span class="comment">#爬虫名</span></span><br><span class="line">    allowed_domains = [<span class="string">'xicidaili.com'</span>] <span class="comment">#允许爬取的域名，可以注释掉</span></span><br><span class="line">    start_urls = [<span class="string">'http://xicidaili.com/'</span>] <span class="comment">#开始采集的网址</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#解析响应数据，提取数据获取网址等  response就是网页源码</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h2 id="5分析网站"><a href="#5分析网站" class="headerlink" title="5分析网站"></a>5分析网站</h2><ul>
<li>提取数据<ul>
<li>正则（基础  ）</li>
<li>xpath ——–&gt;从html中提取数据语法<ul>
<li>response.xpath(‘xpath表达式’).get()  获取一个数据</li>
<li>response.xpath(‘xpath表达式’).getall()  获取多个数据</li>
<li>response.xpath(‘xpath表达式’).extract_first()  和get同样的效果</li>
<li>用xpath helper这个插件，我们可以直观的去查找标签</li>
</ul>
</li>
<li>css</li>
<li>BS4 </li>
</ul>
</li>
</ul>
<h2 id="6提取数据"><a href="#6提取数据" class="headerlink" title="6提取数据"></a>6提取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy   <span class="comment">#导入scrapy包</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建爬虫类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XicidailiSpider</span><span class="params">(scrapy.Spider)</span>:</span>  <span class="comment">#继承自scrapy.Spider类</span></span><br><span class="line">    name = <span class="string">'xicidaili'</span>   <span class="comment">#爬虫名</span></span><br><span class="line">    allowed_domains = [<span class="string">'xicidaili.com'</span>] <span class="comment">#允许爬取的域名，可以注释掉</span></span><br><span class="line">    <span class="comment"># start_urls = ['http://xicidaili.com/'] #开始采集的网址</span></span><br><span class="line">    start_urls = [<span class="string">'https://www.xicidaili.com/nn/'</span>]<span class="comment">#浏览器打开把网址复制下来</span></span><br><span class="line"><span class="comment">#解析响应数据，提取数据获取网址等  response就是网页源码</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment">#提取ip,port</span></span><br><span class="line">        <span class="comment"># response.xpath('表达式')</span></span><br><span class="line">        <span class="comment"># response.xpath('//tr//td[2]/text()')  #ip</span></span><br><span class="line">        <span class="comment"># response.xpath('//tr//td[3]/text()')  #port</span></span><br><span class="line">        <span class="comment"># 我们不像上面几行这么写，拿到tr标签的内容再选择可以保证每个ip和它的port一一对应</span></span><br><span class="line">        selectors = response.xpath(<span class="string">'//tr'</span>)   <span class="comment">#选择所有的tr标签</span></span><br><span class="line">        <span class="keyword">for</span> selector <span class="keyword">in</span> selectors:  <span class="comment">#遍历tr标签下的所有的td标签</span></span><br><span class="line">            ip = selector.xpath(<span class="string">'./td[2]/text()'</span>)  <span class="comment">#   ./ 表示在当前节点下继续选择</span></span><br><span class="line">            port = selector.xpath(<span class="string">'./td[3]/text()'</span>) <span class="comment">#在当前节点下继续选择</span></span><br><span class="line"></span><br><span class="line">            print(ip,port)  <span class="comment">#打印ip  port</span></span><br></pre></td></tr></table></figure>

<h2 id="7运行爬虫"><a href="#7运行爬虫" class="headerlink" title="7运行爬虫"></a>7运行爬虫</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D:\pycode\xicidailiSpider&gt;scrapy crawl xicidaili</span><br></pre></td></tr></table></figure>

<p>发现报错了：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">2019-09-15 20:22:36 [scrapy.utils.log] INFO: Scrapy 1.7.2 started (bot: xicidailiSpider)</span><br><span class="line">2019-09-15 20:22:36 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.1, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.17134-SP0</span><br><span class="line">2019-09-15 20:22:36 [scrapy.crawler] INFO: Overridden settings: &#123;'BOT_NAME': 'xicidailiSpider', 'NEWSPIDER_MODULE': 'xicidailiSpider.spiders', 'SPIDER_MODULES': ['xicidailiSpider.spiders']&#125;</span><br><span class="line">2019-09-15 20:22:36 [scrapy.extensions.telnet] INFO: Telnet Password: 1f33d178e985a6c0</span><br><span class="line">2019-09-15 20:22:36 [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">['scrapy.extensions.corestats.CoreStats',</span><br><span class="line"> 'scrapy.extensions.telnet.TelnetConsole',</span><br><span class="line"> 'scrapy.extensions.logstats.LogStats']</span><br><span class="line">2019-09-15 20:22:36 [scrapy.middleware] INFO: Enabled downloader middlewares:</span><br><span class="line">['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.retry.RetryMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',</span><br><span class="line"> 'scrapy.downloadermiddlewares.stats.DownloaderStats']</span><br><span class="line">2019-09-15 20:22:36 [scrapy.middleware] INFO: Enabled spider middlewares:</span><br><span class="line">['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',</span><br><span class="line"> 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',</span><br><span class="line"> 'scrapy.spidermiddlewares.referer.RefererMiddleware',</span><br><span class="line"> 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',</span><br><span class="line"> 'scrapy.spidermiddlewares.depth.DepthMiddleware']</span><br><span class="line">2019-09-15 20:22:36 [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[]</span><br><span class="line">2019-09-15 20:22:36 [scrapy.core.engine] INFO: Spider opened</span><br><span class="line">2019-09-15 20:22:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2019-09-15 20:22:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023</span><br><span class="line">2019-09-15 20:22:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying &lt;GET https://www.xicidaili.com/nn/&gt; (failed 1 times): 503 Service Unavailable</span><br><span class="line">2019-09-15 20:22:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying &lt;GET https://www.xicidaili.com/nn/&gt; (failed 2 times): 503 Service Unavailable</span><br><span class="line">2019-09-15 20:22:37 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying &lt;GET https://www.xicidaili.com/nn/&gt; (failed 3 times): 503 Service Unavailable</span><br><span class="line">2019-09-15 20:22:37 [scrapy.core.engine] DEBUG: Crawled (503) &lt;GET https://www.xicidaili.com/nn/&gt; (referer: None)</span><br><span class="line">2019-09-15 20:22:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;503 https://www.xicidaili.com/nn/&gt;: HTTP status code is not handled or not allowed</span><br><span class="line">2019-09-15 20:22:37 [scrapy.core.engine] INFO: Closing spider (finished)</span><br><span class="line">2019-09-15 20:22:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;'downloader/request_bytes': 657,</span><br><span class="line"> 'downloader/request_count': 3,</span><br><span class="line"> 'downloader/request_method_count/GET': 3,</span><br><span class="line"> 'downloader/response_bytes': 999,</span><br><span class="line"> 'downloader/response_count': 3,</span><br><span class="line"> 'downloader/response_status_count/503': 3,</span><br><span class="line"> 'elapsed_time_seconds': 0.60528,</span><br><span class="line"> 'finish_reason': 'finished',</span><br><span class="line"> 'finish_time': datetime.datetime(2019, 9, 15, 12, 22, 37, 561550),</span><br><span class="line"> 'httperror/response_ignored_count': 1,</span><br><span class="line"> 'httperror/response_ignored_status_count/503': 1,</span><br><span class="line"> 'log_count/DEBUG': 4,</span><br><span class="line"> 'log_count/INFO': 11,</span><br><span class="line"> 'response_received_count': 1,</span><br><span class="line"> 'retry/count': 2,</span><br><span class="line"> 'retry/max_reached': 1,</span><br><span class="line"> 'retry/reason_count/503 Service Unavailable': 2,</span><br><span class="line"> 'scheduler/dequeued': 3,</span><br><span class="line"> 'scheduler/dequeued/memory': 3,</span><br><span class="line"> 'scheduler/enqueued': 3,</span><br><span class="line"> 'scheduler/enqueued/memory': 3,</span><br><span class="line"> 'start_time': datetime.datetime(2019, 9, 15, 12, 22, 36, 956270)&#125;</span><br><span class="line">2019-09-15 20:22:37 [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre></td></tr></table></figure>

<p>503错误，服务器端的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">downloader&#x2F;response_status_count&#x2F;503&#39;: 3,</span><br></pre></td></tr></table></figure>

<p><strong>补充：常见可能被网站识别返回错误</strong><br> 1、CAPTCHApages （captcha，验证码）</p>
<p>2、Unusualcontent delivery delay  （响应时间、速度变慢了）</p>
<p>3、Frequentresponse with HTTP404,301 or 50x errors</p>
<p>（1）301 MovedTemporarily</p>
<p>（2）401unauthorized</p>
<p>（3）403forbidden  （aAatch处理的）</p>
<p>（4）404 notfound</p>
<p>（5）408 requesttimeout</p>
<p>（6）429 too manyrequests</p>
<p>（7）503 serviceunavailable  （ip层）</p>
<h2 id="8原因分析："><a href="#8原因分析：" class="headerlink" title="8原因分析："></a>8原因分析：</h2><p>1、在settings中将rebots改为False，发现依旧返回503错误</p>
<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Obey robots.txt rules</span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = <span class="keyword">False</span></span><br></pre></td></tr></table></figure>

<p>2、再分析，浏览器中打开正常显示，scrapy跟浏览器请求是同一个ip，所以排除ip封锁问题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">网页不需要登录，排除cookie问题，剩下就是headers中user_agent的问题了。scrapy中添加user_agent之后，成功解决</span><br><span class="line"></span><br><span class="line">结论：没有请求头，被网站识别为爬虫程序（所以要模拟浏览器访问）</span><br></pre></td></tr></table></figure>

<p>所以我们要给他添加请求头header</p>
<h2 id="9添加头部"><a href="#9添加头部" class="headerlink" title="9添加头部"></a>9添加头部</h2><p>在settings.py文件里找到如下代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span>,</span><br><span class="line">  <span class="string">'Accept-Language'</span>: <span class="string">'en'</span>,</span><br><span class="line">  <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>将其取消注释并添加请求头，运行：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/text()' data='9999'&gt;]</span><br><span class="line">[<span class="tag">&lt;<span class="name">Selector</span> <span class="attr">xpath</span>=<span class="string">'./td[2]/text()'</span> <span class="attr">data</span>=<span class="string">'120.83.103.135'</span>&gt;</span>] [<span class="tag">&lt;<span class="name">Selector</span> <span class="attr">xpath</span>=<span class="string">'./td[3]/text()'</span> <span class="attr">data</span>=<span class="string">'9999'</span>&gt;</span>]</span><br><span class="line">[<span class="tag">&lt;<span class="name">Selector</span> <span class="attr">xpath</span>=<span class="string">'./td[2]/text()'</span> <span class="attr">data</span>=<span class="string">'120.83.122.28'</span>&gt;</span>] [<span class="tag">&lt;<span class="name">Selector</span> <span class="attr">xpath</span>=<span class="string">'./td[3]/text()'</span> <span class="attr">data</span>=<span class="string">'9999'</span>&gt;</span>]</span><br><span class="line">[<span class="tag">&lt;<span class="name">Selector</span> <span class="attr">xpath</span>=<span class="string">'./td[2]/text()'</span> <span class="attr">data</span>=<span class="string">'14.115.206.93'</span>&gt;</span>] [<span class="tag">&lt;<span class="name">Selector</span> <span class="attr">xpath</span>=<span class="string">'./td[3]/text()'</span> <span class="attr">data</span>=<span class="string">'61234'</span>&gt;</span>]</span><br><span class="line">[<span class="tag">&lt;<span class="name">Selector</span> <span class="attr">xpath</span>=<span class="string">'./td[2]/text()'</span> <span class="attr">data</span>=<span class="string">'125.123.122.206'</span>&gt;</span>] [<span class="tag">&lt;<span class="name">Selector</span> <span class="attr">xpath</span>=<span class="string">'./td[3]/text()'</span> <span class="attr">data</span>=<span class="string">'9999'</span>&gt;</span>]</span><br></pre></td></tr></table></figure>

<p>但是有一个问题，我们要的数据保存在一个对象里面，我们要把他取出来：</p>
<h2 id="10数据整理"><a href="#10数据整理" class="headerlink" title="10数据整理"></a>10数据整理</h2><p>我们使用get/extract_first/getall方法来获取数据对象里的数据，在这里get/extract_firs所取得的效果是一样的，随意使用，接下来我们继续完善我们的爬虫文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy   <span class="comment">#导入scrapy包</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建爬虫类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XicidailiSpider</span><span class="params">(scrapy.Spider)</span>:</span>  <span class="comment">#继承自scrapy.Spider类</span></span><br><span class="line">    name = <span class="string">'xicidaili'</span>   <span class="comment">#爬虫名</span></span><br><span class="line">    allowed_domains = [<span class="string">'xicidaili.com'</span>] <span class="comment">#允许爬取的域名</span></span><br><span class="line">    <span class="comment"># start_urls = ['http://xicidaili.com/'] #开始采集的网址</span></span><br><span class="line">    start_urls = [<span class="string">'https://www.xicidaili.com/nn/'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#解析响应数据，提取数据获取网址等  response就是网页源码</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment">#提取ip,port</span></span><br><span class="line">        <span class="comment"># response.xpath('表达式')</span></span><br><span class="line">        <span class="comment"># response.xpath('//tr//td[2]/text()')  #ip</span></span><br><span class="line">        <span class="comment"># response.xpath('//tr//td[3]/text()')  #port</span></span><br><span class="line">        <span class="comment"># 我们不像上面几行这么写，拿到tr标签的内容再选择可以保证每个ip和它的port一一对应</span></span><br><span class="line">        selectors = response.xpath(<span class="string">'//tr'</span>)   <span class="comment">#选择所有的tr标签</span></span><br><span class="line">        <span class="keyword">for</span> selector <span class="keyword">in</span> selectors:  <span class="comment">#遍历tr标签下的所有的td标签</span></span><br><span class="line">            ip = selector.xpath(<span class="string">'./td[2]/text()'</span>).get()  <span class="comment">#   ./ 表示在当前节点下继续选择</span></span><br><span class="line">            port = selector.xpath(<span class="string">'./td[3]/text()'</span>).get() <span class="comment">#在当前节点下继续选择</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#ip = selector.xpath('./td[2]/text()').extract_first()    </span></span><br><span class="line">            <span class="comment">#port = selector.xpath('./td[3]/text()').extract_first() #这两行与前两行效果一样</span></span><br><span class="line">            print(ip,port)  <span class="comment">#打印ip  port</span></span><br></pre></td></tr></table></figure>

<p>运行爬虫：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D:\pycode\xicidailiSpider&gt;scrapy crawl xicidaili</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">112<span class="selector-class">.87</span><span class="selector-class">.71</span><span class="selector-class">.119</span> 9999</span><br><span class="line">144<span class="selector-class">.123</span><span class="selector-class">.68</span><span class="selector-class">.159</span> 9999</span><br><span class="line">110<span class="selector-class">.86</span><span class="selector-class">.136</span><span class="selector-class">.82</span> 9999</span><br><span class="line">110<span class="selector-class">.86</span><span class="selector-class">.136</span><span class="selector-class">.128</span> 9999</span><br><span class="line">171<span class="selector-class">.11</span><span class="selector-class">.178</span><span class="selector-class">.251</span> 9999</span><br><span class="line">120<span class="selector-class">.83</span><span class="selector-class">.105</span><span class="selector-class">.156</span> 9999</span><br><span class="line">171<span class="selector-class">.35</span><span class="selector-class">.149</span><span class="selector-class">.199</span> 9999</span><br><span class="line">125<span class="selector-class">.125</span><span class="selector-class">.131</span><span class="selector-class">.41</span> 9999</span><br><span class="line">120<span class="selector-class">.83</span><span class="selector-class">.102</span><span class="selector-class">.74</span> 9999</span><br><span class="line">1<span class="selector-class">.197</span><span class="selector-class">.203</span><span class="selector-class">.187</span> 9999</span><br><span class="line">120<span class="selector-class">.84</span><span class="selector-class">.100</span><span class="selector-class">.140</span> 808</span><br><span class="line">115<span class="selector-class">.207</span><span class="selector-class">.253</span><span class="selector-class">.27</span> 9999</span><br><span class="line">120<span class="selector-class">.83</span><span class="selector-class">.101</span><span class="selector-class">.77</span> 9999</span><br><span class="line">120<span class="selector-class">.83</span><span class="selector-class">.102</span><span class="selector-class">.181</span> 9999</span><br><span class="line">27<span class="selector-class">.43</span><span class="selector-class">.185</span><span class="selector-class">.13</span> 9999</span><br><span class="line">1<span class="selector-class">.197</span><span class="selector-class">.11</span><span class="selector-class">.20</span> 9999</span><br><span class="line">120<span class="selector-class">.83</span><span class="selector-class">.103</span><span class="selector-class">.135</span> 9999</span><br><span class="line">120<span class="selector-class">.83</span><span class="selector-class">.122</span><span class="selector-class">.28</span> 9999</span><br><span class="line">14<span class="selector-class">.115</span><span class="selector-class">.206</span><span class="selector-class">.93</span> 61234</span><br><span class="line">125<span class="selector-class">.123</span><span class="selector-class">.122</span><span class="selector-class">.206</span> 9999</span><br></pre></td></tr></table></figure>

<p>这样就拿到了我们想要的东西</p>
<h2 id="11-问题来了（获取所有页面）"><a href="#11-问题来了（获取所有页面）" class="headerlink" title="11 问题来了（获取所有页面）"></a>11 问题来了（获取所有页面）</h2><p>我们这里只获取到了一个页面的数据，但现实中我们要爬取的远不止一个页面的，拿xicidaili来说，他目前有4071页，我们从最初级的开始 使用for循环，遍历看看：</p>
<p>遍历所有页面：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start_urls = [<span class="string">'https://www.xicidaili.com/nn/&#123;page&#125;'</span> <span class="keyword">for</span> page in <span class="keyword">range</span>(<span class="number">1</span>,<span class="number">4071</span>)]</span><br></pre></td></tr></table></figure>

<p>（但不要轻易尝试上述代码，短时间操作太多次可能会被网址封掉IP）</p>
<p>假如以后该网站继续添加页面数，那怎么办？</p>
<p>我们发现每页代码都标明了下一页，最后一页的next_page为disable,为空，所以就好办了。</p>
<p>我们在第一页时使用xpath获取下一页的链接地址：(用插件：xpath helper)</p>
<figure class="highlight xpath"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//a[@class=<span class="string">"next_page"</span>]/@href</span><br></pre></td></tr></table></figure>

<p>上行指在有键值对为class=”next_page”的a标签中找键为href的值</p>
<p>显示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;nn&#x2F;2</span><br></pre></td></tr></table></figure>

<h3 id="完整的爬虫代码"><a href="#完整的爬虫代码" class="headerlink" title="完整的爬虫代码"></a>完整的爬虫代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy   <span class="comment">#导入scrapy包</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建爬虫类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XicidailiSpider</span><span class="params">(scrapy.Spider)</span>:</span>  <span class="comment">#继承自scrapy.Spider类</span></span><br><span class="line">    name = <span class="string">'xicidaili'</span>   <span class="comment">#爬虫名</span></span><br><span class="line">    allowed_domains = [<span class="string">'xicidaili.com'</span>] <span class="comment">#允许爬取的域名</span></span><br><span class="line">    <span class="comment"># start_urls = ['http://xicidaili.com/'] #开始采集的网址</span></span><br><span class="line">    start_urls = [<span class="string">'https://www.xicidaili.com/nn/4068'</span>]<span class="comment">#尝试一下从4068页开始爬</span></span><br><span class="line">    <span class="comment"># start_urls = ['https://www.xicidaili.com/nn/&#123;page&#125;' for page in range(1,4071)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#解析响应数据，提取数据获取网址等  response就是网页源码</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment">#提取ip,port</span></span><br><span class="line">        <span class="comment"># response.xpath('表达式')</span></span><br><span class="line">        <span class="comment"># response.xpath('//tr//td[2]/text()')  #ip</span></span><br><span class="line">        <span class="comment"># response.xpath('//tr//td[3]/text()')  #port</span></span><br><span class="line">        <span class="comment"># 我们不像上面几行这么写，拿到tr标签的内容再选择可以保证每个ip和它的port一一对应</span></span><br><span class="line">        selectors = response.xpath(<span class="string">'//tr'</span>)   <span class="comment">#选择所有的tr标签</span></span><br><span class="line">        <span class="keyword">for</span> selector <span class="keyword">in</span> selectors:  <span class="comment">#遍历tr标签下的所有的td标签</span></span><br><span class="line">            ip = selector.xpath(<span class="string">'./td[2]/text()'</span>).get()  <span class="comment">#   ./ 表示在当前节点下继续选择</span></span><br><span class="line">            port = selector.xpath(<span class="string">'./td[3]/text()'</span>).get() </span><br><span class="line">            </span><br><span class="line">            items=&#123;</span><br><span class="line">            <span class="string">'ip'</span>:ip,</span><br><span class="line">            <span class="string">'port'</span>:port</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">yield</span> items <span class="comment">#我们定义一个字典相当于给两列数据起名，并用生成器生成，后面进行保存操作</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#翻页操作</span></span><br><span class="line">        next_page = response.xpath(<span class="string">'//a[@class="next_page"]/@href'</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page:</span><br><span class="line">            print(next_page)</span><br><span class="line">            <span class="comment">#拼接网址</span></span><br><span class="line">            <span class="comment"># next_url = "https://www.xicidaili.com"+next_page;与下一行代码效果一样</span></span><br><span class="line">            next_url = response.urljoin(next_page)<span class="comment">#next_page作为相对路径并入url</span></span><br><span class="line">            <span class="comment">#发出请求   Request callback是回调函数，将请求得到响应交给自己处理</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_url,callback=self.parse) <span class="comment">#生成器</span></span><br></pre></td></tr></table></figure>

<p>Request()发出请求，类似于requests.get()</p>
<p>callback 是将发出去的请求得到的响应交还给自己处理</p>
<h2 id="12保存输出"><a href="#12保存输出" class="headerlink" title="12保存输出"></a>12保存输出</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D:\pycode\xicidailiSpider&gt;scrapy crawl xicidaili -o ip.json  (json文件) &#x2F;ip.csv(csv文件)</span><br></pre></td></tr></table></figure>

<p>现在可以看到文件已经保存好了</p>
<p><img src="/2020/07/05/Scrapy%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport/Scrapy-%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport%5C1.png" alt></p>
<p>打开csv文件：</p>
<p><img src="/2020/07/05/Scrapy%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport/Scrapy-%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport%5C2.png" alt></p>
<p>成功！</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/05/Scrapy%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport/" data-id="ckcapl1q6000ig8vt5v8fgan3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Scrapy/" rel="tag">Scrapy</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB/" rel="tag">静态爬虫</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Python爬虫学习笔记
        
      </div>
    </a>
  
  
    <a href="/2020/07/05/Python%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Python之数据结构学习笔记</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%93/">数据分析库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%AF%87/">机器学习基础篇</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/" rel="tag">Matplotlib</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Numpy/" rel="tag">Numpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/" rel="tag">Pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Re/" rel="tag">Re</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Request/" rel="tag">Request</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scrapy/" rel="tag">Scrapy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bs4/" rel="tag">bs4</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB/" rel="tag">动态爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" rel="tag">排序算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">无监督学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">监督学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB/" rel="tag">静态爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Python/" style="font-size: 20px;">Python</a> <a href="/tags/Re/" style="font-size: 10px;">Re</a> <a href="/tags/Request/" style="font-size: 10px;">Request</a> <a href="/tags/Scrapy/" style="font-size: 16.67px;">Scrapy</a> <a href="/tags/bs4/" style="font-size: 10px;">bs4</a> <a href="/tags/%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB/" style="font-size: 10px;">动态爬虫</a> <a href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" style="font-size: 10px;">排序算法</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 10px;">数据库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">无监督学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 13.33px;">机器学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 10px;">爬虫</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">监督学习</a> <a href="/tags/%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB/" style="font-size: 10px;">静态爬虫</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/">Scrapy&amp;Re--动态爬虫实例</a>
          </li>
        
          <li>
            <a href="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习之监督学习算法</a>
          </li>
        
          <li>
            <a href="/2020/07/05/Linux%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Linux操作学习笔记</a>
          </li>
        
          <li>
            <a href="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Matplotlib学习笔记</a>
          </li>
        
          <li>
            <a href="/2020/07/05/Mysql%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Mysql学习笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>