<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Python爬虫学习笔记 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="request库Requests的7个主要方法后四个方法有时由于网络安全限制并不能用。   r&#x3D;request.get(url,params&#x3D;None,**kwargs) #r指服务器资源的Response对象（包含爬虫返回的全部内容）。 url:网页链接 params:url中的额外参数，字典或字节流格式。 **kwargs：12个控制访问的参数。 requests.request(method">
<meta property="og:type" content="article">
<meta property="og:title" content="Python爬虫学习笔记">
<meta property="og:url" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="request库Requests的7个主要方法后四个方法有时由于网络安全限制并不能用。   r&#x3D;request.get(url,params&#x3D;None,**kwargs) #r指服务器资源的Response对象（包含爬虫返回的全部内容）。 url:网页链接 params:url中的额外参数，字典或字节流格式。 **kwargs：12个控制访问的参数。 requests.request(method">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/7.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/8.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/9.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/11.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/12.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/17.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/14.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/15.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/16.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/18.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/19.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/20.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/22.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/23.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/24.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/25.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/27.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/26.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/28.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/29.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/30.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E4%B9%A0note/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%92%8C%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/31.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/32.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/33.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/34.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/35.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/36.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/37.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/38.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/39.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/40.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/41.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/42.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/43.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/44.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/47.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/45.png">
<meta property="og:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/46.png">
<meta property="article:published_time" content="2020-07-05T14:47:48.000Z">
<meta property="article:modified_time" content="2020-07-06T15:39:47.857Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Scrapy">
<meta property="article:tag" content="爬虫">
<meta property="article:tag" content="bs4">
<meta property="article:tag" content="Re">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Python爬虫学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2020-07-05T14:47:48.000Z" itemprop="datePublished">2020-07-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Python爬虫学习笔记
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="request库"><a href="#request库" class="headerlink" title="request库"></a>request库</h2><h3 id="Requests的7个主要方法"><a href="#Requests的7个主要方法" class="headerlink" title="Requests的7个主要方法"></a>Requests的7个主要方法</h3><p>后四个方法有时由于网络安全限制并不能用。</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/5.png" style="zoom:67%;">

<p>r=request.get(url,params=None,**kwargs) #r指服务器资源的Response对象（包含爬虫返回的全部内容）。</p>
<p>url:网页链接</p>
<p>params:url中的额外参数，字典或字节流格式。</p>
<p>**kwargs：12个控制访问的参数。</p>
<p>requests.request(method,url,**kwargs)</p>
<p>**kwargs:13个控制访问的参数，均为可选项（如params:改变url，data，json，headers，options等）</p>
<p><strong>requests.get(url）与requests.request(‘GET’,url）效果相同</strong></p>
<h3 id="Response对象的属性"><a href="#Response对象的属性" class="headerlink" title="Response对象的属性"></a>Response对象的属性</h3><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.png" style="zoom:67%;">

<p>r.encoding:如果header中没有charset字段，则认为编码方式是ISO-8859-1</p>
<h3 id="Requests库的异常"><a href="#Requests库的异常" class="headerlink" title="Requests库的异常"></a>Requests库的异常</h3><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/2.png" style="zoom: 67%;">

<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/3.png" style="zoom: 67%;">

<p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">r=requests.get(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">print(r.status_code)<span class="comment">#若状态码返回200，说明请求成功。404表示失败</span></span><br><span class="line">print(r.headers)<span class="comment">#返回请求页面的头部信息</span></span><br><span class="line">print(r.text)<span class="comment">#发现有些乱码</span></span><br><span class="line">print(r.encoding)<span class="comment">#从页面头部看它的编码方式ISO-8859-1，这种编码方式不能解析中文</span></span><br><span class="line">print(r.apparent_encoding)<span class="comment">#从页面内容解析出来的编码方式（实际）为utf-8，与上一个不一致</span></span><br><span class="line">r.encoding=<span class="string">'utf-8'</span></span><br><span class="line">print(r.text)<span class="comment">#此时输出了正常（包含中文）的页面内容</span></span><br></pre></td></tr></table></figure>



<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/4.png" style="zoom:67%;">

<h4 id="try方法"><a href="#try方法" class="headerlink" title="try方法"></a>try方法</h4><p>用于处理异常，基本语法如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">try:</span><br><span class="line">&lt;语句&gt;        #运行别的代码</span><br><span class="line">except &lt;名字&gt;：</span><br><span class="line">&lt;语句&gt;        #如果在try部份引发了&#39;name&#39;异常</span><br><span class="line">except &lt;名字&gt;，&lt;数据&gt;:</span><br><span class="line">&lt;语句&gt;        #如果引发了&#39;name&#39;异常，获得附加的数据</span><br><span class="line">else:</span><br><span class="line">&lt;语句&gt;        #如果没有异常发生</span><br></pre></td></tr></table></figure>



<p>robots协议：网站通过设置robots协议来告诉用户那些数据可以爬取，哪些不能爬取。如下面nike的robots协议。</p>
<p><a href="https://www.nike.com/robots.txt" target="_blank" rel="noopener">https://www.nike.com/robots.txt</a></p>
<h4 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h4><p>当发生异常时，如r.status_code返回503，不是200：</p>
<p>我们修改请求报文头部信息（header）中的User-Agent，若还是不行，则将以浏览器形式访问的页面request header的字段都添加到我们爬虫时request的header中，之后就可以正常查看网页的代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">r=requests.get(url,timeout=<span class="number">30</span>)</span><br><span class="line">print(r.request.headers)</span><br><span class="line">kv=&#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0'</span>&#125;<span class="comment">#改变请求报文的user-agent，让对方以为我们是浏览器而不是爬虫</span></span><br><span class="line">u=requests.get(url,headers=kv)</span><br><span class="line">u.request.headers<span class="comment">#修改成功</span></span><br></pre></td></tr></table></figure>

<p>Out[37]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;User-Agent&#39;: &#39;python-requests&#x2F;2.22.0&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*&#x2F;*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*&#x2F;*&#39;, &#39;Connection&#39;: &#39;keep-a</span><br></pre></td></tr></table></figure>

<h2 id="实例1（网络图片的爬取和存储"><a href="#实例1（网络图片的爬取和存储" class="headerlink" title="实例1（网络图片的爬取和存储)"></a>实例1（网络图片的爬取和存储)</h2><p>网络图片链接的通用格式：</p>
<p><a href="http://www.example.com/picture.jpg" target="_blank" rel="noopener">http://www.example.com/picture.jpg</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">path=<span class="string">'d:/abc.jpg'</span></span><br><span class="line">url=<span class="string">'http://image.nationalgeographic.com.cn/2017/0211/20170211061910157.jpg'</span></span><br><span class="line">r=requests.get(url)<span class="comment">#图片是以二进制形式保存的</span></span><br><span class="line"><span class="keyword">with</span> open(path,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(r.content)<span class="comment">#因此要把相应内容的二进制形式写到path中</span></span><br></pre></td></tr></table></figure>

<p>可以看到图片已经保存到我们指定的目录中：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/6.png" style="zoom:50%;">

<p>爬取图片的标准代码：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/7.png" style="zoom: 33%;">

<h2 id="Beautiful-Soup-库"><a href="#Beautiful-Soup-库" class="headerlink" title="Beautiful Soup 库"></a>Beautiful Soup 库</h2><h3 id="对Beautiful-Soup的理解"><a href="#对Beautiful-Soup的理解" class="headerlink" title="对Beautiful Soup的理解"></a>对Beautiful Soup的理解</h3><p>Beautiful Soup类对应一个HTML/XML文档的全部内容，等价于html文件等价于标签树。bs4库将HTML文件转换为utf-8编码。</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/8.png" style="zoom: 33%;">

<h3 id="Beautiful-Soup的四种解析器"><a href="#Beautiful-Soup的四种解析器" class="headerlink" title="Beautiful Soup的四种解析器"></a>Beautiful Soup的四种解析器</h3><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/9.png" style="zoom: 67%;">

<h3 id="Beautiful-Soup类的基本元素"><a href="#Beautiful-Soup类的基本元素" class="headerlink" title="Beautiful Soup类的基本元素"></a>Beautiful Soup类的基本元素</h3><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/10.png" style="zoom: 67%;">

<p>如<a>.attrs[‘href’] 指获取a标签中键为href的值</a></p>
<p>代码展示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url=<span class="string">'https://python123.io/ws/demo.html'</span></span><br><span class="line">r=requests.get(url)</span><br><span class="line">demo=r.text</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup<span class="comment">#在anaconda中已经有bs4库，不需要再额外安装</span></span><br><span class="line">soup=BeautifulSoup(demo,<span class="string">'html.parser'</span>)<span class="comment">#用html.parser解析器去解析demo（HTML文件)</span></span><br><span class="line">print(soup)</span><br><span class="line">print(<span class="string">'*'</span>*<span class="number">100</span>)</span><br><span class="line">print(soup.prettify())<span class="comment">#prettify()会在每个标签后加一个换行符，作用是让&lt;html&gt;页面更友好的显示</span></span><br></pre></td></tr></table></figure>

<p>out</p>
<p><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/11.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(soup.title)<span class="comment">#打印标题</span></span><br><span class="line">a_tap=soup.a</span><br><span class="line">print(a_tap)<span class="comment">#只能打印第一个a标签</span></span><br><span class="line">print(soup.a.name)<span class="comment">#打印a标签的名字</span></span><br><span class="line">print(soup.a.parent.name)<span class="comment">#打印包含a标签的那个标签的名字</span></span><br><span class="line">print(a_tap.attrs)<span class="comment">#打印a标签的属性(键值对形式)</span></span><br><span class="line">print(a_tap.attrs[<span class="string">'class'</span>])<span class="comment">#打印a标签中class对应的值</span></span><br><span class="line">print(a_tap.string)<span class="comment">#打印a标签的字符串信息</span></span><br></pre></td></tr></table></figure>

<p>out</p>
<p><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/12.png" alt></p>
<h3 id="标签树的遍历"><a href="#标签树的遍历" class="headerlink" title="标签树的遍历"></a>标签树的遍历</h3><p>（通过这些遍历方法更加直观的去获取我们想要的标签）</p>
<p>三种遍历方式（以上个代码为例）：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/17.png" style="zoom:50%;">

<h4 id="下行遍历"><a href="#下行遍历" class="headerlink" title="下行遍历"></a>下行遍历</h4><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/13.png" style="zoom:67%;">

<p>代码展示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(soup.head)<span class="comment">#打印head标签</span></span><br><span class="line">print(soup.head.contents)<span class="comment">#打印head标签的子标签，返回列表形式</span></span><br><span class="line">print(soup.body.contents)<span class="comment">#打印body标签的子标签，返回列表形式</span></span><br><span class="line">print(len(soup.body.contents))<span class="comment">#打印body标签的子标签数量</span></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> soup.body.children: <span class="comment">#对子标签进行遍历</span></span><br><span class="line">    print(child)</span><br><span class="line">    print(<span class="string">'*'</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>

<p>out</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/14.png" style="zoom:67%;">

<h4 id="上行遍历"><a href="#上行遍历" class="headerlink" title="上行遍历"></a>上行遍历</h4><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/15.png" style="zoom:67%;">

<h4 id="平行遍历"><a href="#平行遍历" class="headerlink" title="平行遍历"></a>平行遍历</h4><p>平行遍历必须发生在同一个父节点的各节点之间。节点不一定都是标签类型，也有可能是字符串类型。</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/16.png" style="zoom:50%;">

<h3 id="Beauitful-Soup的find-all-方法"><a href="#Beauitful-Soup的find-all-方法" class="headerlink" title="Beauitful Soup的find_all()方法"></a>Beauitful Soup的find_all()方法</h3><p>格式：soup.find_all(tag, attributes, recursive, text, limit, keywords)</p>
<p>由于find_all()非常常用，我们还可以用简写：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/18.png" style="zoom:50%;">

<p>find()方法不同的是只返回一个结果：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/19.png" style="zoom:50%;">

<p>代码展示（依旧是上文的soup）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(soup.find_all(<span class="string">'a'</span>))<span class="comment">#查找所有a标签  </span></span><br><span class="line">print(soup.find_all([<span class="string">'a'</span>,<span class="string">'b'</span>]))<span class="comment">#查找所有a标签和b标签</span></span><br></pre></td></tr></table></figure>

<p>out：</p>
<p><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/20.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> tag <span class="keyword">in</span> soup.find_all(<span class="literal">True</span>):</span><br><span class="line">    print(tag.name)<span class="comment">#打印所有标签名</span></span><br></pre></td></tr></table></figure>

<p>out：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/21.png" style="zoom:50%;">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re<span class="comment">#导入正则表达式</span></span><br><span class="line"><span class="keyword">for</span> tag <span class="keyword">in</span> soup.find_all(re.compile(<span class="string">'b'</span>)):</span><br><span class="line">    print(tag.name)<span class="comment">#打印以'b'开头的标签名</span></span><br><span class="line">print(soup.find_all(<span class="string">'p'</span>,<span class="string">'course'</span>))<span class="comment">#打印含有'course'的p标签</span></span><br></pre></td></tr></table></figure>

<p>out:</p>
<p><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/22.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(soup.find_all(id=<span class="string">'link1'</span>))<span class="comment">#打印link(属性)=1(值)的标签</span></span><br><span class="line">print(<span class="string">'*'</span>*<span class="number">100</span>)</span><br><span class="line">print(soup.find_all(id=re.compile(<span class="string">'link'</span>)))<span class="comment">#打印含有名字以'link'开头的属性的标签</span></span><br><span class="line">print(<span class="string">'*'</span>*<span class="number">100</span>)</span><br><span class="line">print(soup.find_all(string=<span class="string">'Basic Python'</span>))</span><br><span class="line">print(<span class="string">'*'</span>*<span class="number">100</span>)</span><br><span class="line">print(soup.find_all(string=re.compile(<span class="string">'Python'</span>)))<span class="comment">#打印所有含有'python'的字符串</span></span><br></pre></td></tr></table></figure>

<p>out:</p>
<p><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/23.png" alt></p>
<h2 id="实例2（利用BS获取大学排名信息）"><a href="#实例2（利用BS获取大学排名信息）" class="headerlink" title="实例2（利用BS获取大学排名信息）"></a>实例2（利用BS获取大学排名信息）</h2><p>url:<a href="http://zuihaodaxue.cn/zuihaodaxuepaiming2016.html" target="_blank" rel="noopener">http://zuihaodaxue.cn/zuihaodaxuepaiming2016.html</a></p>
<p>需要输出大学排名和总分。</p>
<h3 id="方法一：用BeautifulSoup"><a href="#方法一：用BeautifulSoup" class="headerlink" title="方法一：用BeautifulSoup"></a>方法一：用BeautifulSoup</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> bs4 </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">r=requests.get(<span class="string">'http://zuihaodaxue.cn/zuihaodaxuepaiming2016.html'</span>)</span><br><span class="line">r.encoding=r.apparent_encoding </span><br><span class="line">demo=r.text</span><br><span class="line">soup=BeautifulSoup(demo,<span class="string">'html.parser'</span>)</span><br><span class="line">ulist=[]</span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> soup.find(<span class="string">'tbody'</span>).children:<span class="comment">#tr代表一所学校的信息</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(tr,bs4.element.Tag):<span class="comment">#过滤掉所有tr中非标签类型的节点,因为通过打印tr我们发现两个None,属于NavigableString类型;isinstance()用于判断某个对象的类型，若符合，则为True</span></span><br><span class="line">        tds=tr(<span class="string">'td'</span>)<span class="comment">#只留下'td'标签，因为此标签包含了所有我们想要的信息</span></span><br><span class="line">        ulist.append([tds[<span class="number">0</span>].string,tds[<span class="number">1</span>].string,tds[<span class="number">3</span>].string]) <span class="comment">#排名，大学名，总分分别在tds列表的第1，2，4位</span></span><br><span class="line"></span><br><span class="line">pd.DataFrame(ulist,columns=[<span class="string">'排名'</span>,<span class="string">'学校名称'</span>,<span class="string">'总分'</span>])<span class="comment">#ulist正好可以看作310行3列的二维数组传到pd.DataFrame中</span></span><br></pre></td></tr></table></figure>

<p><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/24.png" alt></p>
<h3 id="方法二：-用正则表达式"><a href="#方法二：-用正则表达式" class="headerlink" title="方法二：#用正则表达式"></a>方法二：#用正则表达式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">r=requests.get(<span class="string">'http://zuihaodaxue.cn/zuihaodaxuepaiming2016.html'</span>)</span><br><span class="line">r.encoding=r.apparent_encoding</span><br><span class="line">demo=r.text</span><br><span class="line">a=re.findall(<span class="string">'(?&lt;=&lt;tr class="alt"&gt;&lt;td&gt;)\d+(?=&lt;/td&gt;)|(?&lt;=&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;)\d+(?=&lt;/td&gt;)'</span>,demo)<span class="comment">#排名,列表类型</span></span><br><span class="line">b=re.findall(<span class="string">'&lt;div align="left"&gt;(.*?)&lt;/div&gt;'</span>,demo)<span class="comment">#大学名称</span></span><br><span class="line">c=re.findall(<span class="string">'&lt;/td&gt;&lt;td&gt;(.*?)&lt;/td&gt;&lt;td class="hidden-xs need-hidden indicator5"&gt;'</span>,demo)<span class="comment">#对应的分数</span></span><br><span class="line">d=zip(a,b,c)</span><br><span class="line">e=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> d:</span><br><span class="line">    e.append(list(i))</span><br><span class="line">pd.DataFrame(e,columns=[<span class="string">'排名'</span>,<span class="string">'学校名称'</span>,<span class="string">'总分'</span>])</span><br></pre></td></tr></table></figure>

<p>输出结果与方法一 一模一样。</p>
<h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><h3 id="re库的主要功能函数"><a href="#re库的主要功能函数" class="headerlink" title="re库的主要功能函数"></a>re库的主要功能函数</h3><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/25.png" style="zoom: 67%;">

<p>代码展示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">a=<span class="string">'pyxdddyddpyddddxddddz'</span></span><br><span class="line">b=re.search(<span class="string">'xd'</span>,a) <span class="comment">#返回第一个匹配成功的结果，b为match对象.如果想得到每次的，则应该用.finilter()</span></span><br><span class="line"><span class="keyword">if</span> b:</span><br><span class="line">    print(b.group(<span class="number">0</span>))</span><br><span class="line">c=re.match(<span class="string">'py'</span>,a) <span class="comment">#默认匹配起始的字符串，这里a中起始的两个字符是'py',所以匹配成功。c为match对象</span></span><br><span class="line"><span class="keyword">if</span> c:</span><br><span class="line">    print(c.group())</span><br><span class="line">d=re.match(<span class="string">'xd'</span>,a)</span><br><span class="line">d.group(<span class="number">0</span>)<span class="comment">#报错，因为d为NoneType，为空。</span></span><br></pre></td></tr></table></figure>

<p>out：</p>
<p><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/27.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(re.split(<span class="string">'[1-9]\d&#123;5&#125;'</span>,<span class="string">'BIT100081 TSU100084'</span>))<span class="comment">#切割</span></span><br><span class="line">print(re.split(<span class="string">'[1-9]\d&#123;5&#125;'</span>,<span class="string">'BIT100081 TSU100084'</span>,maxsplit=<span class="number">1</span>))<span class="comment">#maxsplit为最大分割数</span></span><br><span class="line">print(re.sub(<span class="string">'[1-9]\d&#123;5&#125;'</span>,<span class="string">'000000'</span>,<span class="string">'BIT100081 TSU100084'</span>))<span class="comment">#替换</span></span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> re.finditer(<span class="string">'[1-9]\d&#123;5&#125;'</span>,<span class="string">'BIT100081 TSU100084'</span>):</span><br><span class="line">    <span class="keyword">if</span> m:</span><br><span class="line">        print(m.group(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<p>out:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[&#39;BIT&#39;, &#39; TSU&#39;, &#39;&#39;]</span><br><span class="line">[&#39;BIT&#39;, &#39; TSU100084&#39;]</span><br><span class="line">BIT000000 TSU000000</span><br><span class="line">100081</span><br><span class="line">100084</span><br></pre></td></tr></table></figure>



<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/26.png" style="zoom:67%;">



<p>rex=’\ba\w*\b’</p>
<p>regex=re.compile(rex) #将符合正则表达式规则的字符串编译成正则表达式。适用于我们需要在代码中多次用这个regex。（上图中的面向对象用法）</p>
<h3 id="Match对象"><a href="#Match对象" class="headerlink" title="Match对象"></a>Match对象</h3><p>re.search()和re.match()等返回的都是match对象。</p>
<h4 id="Match对象的属性"><a href="#Match对象的属性" class="headerlink" title="Match对象的属性"></a>Match对象的属性</h4><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/28.png" style="zoom:50%;">

<h4 id="Match对象的方法"><a href="#Match对象的方法" class="headerlink" title="Match对象的方法"></a>Match对象的方法</h4><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/29.png" style="zoom:50%;">

<p>关于.group()的理解</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re </span><br><span class="line">a = <span class="string">"123abc456"</span></span><br><span class="line"><span class="comment">#group()的用法</span></span><br><span class="line">print( re.search(<span class="string">"([0-9]*)([a-z]*)([0-9]*)"</span>,a).group(<span class="number">0</span>))   <span class="comment">#123abc456,返回整体</span></span><br><span class="line"><span class="keyword">print</span> (re.search(<span class="string">"([0-9]*)([a-z]*)([0-9]*)"</span>,a).group(<span class="number">1</span>) )  <span class="comment">#123</span></span><br><span class="line">print( re.search(<span class="string">"([0-9]*)([a-z]*)([0-9]*)"</span>,a).group(<span class="number">2</span>) )  <span class="comment">#abc</span></span><br><span class="line"><span class="keyword">print</span> (re.search(<span class="string">"([0-9]*)([a-z]*)([0-9]*)"</span>,a).group(<span class="number">3</span>))   <span class="comment">#456</span></span><br></pre></td></tr></table></figure>

<p>out:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">123abc456</span><br><span class="line">123</span><br><span class="line">abc</span><br><span class="line">456</span><br></pre></td></tr></table></figure>

<h3 id="常用操作符"><a href="#常用操作符" class="headerlink" title="常用操作符"></a>常用操作符</h3><p>？表示重复前一个字符0或1次。如abc?可以表示ab或abc。</p>
<p>.表示除换行符之外的任意字符。</p>
<p>[]表示字符集，如 [abc],[a-z],[0-9]，只取其中一个。如\w相当于[a-zA-Z0-9_]</p>
<p>{m}表示扩展前一个字符m次。{m,n}表示扩展前一个字符m次到n次中的任意一次。</p>
<p>|表示左右表达式任意一个</p>
<p>^表示匹配字符串开头； $ 表示匹配字符串结尾,如abc$表示以abc结尾的字符串。</p>
<p>()分组标记，内部只能用|操作符。(abc)表示abc，(abc|efg)表示abc或efg。</p>
<h2 id="实例三（利用正则打印milktea的博客文章名称及阅读量-并降序排列。）"><a href="#实例三（利用正则打印milktea的博客文章名称及阅读量-并降序排列。）" class="headerlink" title="实例三（利用正则打印milktea的博客文章名称及阅读量,并降序排列。）"></a>实例三（利用正则打印milktea的博客文章名称及阅读量,并降序排列。）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">path=<span class="string">'https://blog.csdn.net/SKI_12/article/list/'</span></span><br><span class="line">a=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>):</span><br><span class="line">    current_path=path+str(i)</span><br><span class="line">    text=requests.get(current_path).text</span><br><span class="line">    current_course=re.findall(<span class="string">'&lt;span class="article-type type-1 float-none"&gt;原创&lt;/span&gt;\s*(.*?)\s*&lt;/a&gt;'</span>,text)</span><br><span class="line">    read_count=re.findall(<span class="string">'readCountWhite.png" alt=""&gt;(.*?)&lt;/span&gt;'</span>,text)</span><br><span class="line">    a.extend(zip(current_course,read_count))</span><br><span class="line">final=sorted(a,key=<span class="keyword">lambda</span> x:int(x[<span class="number">1</span>]),reverse=<span class="literal">True</span>)<span class="comment">#</span></span><br><span class="line">b=[]</span><br><span class="line"><span class="keyword">for</span> j,k <span class="keyword">in</span> final:</span><br><span class="line">    b.append([j,k])</span><br><span class="line"><span class="comment">#显示所有列</span></span><br><span class="line">pd.set_option(<span class="string">'display.max_columns'</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="comment">#显示所有行</span></span><br><span class="line">pd.set_option(<span class="string">'display.max_rows'</span>, <span class="literal">None</span>)</span><br><span class="line">pd.DataFrame(b,columns=[<span class="string">'文章名'</span>,<span class="string">'阅读数'</span>])</span><br></pre></td></tr></table></figure>

<p>out:</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/30.png" style="zoom: 50%;">



<h2 id="如何应对网页的反爬机制"><a href="#如何应对网页的反爬机制" class="headerlink" title="如何应对网页的反爬机制"></a>如何应对网页的反爬机制</h2><p>有时一些网页会有反爬机制（它们会根据我们的request header判断我们是爬虫而不是人工进行浏览器访问），我们无法直接爬取其html。</p>
<p>这时，可以用浏览器打开那个网页，F12键查看相关网络请求信息，并将Request headers 的信息复制下来按照键值对的形式放到我们爬虫请求访问的headers中。这样网页就会以为我们是普通的人工访问而不是爬虫了。</p>
<p>例如现在我们想爬取淘宝网关于书包的信息。</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/计算机学习note\Python网络爬虫和信息提取\31.png" style="zoom: 33%;">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">url=<span class="string">'https://s.taobao.com/search?q=书包&amp;s=44'</span></span><br><span class="line">r=requests.get(url,timeout=<span class="number">30</span>)</span><br><span class="line">r.encoding=r.apparent_encoding</span><br><span class="line">text=r.text</span><br><span class="line">print(r.status_code)</span><br><span class="line">print(text)</span><br></pre></td></tr></table></figure>

<p>out：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/32.png" style="zoom:50%;">

<p>可以看到得到的网页代码与网页源码完全（在网页右键即可查询源码）不一样。</p>
<p>现在将淘宝网上Request headers 的信息（不包含以:开头的那几个）复制下来：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/33.png" style="zoom: 33%;">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">new=&#123;</span><br><span class="line"><span class="string">'accept'</span>:<span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'</span>,</span><br><span class="line"><span class="string">'accept-encoding'</span>:<span class="string">'gzip, deflate, br'</span>,</span><br><span class="line"><span class="string">'accept-language'</span>:<span class="string">'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7'</span>,</span><br><span class="line"><span class="string">'cookie'</span>:<span class="string">'cookie2=1744088efca9155b56bd3e0f9d5905eb; t=75313cb0923e69ae22728f64f078da4d; _tb_token_=68aebf17357e; _samesite_flag_=true; tfstk=cWtNB0aOHcna6NsjyGsVhHyrtWJOZd_czkWASUwtHiCu3gQGi7NAKI15LTC-xNf..; enc=LZ3cO6wxiQ6wBk3Vfr7BQplKkPcZy2KKVPKBQIhOo9K4PUiM8DnCKnEPt%2BP6IdufTjGpTMl%2BKK%2FlclC4XVYfpg%3D%3D; thw=cn; hng=CN%7Czh-CN%7CCNY%7C156; mt=ci=0_0; tracknick=; cna=AZYtFzbew1QCAXFscipGmgIf; JSESSIONID=65A5F295B8F1C53428DC6226CC3A7DEC; l=eBOBnIBIQV20X92QBOfZhurza77OSIRADuPzaNbMiOCPO_5p5XkfWZvwzgY9C3GNh60HR35QyF6TBeYBqIv4n5U62j-la_kmn; isg=BGxsuk83HuXukwrZdSH0qhWOPUqeJRDP4jDy5MateJe60Qzb7jXgX2Jj8ZHplUgn'</span>,</span><br><span class="line"><span class="string">'sec-fetch-dest'</span>:<span class="string">'document'</span>,</span><br><span class="line"><span class="string">'sec-fetch-mode'</span>:<span class="string">'navigate'</span>,</span><br><span class="line"><span class="string">'sec-fetch-site'</span>:<span class="string">'none'</span>,</span><br><span class="line"><span class="string">'sec-fetch-user'</span>:<span class="string">'?1'</span>,</span><br><span class="line"><span class="string">'upgrade-insecure-requests'</span>: <span class="string">'1'</span>,</span><br><span class="line"><span class="string">'user-agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'</span></span><br><span class="line">&#125;<span class="comment">#按键值对的格式保存下来</span></span><br><span class="line">r=requests.get(url,timeout=<span class="number">30</span>,headers=new)<span class="comment">#放到headers中</span></span><br><span class="line">text=r.text</span><br><span class="line">text</span><br></pre></td></tr></table></figure>

<p>此时输出了正常的代码，现在我们就可以进行后续的操作啦！</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/34.png" style="zoom:50%;">

<h2 id="scrapy爬虫框架"><a href="#scrapy爬虫框架" class="headerlink" title="scrapy爬虫框架"></a>scrapy爬虫框架</h2><h3 id="scrapy简介"><a href="#scrapy简介" class="headerlink" title="scrapy简介"></a>scrapy简介</h3><p>爬虫框架是实现爬虫功能的一个软件结构和功能组件结合。可以看作一个半成品，帮助用户实现专业网络爬虫。</p>
<p>如图所示，数据流在此框架传输有三种路径：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/35.png" style="zoom:50%;">

<p>其中：</p>
<p>（5+2）指5个模块和两个中间键</p>
<p>SPIDER模块由用户编写配置。在这个模块解析DOWNLOADE返回的响应，产生爬取项（scraped item)以及额外的爬取请求（比如响应中还有新的url，而我们对这个链接也感兴趣）。</p>
<p>ITEM PIPELINES模块由用户编写配置。这个模块以流水线方式处理SPIDER产生的爬取项；它又一组操作顺序构成，类似流水线，每个操作是一个Item Pipeline类型。这些操作包括：清理、检验和查重爬取项中的HTML数据、将数据存储到数据库。</p>
<p>ENGINE模块不需要用户修改。它控制所有模块间的数据流，并根据条件触发事件。</p>
<p>DOWNLOADER模块不需要用户修改。它根据请求下载网页。</p>
<p>SCHEDULER模块不需要用户修改。它对所有爬取请求的优先级进行调度管理。</p>
<p>ENGINE与DOWNLOADER之间的中间键（Downloader Middleware）：</p>
<p>目的：在DOWNLOADER，ENGINE，SCHEDULER中进行用户可控制的配置。</p>
<p>功能：修改、丢弃，新增请求或响应。</p>
<p>ENGINE与SPIDER之间的中间键（Spider Middleware）：</p>
<p>目地：对请求和爬取项的再处理。</p>
<p>功能：修改、丢弃，新增请求或爬取项。</p>
<h3 id="Requests-VS-Scrapy"><a href="#Requests-VS-Scrapy" class="headerlink" title="Requests VS Scrapy"></a>Requests VS Scrapy</h3><p>相同点：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/36.png" style="zoom: 67%;">

<p>不同点：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/37.png" style="zoom: 67%;">

<h3 id="Scrapy爬虫的常用命令"><a href="#Scrapy爬虫的常用命令" class="headerlink" title="Scrapy爬虫的常用命令"></a>Scrapy爬虫的常用命令</h3><p>在cmd中输入 <strong>scrapy -h</strong>可以看它的常用命令。</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/38.png" style="zoom:67%;">

<h2 id="实例四（scrapy应用）"><a href="#实例四（scrapy应用）" class="headerlink" title="实例四（scrapy应用）"></a>实例四（scrapy应用）</h2><p>爬取一个html文件保存到电脑中</p>
<p>步骤一：在cmd中建立一个scrapy工程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\YI&gt;D: #进入D盘 </span><br><span class="line"></span><br><span class="line">D:\&gt;cd pycode #进入pycode文件</span><br><span class="line"></span><br><span class="line">D:\pycode&gt;scrapy startproject python123demo #创建一个名为python123demo的scrapy的工程</span><br></pre></td></tr></table></figure>

<p>生成的目录：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/39.png" style="zoom:50%;">

<p>步骤二：在工程中产生一个Scrapy爬虫</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">D:\pycode&gt;cd python123demo #进入python123demo文件</span><br><span class="line"></span><br><span class="line">D:\pycode\python123demo&gt;scrapy genspider demo python123.io #新建一个爬虫（demo.py），在spiders目录下可以看到。demo指爬虫名，python123.io指要爬取的网站域名</span><br><span class="line">#注意！爬虫名不能与工程名一样</span><br><span class="line">Created spider &#39;demo&#39; using template &#39;basic&#39; in module:</span><br><span class="line">  python123demo.spiders.demo</span><br></pre></td></tr></table></figure>

<p>打开demo.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="comment">#创建爬虫类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoSpider</span><span class="params">(scrapy.Spider)</span>:</span> <span class="comment">#DemoSpider函数必须继承scrapy.spider类</span></span><br><span class="line">    name = <span class="string">'demo'</span> <span class="comment">#当前爬虫的名字叫demo</span></span><br><span class="line">    allowed_domains = [<span class="string">'python123.io'</span>] <span class="comment">#允许采集的域名，可以不要</span></span><br><span class="line">    start_urls = [<span class="string">'http://python123.io/'</span>] <span class="comment">#开始采集的网站</span></span><br><span class="line"><span class="comment">#解析响应数据，提取数据或者网址等</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span> <span class="comment">#用于处理响应、解析内容形成字典、发现新的url爬取请求，目前为初始化值。response就是网页源码</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>步骤三：配置这个爬虫</p>
<p>将返回的html文件存成文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoSpider</span><span class="params">(scrapy.Spider)</span>:</span> <span class="comment">#DemoSpider函数必须继承scrapy.spider类</span></span><br><span class="line">    name = <span class="string">'demo'</span> <span class="comment">#当前爬虫的名字叫demo</span></span><br><span class="line"> <span class="comment">#   allowed_domains = ['python123.io'] #爬取的域名，现在不需要这行</span></span><br><span class="line">    start_urls = [<span class="string">'http://python123.io/ws/demo.html'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span> <span class="comment">#这是一个用于处理响应、解析内容形成字典、发现新的url爬取请求的方法，目前为初始化值。</span></span><br><span class="line">        <span class="comment"># self指面向对象类所属关系的标记，把response的内容写到一个html文件中</span></span><br><span class="line">        fname=response.url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">with</span> open(fname,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line"></span><br><span class="line">        self.log(<span class="string">'saved file %s.'</span> % name ) <span class="comment">#记录日志</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>步骤四：运行爬虫，获取网页</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D:\pycode\python123demo&gt;scrapy crawl demo</span><br></pre></td></tr></table></figure>

<p>可以看到html文件已经被保存到python123demo目录下</p>
<p><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/40.png" alt></p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/41.png" style="zoom: 67%;">



<h3 id="理解yield"><a href="#理解yield" class="headerlink" title="理解yield"></a>理解yield</h3><p>(python3中33个关键字之一)，一般与for循环搭配使用</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/42.png" style="zoom:50%;">

<p>代码展示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">yield</span> i**<span class="number">2</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> gen(<span class="number">5</span>):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure>

<p>0 1 4 9 16</p>
<p>为什么要用yield：</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/43.png" style="zoom:50%;">

<h3 id="scrapy爬虫的步骤和总结"><a href="#scrapy爬虫的步骤和总结" class="headerlink" title="scrapy爬虫的步骤和总结"></a>scrapy爬虫的步骤和总结</h3><img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/44.png" style="zoom:50%;">

<p>scrapy支持BS4,re，xpath，lxml，CSS Selector等html信息提取方法。</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/47.png" style="zoom:67%;">

<h4 id="scrapy的request类"><a href="#scrapy的request类" class="headerlink" title="scrapy的request类"></a>scrapy的request类</h4><p>class scrapy.http.Request()</p>
<p>.Request对象表示一个http请求。</p>
<p>.由Spider生成，由DownLoader执行。</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/45.png" style="zoom: 67%;">

<h4 id="scrapy的response类"><a href="#scrapy的response类" class="headerlink" title="scrapy的response类"></a>scrapy的response类</h4><p>class scrapy.http.Response()</p>
<p>.Response对象表示一个http请求。</p>
<p>.由DownLoader生成，由Spider处理。</p>
<img src="/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/46.png" style="zoom:67%;">



<p>上面两个与requests库中的request和response非常相似。</p>
<h4 id="scrapy的item类"><a href="#scrapy的item类" class="headerlink" title="scrapy的item类"></a>scrapy的item类</h4><p>class scrapy.item.Item()</p>
<p>.Item对象表示一个从http页面中提取的信息内容。</p>
<p>.由Spider生成，由ItemPipeline处理。</p>
<p>.Item类似字典类型，可以按照字典类型操作。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/05/Python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-id="ckcapekfb000bcovt8re740xd" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Re/" rel="tag">Re</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Scrapy/" rel="tag">Scrapy</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/bs4/" rel="tag">bs4</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/07/05/Pandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Pandas学习笔记
        
      </div>
    </a>
  
  
    <a href="/2020/07/05/Scrapy%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B-%E8%8E%B7%E5%8F%96ip%E5%92%8Cport/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Scrapy静态爬虫实例-爬取ip和port</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%93/">数据分析库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%AF%87/">机器学习基础篇</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/" rel="tag">Matplotlib</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Numpy/" rel="tag">Numpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/" rel="tag">Pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Re/" rel="tag">Re</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Request/" rel="tag">Request</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scrapy/" rel="tag">Scrapy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bs4/" rel="tag">bs4</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB/" rel="tag">动态爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" rel="tag">排序算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">无监督学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">监督学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB/" rel="tag">静态爬虫</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Python/" style="font-size: 20px;">Python</a> <a href="/tags/Re/" style="font-size: 10px;">Re</a> <a href="/tags/Request/" style="font-size: 10px;">Request</a> <a href="/tags/Scrapy/" style="font-size: 16.67px;">Scrapy</a> <a href="/tags/bs4/" style="font-size: 10px;">bs4</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB/" style="font-size: 10px;">动态爬虫</a> <a href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" style="font-size: 10px;">排序算法</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 10px;">数据库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">无监督学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 13.33px;">机器学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 10px;">爬虫</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">监督学习</a> <a href="/tags/%E9%9D%99%E6%80%81%E7%88%AC%E8%99%AB/" style="font-size: 10px;">静态爬虫</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/07/06/Scrapy-Re-%E5%8A%A8%E6%80%81%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B/">Scrapy&amp;Re--动态爬虫实例</a>
          </li>
        
          <li>
            <a href="/2020/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习之监督学习算法</a>
          </li>
        
          <li>
            <a href="/2020/07/05/Linux%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Linux操作学习笔记</a>
          </li>
        
          <li>
            <a href="/2020/07/05/Matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Matplotlib学习笔记</a>
          </li>
        
          <li>
            <a href="/2020/07/05/Mysql%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Mysql学习笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>